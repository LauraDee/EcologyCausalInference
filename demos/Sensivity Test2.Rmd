---
title: "Sensitivity Tests"
author: "Laura Dee"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r echo = FALSE}
# Load necessary libraries
library(tidyverse)
library(lmtest)

# Install and load package
#install.packages("rbounds")
library(rbounds)

#install.packages("EValue")
library(EValue)
library(sensemakr)
```

# Learning objectives and take aways
✅ Placebo tests ensure the estimated effect is not driven by spurious correlations.
✅ Sensitivity analyses check if conclusions remain valid under different assumptions.
✅ R provides powerful tools (e.g., EValue, rbounds, placebo regressions) for assessing robustness in causal inference.

# 1.Placebo Tests in R
Placebo tests are used to check if a treatment effect appears where it should not exist, which can indicate spurious correlation or model misspecification.

A placebo test checks if an effect appears where it should not. In Example 2, we test whether a randomly assigned treatment has a significant effect on student test scores. This helps us determine if our model incorrectly detects an effect due to statistical artifacts, confounding, or model misspecification.

## Example 1: Placebo Test with Pre-Treatment Outcomes 
We check if a policy or treatment has an effect before it was implemented. If significant effects are found before the actual intervention, the causal claim is questionable.

Scenario Examples: A state implements a minimum wage increase in 2018, and we estimate its effect on employment. A placebo test checks if a pre-2018 placebo treatment also shows an effect.

```{r ex1}
# Simulate data
set.seed(123)
data <- tibble(
  state = rep(1:50, each = 10),
  year = rep(2009:2018, times = 50),
  treated = ifelse(state <= 25, 1, 0), # 25 treated states
  post_treatment = ifelse(year >= 2018, 1, 0), # Policy starts in 2018
  employment = rnorm(500, mean = 100, sd = 10) - 2 * treated * post_treatment # Simulated policy effect
)

# Run regression for actual treatment
model_actual <- lm(employment ~ treated * post_treatment + factor(state) + factor(year), data = data)
summary(model_actual)

# Placebo Test: Using 2016 as the fake treatment year
data$placebo_post <- ifelse(data$year >= 2016, 1, 0)
model_placebo <- lm(employment ~ treated * placebo_post + factor(state) + factor(year), data = data)
summary(model_placebo)

# Visualizing results
ggplot(data, aes(x = year, y = employment, color = as.factor(treated))) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~treated) +
  labs(title = "Employment Trends Before and After Policy",
       x = "Year", y = "Employment Level",
       color = "Treatment Group")
```

 *Interpretation:*
If the placebo treatment (2016) shows a significant effect, it suggests that trends or confounding factors, not the actual policy, might be driving results.
If no effect is found, it supports the credibility of the causal effect estimated from the actual policy change.

## Example 2: Placebo Test with Fake Treatment Groups

This placebo test ensures that the observed effect isn’t due to random chance by testing a fake, randomized treatment assignment.

Scenario: We analyze the effect of an education program on students' test scores, but we randomly assign treatment as a placebo test.

Expanded Explanation:

A real education program is tested to see if it improves student test scores.
We run a placebo test where treatment is randomly assigned to see if the model falsely detects an effect.
If the placebo treatment appears significant, our original causal claim is likely biased.

Why Randomly Assigning the Treatment is a Placebo Test
True Treatment vs. Fake Treatment:
In a real study, a policy or intervention (e.g., an education program) is assigned to students based on specific criteria.
In a placebo test, we replace the real assignment with a completely random assignment.
Key Idea:
If our model finds a significant effect for a randomly assigned treatment, it suggests our original results may be driven by spurious correlations or bias rather than a true causal effect.
Expected Outcome:
A valid causal model should show no significant effect for a placebo (randomly assigned) treatment.
If the placebo treatment is significant, it suggests our main analysis may be biased (e.g., due to confounding, model errors, or random chance).


```{r ex2}
# Simulate Data
set.seed(456)
data <- tibble(
  student_id = 1:1000,
  treated_real = sample(0:1, 1000, replace = TRUE),  # Real treatment
  treated_placebo = sample(0:1, 1000, replace = TRUE),  # Fake treatment
  pre_score = rnorm(1000, 50, 10),  # Pre-treatment test scores
  post_score = pre_score + treated_real * 5 + rnorm(1000, 0, 3)  # Treatment adds 5 points on avg
)

# Real treatment effect
model_real <- lm(post_score ~ treated_real + pre_score, data = data)
summary(model_real)

# Placebo test: Fake treatment group
model_placebo <- lm(post_score ~ treated_placebo + pre_score, data = data)
summary(model_placebo)
```



```{r ex4}
# Set seed for reproducibility
set.seed(456)

# Generate simulated data
data <- tibble(
  student_id = 1:1000,
  treated_real = sample(0:1, 1000, replace = TRUE),  # True treatment assignment
  treated_placebo = sample(0:1, 1000, replace = TRUE),  # Fake (random) treatment
  pre_score = rnorm(1000, 50, 10),  # Pre-treatment test scores
  post_score = pre_score + treated_real * 5 + rnorm(1000, 0, 3)  # True treatment adds 5 points
)
```
treated_real: This represents the actual education program assignment.
treated_placebo: This is a randomly assigned treatment (placebo).
pre_score: Students’ scores before the program.
post_score: Students’ scores after the program, where the real treatment increases scores by 5 points on average.

Step 2: Run the Real Treatment Model
```{r }
# Analyze the effect of the real treatment
model_real <- lm(post_score ~ treated_real + pre_score, data = data)
summary(model_real)
```
This regression estimates the effect of the real education program.
We expect a positive, significant coefficient for treated_real, since the real program increases test scores.

Step 3: Run the Placebo Test (Fake Treatment)
```{r }
# Analyze the effect of the fake (randomly assigned) treatment
model_placebo <- lm(post_score ~ treated_placebo + pre_score, data = data)
summary(model_placebo)
```
If the placebo treatment shows no effect, the original study is likely valid.
If the placebo treatment is significant, it suggests that our main analysis may be biased (e.g., by omitted variables, incorrect assumptions, or random noise).


Step 4: Visualize Results

We plot test scores before and after the placebo treatment to see if any patterns emerge.
```{r viz}
ggplot(data, aes(x = pre_score, y = post_score, color = as.factor(treated_placebo))) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Placebo Test: Fake Treatment vs. Test Scores",
       x = "Pre-Test Score", y = "Post-Test Score",
       color = "Fake Treatment Group")
```
If students who received the fake treatment have significantly different test scores, it suggests an issue in our model.
If the lines for placebo and non-placebo groups overlap, it means the fake treatment has no real effect, supporting the validity of our causal estimate.


nterpretation:

If the placebo treatment (randomly assigned) is statistically significant, it suggests that the main analysis may be biased.
If the placebo effect is insignificant, it strengthens the validity of the causal effect estimate.

# 2. Sensitivity Analyses in R

Sensitivity analyses check how robust our causal conclusions are to potential violations of key assumptions (e.g., unmeasured confounding, model specification).

## Example 3: Sensitivity Analysis for Unmeasured Confounding (E-value Calculation)
The E-value estimates how strong an unmeasured confounder would need to be to explain away the observed effect. We can use the E-value package.

*Scenario:* We estimate the effect of smoking on lung cancer and check how sensitive the result is to unmeasured confounding.


Expanded Explanation of E-Values in Sensitivity Analysis
The E-value is a statistical tool used in causal inference to assess the robustness of an observed treatment effect to unmeasured confounding. It quantifies how strong an unmeasured confounder would have to be to fully explain away the observed association between treatment and outcome.

1. Why Use E-Values?

In observational studies, treatment and control groups may differ due to unmeasured confounding (e.g., socioeconomic status, genetic factors, environment).
E-values help answer:
How strong must an unmeasured confounder be to reduce our estimated effect to zero (null effect)?
2. How Do E-Values Work?

Suppose we estimate the effect of smoking on lung cancer and find a risk ratio (RR) of 2.5.
The E-value tells us how strong an unmeasured confounder must be (in terms of association with both treatment and outcome) to fully explain away the observed RR of 2.5.
If the E-value is large (e.g., 5.0), an extremely strong confounder would be needed to invalidate the result.
If the E-value is small (e.g., 1.5), even a weak confounder could explain away the effect.
3. Formula for E-Value Calculation

For a risk ratio (RR) or odds ratio (OR) estimate:


4. Interpreting E-Values

E-Value	Interpretation
High (e.g., >4.0)	✅ Very strong confounding needed to explain away the result → The effect is robust.
Moderate (e.g., 2.0 - 4.0)	⚠️ Some confounding could reduce the effect, but strong confounders are still needed.
Low (e.g., <2.0)	❌ A relatively weak confounder could fully explain away the observed effect → The effect is not robust.

# Assume an estimated risk ratio (RR) of 2.5 with a confidence interval (1.8, 3.2)
```{r } 
help(package = "EValue")
EValue::evalues.RR(2.5, lo = 1.8, hi = 3.2)
EValue::evalues.RR(est = 2.5, lo = 1.8, hi = 3.2)
```
Or do it manually
```{r } 
RR <- 2.5  # Estimated risk ratio
E_value <- RR + sqrt(RR * (RR - 1))
E_value
```

Interpretation:

If the E-value is high (e.g., 5+), an extremely strong unmeasured confounder would be needed to fully explain away the effect.
If the E-value is low (e.g., 1.5), the result is more sensitive to hidden confounding.


## Example 4: Sensitivity to Model Specification (Robustness Checks)
We check if the results change under different regression specifications.

Scenario:

A study examines whether a new drug reduces blood pressure.

R Code:
```{r } 
# Simulated Data
set.seed(789)
data <- tibble(
  patient_id = 1:500,
  treated = sample(0:1, 500, replace = TRUE),
  age = rnorm(500, 50, 10),
  bmi = rnorm(500, 25, 4),
  blood_pressure = 120 - treated * 5 + rnorm(500, 0, 5)  # Drug reduces BP by 5 units
)

# Model 1: Basic regression
model1 <- lm(blood_pressure ~ treated, data = data)
summary(model1)

# Model 2: Controlling for age
model2 <- lm(blood_pressure ~ treated + age, data = data)
summary(model2)

# Model 3: Controlling for age and BMI
model3 <- lm(blood_pressure ~ treated + age + bmi, data = data)
summary(model3)
```

Interpretation:

If the estimated effect remains stable across models, it suggests robustness.
If the effect changes dramatically, it indicates sensitivity to model specification (potential omitted variable bias).

## Example 5: Rosenbaum Bounds for Sensitivity to Unmeasured Confounding
We test how strong an unmeasured confounder must be to invalidate our results using the rbounds package.

Scenario:

We estimate the effect of a scholarship program on college enrollment and check for unmeasured confounding.

R Code:
```{r } 
# Simulated matched dataset (outcome: enrollment, treatment: scholarship)
set.seed(111)
scholarship <- sample(0:1, 500, replace = TRUE)
enrollment <- scholarship + rnorm(500, 0, 1)

# Wilcoxon signed-rank test for matching
psens(enrollment, scholarship, Gamma = 2)  # Gamma = strength of confounding
```

Interpretation:

If the effect remains significant for large values of Gamma (e.g., 2+), the results are robust.
If the effect disappears for small values of Gamma (<1.2), the findings are highly sensitive to hidden bias.

## Example X: Sensitivity Analysis Using the sensemakr Package in R
The sensemakr package is a powerful tool for sensitivity analysis in causal inference. It helps assess how unmeasured confounders might impact estimated treatment effects.


```{r } 
# Set seed for reproducibility
set.seed(123)

# Simulate data: Job training impact on wages
n <- 500
training <- rbinom(n, 1, 0.5)  # 50% received training
education <- rnorm(n, mean = 12, sd = 2)  # Avg. 12 years of education
experience <- rnorm(n, mean = 5, sd = 2)  # Avg. 5 years of experience
wages <- 20 + 3 * training + 2 * education + 1.5 * experience + rnorm(n, sd = 5)  # Wage equation

# Create data frame
data <- data.frame(wages, training, education, experience)

# Fit linear regression model
model <- lm(wages ~ training + education + experience, data = data)

# Display model summary
summary(model)
```
Now, check the sensitivity

```{r } 
# Perform sensitivity analysis with sensemakr
sensitivity <- sensemakr(
  model = model,
  treatment = "training",
  benchmark_covariates = c("education", "experience"),  # Use observed confounders as benchmarks
  q = 1,  # Assess robustness to omitted variable bias
  alpha = 0.05  # Significance level
)

# Print sensitivity analysis summary
summary(sensitivity)

# Plot sensitivity results
plot(sensitivity)
```


Step 6: Interpret the Sensitivity Analysis Results
The summary output will show:

R-squared of omitted variable needed to fully explain the effect
How much the observed effect reduces under different confounding scenarios
Whether the treatment effect remains significant under potential omitted variables
If the sensitivity analysis shows that an extremely strong confounder is needed to nullify the effect, we can be confident in our causal estimate. If a moderate confounder could eliminate the effect, we should be cautious in our conclusions.
