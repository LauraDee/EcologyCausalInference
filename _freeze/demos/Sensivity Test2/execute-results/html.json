{
  "hash": "1f475c77ae76648094f47f7990f0aa44",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Sensitivity Tests\"\nauthor: \"Laura Dee\"\ndate: \"2025-03-28\"\noutput: html_document\n---\n\n\n\n\n\n\n# Learning objectives and take aways\n✅ Placebo tests ensure the estimated effect is not driven by spurious correlations.\n✅ Sensitivity analyses check if conclusions remain valid under different assumptions.\n✅ R provides powerful tools (e.g., EValue, rbounds, placebo regressions) for assessing robustness in causal inference.\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nLoading required package: zoo\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nSee details in:\n\nCarlos Cinelli and Chad Hazlett (2020). Making Sense of Sensitivity: Extending Omitted Variable Bias. Journal of the Royal Statistical Society, Series B (Statistical Methodology).\n```\n\n\n:::\n:::\n\n\n\n\n# 1.Placebo Tests in R\nPlacebo tests are used to check if a treatment effect appears where it should not exist, which can indicate spurious correlation or model misspecification.\n\nA placebo test checks if an effect appears where it should not. In Example 2, we test whether a randomly assigned treatment has a significant effect on student test scores. This helps us determine if our model incorrectly detects an effect due to statistical artifacts, confounding, or model misspecification.\n\n## Example 1: Placebo Test with Pre-Treatment Outcomes \nWe check if a policy or treatment has an effect before it was implemented. If significant effects are found before the actual intervention, the causal claim is questionable.\n\nScenario Examples: A state implements a minimum wage increase in 2018, and we estimate its effect on employment. A placebo test checks if a pre-2018 placebo treatment also shows an effect.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate data\nset.seed(123)\ndata <- tibble(\n  state = rep(1:50, each = 10),\n  year = rep(2009:2018, times = 50),\n  treated = ifelse(state <= 25, 1, 0), # 25 treated states\n  post_treatment = ifelse(year >= 2018, 1, 0), # Policy starts in 2018\n  employment = rnorm(500, mean = 100, sd = 10) - 2 * treated * post_treatment # Simulated policy effect\n)\n\n# Run regression for actual treatment\nmodel_actual <- lm(employment ~ treated * post_treatment + factor(state) + factor(year), data = data)\nsummary(model_actual)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = employment ~ treated * post_treatment + factor(state) + \n    factor(year), data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-28.7438  -6.0938  -0.2846   6.0033  29.7668 \n\nCoefficients: (2 not defined because of singularities)\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            103.0010     3.3976  30.315   <2e-16 ***\ntreated                  0.4352     4.4290   0.098   0.9218    \npost_treatment          -2.6077     2.4649  -1.058   0.2907    \nfactor(state)2           1.3400     4.4192   0.303   0.7619    \nfactor(state)3          -4.9918     4.4192  -1.130   0.2593    \nfactor(state)4           2.4742     4.4192   0.560   0.5759    \nfactor(state)5          -0.8334     4.4192  -0.189   0.8505    \nfactor(state)6           1.4706     4.4192   0.333   0.7395    \nfactor(state)7           0.4846     4.4192   0.110   0.9127    \nfactor(state)8          -4.3754     4.4192  -0.990   0.3227    \nfactor(state)9           2.3847     4.4192   0.540   0.5897    \nfactor(state)10          3.6247     4.4192   0.820   0.4125    \nfactor(state)11         -4.7047     4.4192  -1.065   0.2876    \nfactor(state)12         -3.9754     4.4192  -0.900   0.3688    \nfactor(state)13         -1.8506     4.4192  -0.419   0.6756    \nfactor(state)14         -0.4089     4.4192  -0.093   0.9263    \nfactor(state)15         -5.4867     4.4192  -1.242   0.2151    \nfactor(state)16          0.5274     4.4192   0.119   0.9050    \nfactor(state)17          2.1581     4.4192   0.488   0.6255    \nfactor(state)18         -0.3129     4.4192  -0.071   0.9436    \nfactor(state)19         -0.7456     4.4192  -0.169   0.8661    \nfactor(state)20         -3.4181     4.4192  -0.773   0.4397    \nfactor(state)21          2.3661     4.4192   0.535   0.5926    \nfactor(state)22         -2.6689     4.4192  -0.604   0.5462    \nfactor(state)23         -0.7415     4.4192  -0.168   0.8668    \nfactor(state)24         -1.5179     4.4192  -0.343   0.7314    \nfactor(state)25         -1.5951     4.4192  -0.361   0.7183    \nfactor(state)26         -0.1501     4.4192  -0.034   0.9729    \nfactor(state)27          1.4536     4.4192   0.329   0.7424    \nfactor(state)28          1.6677     4.4192   0.377   0.7061    \nfactor(state)29          0.7183     4.4192   0.163   0.8710    \nfactor(state)30          6.1122     4.4192   1.383   0.1673    \nfactor(state)31         -2.6271     4.4192  -0.594   0.5525    \nfactor(state)32          0.7785     4.4192   0.176   0.8602    \nfactor(state)33          2.3192     4.4192   0.525   0.6000    \nfactor(state)34         -2.3832     4.4192  -0.539   0.5900    \nfactor(state)35         -0.6556     4.4192  -0.148   0.8821    \nfactor(state)36         -1.8980     4.4192  -0.429   0.6678    \nfactor(state)37          1.6467     4.4192   0.373   0.7096    \nfactor(state)38         -2.1433     4.4192  -0.485   0.6279    \nfactor(state)39         -0.1039     4.4192  -0.024   0.9813    \nfactor(state)40         -3.8974     4.4192  -0.882   0.3783    \nfactor(state)41         -3.3247     4.4192  -0.752   0.4523    \nfactor(state)42         -1.9829     4.4192  -0.449   0.6539    \nfactor(state)43          4.3652     4.4192   0.988   0.3238    \nfactor(state)44         -1.3018     4.4192  -0.295   0.7684    \nfactor(state)45         -3.7068     4.4192  -0.839   0.4020    \nfactor(state)46          1.9103     4.4192   0.432   0.6657    \nfactor(state)47          3.8897     4.4192   0.880   0.3792    \nfactor(state)48          1.0666     4.4192   0.241   0.8094    \nfactor(state)49          4.3279     4.4192   0.979   0.3280    \nfactor(state)50              NA         NA      NA       NA    \nfactor(year)2010        -2.9238     1.9763  -1.479   0.1397    \nfactor(year)2011        -2.4424     1.9763  -1.236   0.2172    \nfactor(year)2012        -2.9507     1.9763  -1.493   0.1361    \nfactor(year)2013        -3.5011     1.9763  -1.772   0.0772 .  \nfactor(year)2014        -2.7767     1.9763  -1.405   0.1607    \nfactor(year)2015        -1.2810     1.9763  -0.648   0.5172    \nfactor(year)2016        -4.3472     1.9763  -2.200   0.0284 *  \nfactor(year)2017        -1.8373     1.9763  -0.930   0.3531    \nfactor(year)2018             NA         NA      NA       NA    \ntreated:post_treatment  -4.2308     2.9461  -1.436   0.1517    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.882 on 440 degrees of freedom\nMultiple R-squared:  0.09686,\tAdjusted R-squared:  -0.02424 \nF-statistic: 0.7999 on 59 and 440 DF,  p-value: 0.8544\n```\n\n\n:::\n\n```{.r .cell-code}\n# Placebo Test: Using 2016 as the fake treatment year\ndata$placebo_post <- ifelse(data$year >= 2016, 1, 0)\nmodel_placebo <- lm(employment ~ treated * placebo_post + factor(state) + factor(year), data = data)\nsummary(model_placebo)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = employment ~ treated * placebo_post + factor(state) + \n    factor(year), data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-28.7205  -6.1867  -0.1883   6.1179  29.7436 \n\nCoefficients: (2 not defined because of singularities)\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          102.9777     3.4122  30.179   <2e-16 ***\ntreated                0.4817     4.4640   0.108   0.9141    \nplacebo_post          -3.9403     2.2026  -1.789   0.0743 .  \nfactor(state)2         1.3400     4.4262   0.303   0.7622    \nfactor(state)3        -4.9918     4.4262  -1.128   0.2600    \nfactor(state)4         2.4742     4.4262   0.559   0.5765    \nfactor(state)5        -0.8334     4.4262  -0.188   0.8507    \nfactor(state)6         1.4706     4.4262   0.332   0.7399    \nfactor(state)7         0.4846     4.4262   0.109   0.9129    \nfactor(state)8        -4.3754     4.4262  -0.989   0.3234    \nfactor(state)9         2.3847     4.4262   0.539   0.5903    \nfactor(state)10        3.6247     4.4262   0.819   0.4133    \nfactor(state)11       -4.7047     4.4262  -1.063   0.2884    \nfactor(state)12       -3.9754     4.4262  -0.898   0.3696    \nfactor(state)13       -1.8506     4.4262  -0.418   0.6761    \nfactor(state)14       -0.4089     4.4262  -0.092   0.9264    \nfactor(state)15       -5.4867     4.4262  -1.240   0.2158    \nfactor(state)16        0.5274     4.4262   0.119   0.9052    \nfactor(state)17        2.1581     4.4262   0.488   0.6261    \nfactor(state)18       -0.3129     4.4262  -0.071   0.9437    \nfactor(state)19       -0.7456     4.4262  -0.168   0.8663    \nfactor(state)20       -3.4181     4.4262  -0.772   0.4404    \nfactor(state)21        2.3661     4.4262   0.535   0.5932    \nfactor(state)22       -2.6689     4.4262  -0.603   0.5468    \nfactor(state)23       -0.7415     4.4262  -0.168   0.8670    \nfactor(state)24       -1.5179     4.4262  -0.343   0.7318    \nfactor(state)25       -1.5951     4.4262  -0.360   0.7187    \nfactor(state)26       -0.1501     4.4262  -0.034   0.9730    \nfactor(state)27        1.4536     4.4262   0.328   0.7428    \nfactor(state)28        1.6677     4.4262   0.377   0.7065    \nfactor(state)29        0.7183     4.4262   0.162   0.8712    \nfactor(state)30        6.1122     4.4262   1.381   0.1680    \nfactor(state)31       -2.6271     4.4262  -0.594   0.5531    \nfactor(state)32        0.7785     4.4262   0.176   0.8605    \nfactor(state)33        2.3192     4.4262   0.524   0.6006    \nfactor(state)34       -2.3832     4.4262  -0.538   0.5906    \nfactor(state)35       -0.6556     4.4262  -0.148   0.8823    \nfactor(state)36       -1.8980     4.4262  -0.429   0.6683    \nfactor(state)37        1.6467     4.4262   0.372   0.7101    \nfactor(state)38       -2.1433     4.4262  -0.484   0.6285    \nfactor(state)39       -0.1039     4.4262  -0.023   0.9813    \nfactor(state)40       -3.8974     4.4262  -0.881   0.3791    \nfactor(state)41       -3.3247     4.4262  -0.751   0.4530    \nfactor(state)42       -1.9829     4.4262  -0.448   0.6544    \nfactor(state)43        4.3652     4.4262   0.986   0.3246    \nfactor(state)44       -1.3018     4.4262  -0.294   0.7688    \nfactor(state)45       -3.7068     4.4262  -0.837   0.4028    \nfactor(state)46        1.9103     4.4262   0.432   0.6662    \nfactor(state)47        3.8897     4.4262   0.879   0.3800    \nfactor(state)48        1.0666     4.4262   0.241   0.8097    \nfactor(state)49        4.3279     4.4262   0.978   0.3287    \nfactor(state)50            NA         NA      NA       NA    \nfactor(year)2010      -2.9238     1.9795  -1.477   0.1404    \nfactor(year)2011      -2.4424     1.9795  -1.234   0.2179    \nfactor(year)2012      -2.9507     1.9795  -1.491   0.1368    \nfactor(year)2013      -3.5011     1.9795  -1.769   0.0776 .  \nfactor(year)2014      -2.7767     1.9795  -1.403   0.1614    \nfactor(year)2015      -1.2810     1.9795  -0.647   0.5179    \nfactor(year)2016       0.3758     1.9795   0.190   0.8495    \nfactor(year)2017       2.8858     1.9795   1.458   0.1456    \nfactor(year)2018           NA         NA      NA       NA    \ntreated:placebo_post  -1.5655     1.9318  -0.810   0.4182    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.897 on 440 degrees of freedom\nMultiple R-squared:  0.09398,\tAdjusted R-squared:  -0.0275 \nF-statistic: 0.7736 on 59 and 440 DF,  p-value: 0.8878\n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualizing results\nggplot(data, aes(x = year, y = employment, color = as.factor(treated))) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~treated) +\n  labs(title = \"Employment Trends Before and After Policy\",\n       x = \"Year\", y = \"Employment Level\",\n       color = \"Treatment Group\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Sensivity-Test2_files/figure-html/ex1-1.png){width=672}\n:::\n:::\n\n\n\n\n *Interpretation:*\nIf the placebo treatment (2016) shows a significant effect, it suggests that trends or confounding factors, not the actual policy, might be driving results.\nIf no effect is found, it supports the credibility of the causal effect estimated from the actual policy change.\n\n## Example 2: Placebo Test with Fake Treatment Groups\n\nThis placebo test ensures that the observed effect isn’t due to random chance by testing a fake, randomized treatment assignment.\n\nScenario: We analyze the effect of an education program on students' test scores, but we randomly assign treatment as a placebo test.\n\nExpanded Explanation:\n\nA real education program is tested to see if it improves student test scores.\nWe run a placebo test where treatment is randomly assigned to see if the model falsely detects an effect.\nIf the placebo treatment appears significant, our original causal claim is likely biased.\n\nWhy Randomly Assigning the Treatment is a Placebo Test\nTrue Treatment vs. Fake Treatment:\nIn a real study, a policy or intervention (e.g., an education program) is assigned to students based on specific criteria.\nIn a placebo test, we replace the real assignment with a completely random assignment.\nKey Idea:\nIf our model finds a significant effect for a randomly assigned treatment, it suggests our original results may be driven by spurious correlations or bias rather than a true causal effect.\nExpected Outcome:\nA valid causal model should show no significant effect for a placebo (randomly assigned) treatment.\nIf the placebo treatment is significant, it suggests our main analysis may be biased (e.g., due to confounding, model errors, or random chance).\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate Data\nset.seed(456)\ndata <- tibble(\n  student_id = 1:1000,\n  treated_real = sample(0:1, 1000, replace = TRUE),  # Real treatment\n  treated_placebo = sample(0:1, 1000, replace = TRUE),  # Fake treatment\n  pre_score = rnorm(1000, 50, 10),  # Pre-treatment test scores\n  post_score = pre_score + treated_real * 5 + rnorm(1000, 0, 3)  # Treatment adds 5 points on avg\n)\n\n# Real treatment effect\nmodel_real <- lm(post_score ~ treated_real + pre_score, data = data)\nsummary(model_real)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = post_score ~ treated_real + pre_score, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-10.149  -1.991   0.069   1.907  11.665 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.669861   0.478638    -1.4    0.162    \ntreated_real  5.192764   0.182868    28.4   <2e-16 ***\npre_score     1.009045   0.009113   110.7   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.891 on 997 degrees of freedom\nMultiple R-squared:  0.9292,\tAdjusted R-squared:  0.9291 \nF-statistic:  6543 on 2 and 997 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Placebo test: Fake treatment group\nmodel_placebo <- lm(post_score ~ treated_placebo + pre_score, data = data)\nsummary(model_placebo)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = post_score ~ treated_placebo + pre_score, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2109  -2.7614   0.0739   2.8359  12.2893 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      1.97996    0.64436   3.073  0.00218 ** \ntreated_placebo -0.26389    0.24592  -1.073  0.28350    \npre_score        1.00997    0.01225  82.449  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.886 on 997 degrees of freedom\nMultiple R-squared:  0.8721,\tAdjusted R-squared:  0.8718 \nF-statistic:  3399 on 2 and 997 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set seed for reproducibility\nset.seed(456)\n\n# Generate simulated data\ndata <- tibble(\n  student_id = 1:1000,\n  treated_real = sample(0:1, 1000, replace = TRUE),  # True treatment assignment\n  treated_placebo = sample(0:1, 1000, replace = TRUE),  # Fake (random) treatment\n  pre_score = rnorm(1000, 50, 10),  # Pre-treatment test scores\n  post_score = pre_score + treated_real * 5 + rnorm(1000, 0, 3)  # True treatment adds 5 points\n)\n```\n:::\n\n\n\ntreated_real: This represents the actual education program assignment.\ntreated_placebo: This is a randomly assigned treatment (placebo).\npre_score: Students’ scores before the program.\npost_score: Students’ scores after the program, where the real treatment increases scores by 5 points on average.\n\nStep 2: Run the Real Treatment Model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Analyze the effect of the real treatment\nmodel_real <- lm(post_score ~ treated_real + pre_score, data = data)\nsummary(model_real)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = post_score ~ treated_real + pre_score, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-10.149  -1.991   0.069   1.907  11.665 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.669861   0.478638    -1.4    0.162    \ntreated_real  5.192764   0.182868    28.4   <2e-16 ***\npre_score     1.009045   0.009113   110.7   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.891 on 997 degrees of freedom\nMultiple R-squared:  0.9292,\tAdjusted R-squared:  0.9291 \nF-statistic:  6543 on 2 and 997 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\nThis regression estimates the effect of the real education program. We expect a positive, significant coefficient for treated_real, since the real program increases test scores.\n\nStep 3: Run the Placebo Test (Fake Treatment)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Analyze the effect of the fake (randomly assigned) treatment\nmodel_placebo <- lm(post_score ~ treated_placebo + pre_score, data = data)\nsummary(model_placebo)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = post_score ~ treated_placebo + pre_score, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2109  -2.7614   0.0739   2.8359  12.2893 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      1.97996    0.64436   3.073  0.00218 ** \ntreated_placebo -0.26389    0.24592  -1.073  0.28350    \npre_score        1.00997    0.01225  82.449  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.886 on 997 degrees of freedom\nMultiple R-squared:  0.8721,\tAdjusted R-squared:  0.8718 \nF-statistic:  3399 on 2 and 997 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\nIf the placebo treatment shows no effect, the original study is likely valid.\n\nIf the placebo treatment is significant, it suggests that our main analysis may be biased (e.g., by omitted variables, incorrect assumptions, or random noise).\n\nStep 4: Visualize Results\nWe can plot test scores before and after the placebo treatment to see if any patterns emerge.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data, aes(x = pre_score, y = post_score, color = as.factor(treated_placebo))) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Placebo Test: Fake Treatment vs. Test Scores\",\n       x = \"Pre-Test Score\", y = \"Post-Test Score\",\n       color = \"Fake Treatment Group\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Sensivity-Test2_files/figure-html/viz-1.png){width=672}\n:::\n:::\n\n\n\nIf students who received the fake treatment have significantly different test scores, it suggests an issue in our model.\n\nIf the lines for placebo and non-placebo groups overlap, it means the fake treatment has no real effect, supporting the validity of our causal estimate.\n\nInterpretation:\n\nIf the placebo treatment (randomly assigned) is statistically significant, it suggests that the main analysis may be biased.\nIf the placebo effect is insignificant, it strengthens the validity of the causal effect estimate.\n\n# 2. Sensitivity Analyses in R\n\nSensitivity analyses check how robust our causal conclusions are to potential violations of key assumptions (e.g., unmeasured confounding, model specification).\n\n## Example 3: Sensitivity Analysis for Unmeasured Confounding (E-value Calculation)\nThe E-value estimates how strong an unmeasured confounder would need to be to explain away the observed effect. We can use the E-value package.\n\n*Scenario:* We estimate the effect of smoking on lung cancer and check how sensitive the result is to unmeasured confounding.\n\nThe E-value is a statistical tool used in causal inference to assess the robustness of an observed treatment effect to unmeasured confounding. It quantifies how strong an unmeasured confounder would have to be to fully explain away the observed association between treatment and outcome.\n\n### 1. Why Use E-Values?\n\nIn observational studies, treatment and control groups may differ due to unmeasured confounding (e.g., socioeconomic status, genetic factors, environment).\nE-values help answer:\nHow strong must an unmeasured confounder be to reduce our estimated effect to zero (null effect)?\n\n### 2. How Do E-Values Work?\n\nSuppose we estimate the effect of smoking on lung cancer and find a risk ratio (RR) of 2.5.\nThe E-value tells us how strong an unmeasured confounder must be (in terms of association with both treatment and outcome) to fully explain away the observed RR of 2.5.\nIf the E-value is large (e.g., 5.0), an extremely strong confounder would be needed to invalidate the result.\nIf the E-value is small (e.g., 1.5), even a weak confounder could explain away the effect.\n3. Formula for E-Value Calculation\n\n###  For a risk ratio (RR) or odds ratio (OR) estimate:\n\n\n###  4. Interpreting E-Values\n\nE-Value\tInterpretation\nHigh (e.g., >4.0)\t✅ Very strong confounding needed to explain away the result → The effect is robust.\nModerate (e.g., 2.0 - 4.0)\t⚠️ Some confounding could reduce the effect, but strong confounders are still needed.\nLow (e.g., <2.0)\t❌ A relatively weak confounder could fully explain away the observed effect → The effect is not robust.\n\n###  Assume an estimated risk ratio (RR) of 2.5 with a confidence interval (1.8, 3.2)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhelp(package = \"EValue\")\nEValue::evalues.RR(2.5, lo = 1.8, hi = 3.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            point lower upper\nRR       2.500000   1.8   3.2\nE-values 4.436492   3.0    NA\n```\n\n\n:::\n\n```{.r .cell-code}\nEValue::evalues.RR(est = 2.5, lo = 1.8, hi = 3.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            point lower upper\nRR       2.500000   1.8   3.2\nE-values 4.436492   3.0    NA\n```\n\n\n:::\n:::\n\n\n\nOr do it manually\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nRR <- 2.5  # Estimated risk ratio\nE_value <- RR + sqrt(RR * (RR - 1))\nE_value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.436492\n```\n\n\n:::\n:::\n\n\n\n\n###  Interpretation:\nIf the E-value is high (e.g., 5+), an extremely strong unmeasured confounder would be needed to fully explain away the effect.\nIf the E-value is low (e.g., 1.5), the result is more sensitive to hidden confounding.\n\n\n## Example 4: Sensitivity to Model Specification (Robustness Checks)\nWe check if the results change under different regression specifications.\n\nScenario: A study examines whether a new drug reduces blood pressure.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulated Data\nset.seed(789)\ndata <- tibble(\n  patient_id = 1:500,\n  treated = sample(0:1, 500, replace = TRUE),\n  age = rnorm(500, 50, 10),\n  bmi = rnorm(500, 25, 4),\n  blood_pressure = 120 - treated * 5 + rnorm(500, 0, 5)  # Drug reduces BP by 5 units\n)\n\n# Model 1: Basic regression\nmodel1 <- lm(blood_pressure ~ treated, data = data)\nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = blood_pressure ~ treated, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.4700  -3.1209  -0.1249   3.2771  13.6328 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 120.0594     0.3150  381.19   <2e-16 ***\ntreated      -4.8644     0.4278  -11.37   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.766 on 498 degrees of freedom\nMultiple R-squared:  0.2061,\tAdjusted R-squared:  0.2045 \nF-statistic: 129.3 on 1 and 498 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Model 2: Controlling for age\nmodel2 <- lm(blood_pressure ~ treated + age, data = data)\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = blood_pressure ~ treated + age, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.3825  -3.2104  -0.1291   3.2562  13.4816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 119.5588     1.1514 103.834   <2e-16 ***\ntreated      -4.8608     0.4282 -11.351   <2e-16 ***\nage           0.0099     0.0219   0.452    0.651    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.77 on 497 degrees of freedom\nMultiple R-squared:  0.2064,\tAdjusted R-squared:  0.2032 \nF-statistic: 64.64 on 2 and 497 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Model 3: Controlling for age and BMI\nmodel3 <- lm(blood_pressure ~ treated + age + bmi, data = data)\nsummary(model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = blood_pressure ~ treated + age + bmi, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.3533  -3.1962  -0.1751   3.2654  13.4810 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 119.255805   1.674354  71.225   <2e-16 ***\ntreated      -4.865551   0.429052 -11.340   <2e-16 ***\nage           0.009542   0.021968   0.434    0.664    \nbmi           0.013045   0.052294   0.249    0.803    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.774 on 496 degrees of freedom\nMultiple R-squared:  0.2065,\tAdjusted R-squared:  0.2017 \nF-statistic: 43.04 on 3 and 496 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\nInterpretation:\n\n- If the estimated effect remains stable across models, it suggests robustness.\n- If the effect changes dramatically, it indicates sensitivity to model specification (potential omitted variable bias).\n\n## Example 5: Rosenbaum Bounds for Sensitivity to Unmeasured Confounding\nWe test how strong an unmeasured confounder must be to invalidate our results using the rbounds package.\n\nScenario: We estimate the effect of a scholarship program on college enrollment and check for unmeasured confounding.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulated matched dataset (outcome: enrollment, treatment: scholarship)\nset.seed(111)\nscholarship <- sample(0:1, 500, replace = TRUE)\nenrollment <- scholarship + rnorm(500, 0, 1)\n\n# Wilcoxon signed-rank test for matching\npsens(enrollment, scholarship, Gamma = 2)  # Gamma = strength of confounding\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n Rosenbaum Sensitivity Test for Wilcoxon Signed Rank P-Value \n \nUnconfounded estimate ....  0.5866 \n\n Gamma Lower bound Upper bound\n     1      0.5866      0.5866\n     2      0.0000      1.0000\n\n Note: Gamma is Odds of Differential Assignment To\n Treatment Due to Unobserved Factors \n \n```\n\n\n:::\n:::\n\n\n\n\nInterpretation:\nIf the effect remains significant for large values of Gamma (e.g., 2+), the results are robust.\nIf the effect disappears for small values of Gamma (<1.2), the findings are highly sensitive to hidden bias.\n\n## Example X: Sensitivity Analysis Using the sensemakr Package in R\nThe sensemakr package is a powerful tool for sensitivity analysis in causal inference. It helps assess how unmeasured confounders might impact estimated treatment effects.\n\n📌 Step 2: Simulate a Dataset\nWe create a dataset where:\n\nwages: Worker’s wage after training\ntraining: Binary variable (1 = received job training, 0 = no training)\neducation: Years of education (a potential confounder)\nexperience: Years of work experience (another potential confounder)\n\nHere, we assume the true treatment effect of training on wages is $3 per hour.\n\n📌 Step 3: Run the Initial Regression\nWe estimate the impact of job training on wages, controlling for education and experience.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set seed for reproducibility\nset.seed(123)\n\n# Simulate data: Job training impact on wages\nn <- 500\ntraining <- rbinom(n, 1, 0.5)  # 50% received training\neducation <- rnorm(n, mean = 12, sd = 2)  # Avg. 12 years of education\nexperience <- rnorm(n, mean = 5, sd = 2)  # Avg. 5 years of experience\nwages <- 20 + 3 * training + 2 * education + 1.5 * experience + rnorm(n, sd = 5)  # Wage equation\n\n# Create data frame\ndata <- data.frame(wages, training, education, experience)\n\n# Fit linear regression model\nmodel <- lm(wages ~ training + education + experience, data = data)\n\n# Display model summary\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = wages ~ training + education + experience, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.6127  -3.1967   0.0747   3.4101  17.0122 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  18.5307     1.4648  12.651  < 2e-16 ***\ntraining      3.4949     0.4445   7.862 2.38e-14 ***\neducation     2.2004     0.1105  19.921  < 2e-16 ***\nexperience    1.3084     0.1091  11.989  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.952 on 496 degrees of freedom\nMultiple R-squared:  0.5555,\tAdjusted R-squared:  0.5528 \nF-statistic: 206.6 on 3 and 496 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\nNow, check the sensitivity: \n\n📌 Step 4: Conduct Sensitivity Analysis with sensemakr\nWe now check if an unmeasured confounder could explain away the observed effect of training.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Perform sensitivity analysis with sensemakr\nsensitivity <- sensemakr(\n  model = model,\n  treatment = \"training\",\n  benchmark_covariates = c(\"education\", \"experience\"),  # Use observed confounders as benchmarks\n  q = 1,  # Assess robustness to omitted variable bias\n  alpha = 0.05  # Significance level\n)\n\n# Print sensitivity analysis summary\nsummary(sensitivity)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSensitivity Analysis to Unobserved Confounding\n\nModel Formula: wages ~ training + education + experience\n\nNull hypothesis: q = 1 and reduce = TRUE \n-- This means we are considering biases that reduce the absolute value of the current estimate.\n-- The null hypothesis deemed problematic is H0:tau = 0 \n\nUnadjusted Estimates of 'training': \n  Coef. estimate: 3.4949 \n  Standard Error: 0.4445 \n  t-value (H0:tau = 0): 7.8616 \n\nSensitivity Statistics:\n  Partial R2 of treatment with outcome: 0.1108 \n  Robustness Value, q = 1: 0.2961 \n  Robustness Value, q = 1, alpha = 0.05: 0.232 \n\nVerbal interpretation of sensitivity statistics:\n\n-- Partial R2 of the treatment with the outcome: an extreme confounder (orthogonal to the covariates) that explains 100% of the residual variance of the outcome, would need to explain at least 11.08% of the residual variance of the treatment to fully account for the observed estimated effect.\n\n-- Robustness Value, q = 1: unobserved confounders (orthogonal to the covariates) that explain more than 29.61% of the residual variance of both the treatment and the outcome are strong enough to bring the point estimate to 0 (a bias of 100% of the original estimate). Conversely, unobserved confounders that do not explain more than 29.61% of the residual variance of both the treatment and the outcome are not strong enough to bring the point estimate to 0.\n\n-- Robustness Value, q = 1, alpha = 0.05: unobserved confounders (orthogonal to the covariates) that explain more than 23.2% of the residual variance of both the treatment and the outcome are strong enough to bring the estimate to a range where it is no longer 'statistically different' from 0 (a bias of 100% of the original estimate), at the significance level of alpha = 0.05. Conversely, unobserved confounders that do not explain more than 23.2% of the residual variance of both the treatment and the outcome are not strong enough to bring the estimate to a range where it is no longer 'statistically different' from 0, at the significance level of alpha = 0.05.\n\nBounds on omitted variable bias:\n\n--The table below shows the maximum strength of unobserved confounders with association with the treatment and the outcome bounded by a multiple of the observed explanatory power of the chosen benchmark covariate(s).\n\n   Bound Label R2dz.x R2yz.dx Treatment Adjusted Estimate Adjusted Se\n  1x education 0.0037  0.8061  training            2.9529      0.1963\n 1x experience 0.0001  0.2898  training            3.4460      0.3750\n Adjusted T Adjusted Lower CI Adjusted Upper CI\n    15.0403            2.5671            3.3386\n     9.1888            2.7092            4.1828\n```\n\n\n:::\n:::\n\n\n\n\n📌 Step 5: Visualize Sensitivity Results\nsensemakr provides a contour plot that shows how strong an unmeasured confounder would have to be to invalidate our conclusions.\n\n🔍 How to Interpret the Plot:\n\nThe x-axis represents the strength of an unmeasured confounder’s association with treatment (job training).\nThe y-axis represents its effect on the outcome (wages).\nIf the observed effect remains above the significance threshold across plausible values, the result is robust.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot sensitivity results\nplot(sensitivity)\n```\n\n::: {.cell-output-display}\n![](Sensivity-Test2_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\nStep 6: Interpret the Sensitivity Analysis Results\nThe summary output will show:\n\nR-squared of omitted variable needed to fully explain the effect\nHow much the observed effect reduces under different confounding scenarios\nWhether the treatment effect remains significant under potential omitted variables\nIf the sensitivity analysis shows that an extremely strong confounder is needed to nullify the effect, we can be confident in our causal estimate. If a moderate confounder could eliminate the effect, we should be cautious in our conclusions.\n\n# Rosenbaum Bounds\n\nRosenbaum bounds are used in sensitivity analysis for observational studies, particularly in propensity score matching. They assess how sensitive causal inferences are to potential hidden bias due to unmeasured confounders. Since observational studies lack randomized treatment assignment, unmeasured variables may affect both the treatment and the outcome. Rosenbaum bounds estimate how strong an unmeasured confounder would have to be to alter the conclusions of a study.\n\nConcept of Rosenbaum Bounds\nThe method introduces a sensitivity parameter, Gamma (Γ), which represents the degree of potential hidden bias:\n\nIf Γ = 1, the study is free from hidden bias, meaning matched pairs have the same probability of treatment assignment.\nIf Γ > 1, there is potential unmeasured confounding, and the higher the value of Γ, the more sensitive the results are to hidden bias.\nRosenbaum bounds use Wilcoxon signed-rank tests or Mantel-Haenszel tests to check how the significance of an estimated treatment effect changes under different levels of Γ.\n\n### *Example:* \nI will illustrate Rosenbaum bounds with an ecological example: assessing the effect of deforestation on bird species richness. Suppose we have an observational dataset where sites with deforestation are matched to sites without deforestation based on environmental factors (e.g., elevation, precipitation).\n\n\n## Introduction\n\nThis document demonstrates how to perform **Rosenbaum Bounds Sensitivity Analysis** using R, applied to an ecological study assessing the impact of deforestation on bird species richness.\n\n## Load Required Libraries\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install packages if not already installed\nif (!require(MatchIt)) install.packages(\"MatchIt\", dependencies = TRUE)\nif (!require(rbounds)) install.packages(\"rbounds\", dependencies = TRUE)\n\n# Load the libraries\nlibrary(MatchIt)\nlibrary(rbounds)\n```\n:::\n\n\n\n\n## Simulate Ecological Data\n\nWe create a dataset where bird species richness is compared between deforested and non-deforested sites while controlling for elevation and precipitation.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\ndata <- data.frame(\n  deforestation = rep(c(1, 0), each = 50),\n  species_richness = c(rnorm(50, mean = 10, sd = 3), rnorm(50, mean = 15, sd = 3)),\n  elevation = rnorm(100, mean = 500, sd = 100),\n  precipitation = rnorm(100, mean = 1000, sd = 200)\n)\n```\n:::\n\n\n\n\n## Perform Propensity Score Matching\n\nWe match deforested and non-deforested sites based on elevation and precipitation.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmatch_model <- matchit(deforestation ~ elevation + precipitation, data = data, method = \"nearest\")\nmatched_data <- match.data(match_model)\n```\n:::\n\n\n\n\n## Conduct Sensitivity Analysis with Rosenbaum Bounds\n\nWe check how sensitive our results are to hidden bias.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract matched treatment and control outcomes\ntreated_outcomes <- matched_data$species_richness[matched_data$deforestation == 1]\ncontrol_outcomes <- matched_data$species_richness[matched_data$deforestation == 0]\n\n# Perform Rosenbaum sensitivity analysis\npsens(x = treated_outcomes, y = control_outcomes, Gamma = 1.5)  # Adjust Gamma to test sensitivity\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n Rosenbaum Sensitivity Test for Wilcoxon Signed Rank P-Value \n \nUnconfounded estimate ....  1 \n\n Gamma Lower bound Upper bound\n     1           1           1\n\n Note: Gamma is Odds of Differential Assignment To\n Treatment Due to Unobserved Factors \n \n```\n\n\n:::\n:::\n\n\n\n\n## Interpretation of Results\n\n- If the p-value remains small at **Γ = 1.5**, our conclusions are robust to moderate hidden bias.\n- If the p-value becomes insignificant at lower values of **Γ**, our study is highly sensitive to hidden confounders.\n\n## Conclusion\n\nRosenbaum bounds allow us to assess the robustness of our ecological study results against unmeasured confounding. This is particularly useful when controlled experiments are infeasible.\n\n### Take Aways\n\nRosenbaum bounds provide a way to assess how much an unmeasured confounder could influence the causal relationship between deforestation and bird species richness. In ecological studies, where controlled experiments are often impractical, this method helps gauge the reliability of observational findings.\n",
    "supporting": [
      "Sensivity-Test2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}