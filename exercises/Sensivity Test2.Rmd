---
title: "Sensitivity Tests"
author: "Laura Dee"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Learning objectives and take aways
‚úÖ Placebo tests ensure the estimated effect is not driven by spurious correlations.
‚úÖ Sensitivity analyses check if conclusions remain valid under different assumptions.
‚úÖ R provides powerful tools (e.g., EValue, sensemakr, rbounds, placebo regressions) for assessing robustness in causal inference.

```{r echo = FALSE}
# Load necessary libraries
library(tidyverse)
library(lmtest)

# Install and load package
#install.packages("rbounds")
library(rbounds)

#install.packages("EValue")
library(EValue)
library(sensemakr)

library(MatchIt)
library(rbounds)
```

# 1.Placebo Tests in R
Placebo tests are used to check if a treatment effect appears where it should not exist, which can indicate spurious correlation or model misspecification.

A placebo test checks if an effect appears where it should not. In Example 2, we test whether a randomly assigned treatment has a significant effect on student test scores. This helps us determine if our model incorrectly detects an effect due to statistical artifacts, confounding, or model misspecification.

## Example 1: Placebo Test with Pre-Treatment Outcomes 
We check if a policy or treatment has an effect before it was implemented. If significant effects are found before the actual intervention, the causal claim is questionable.

Scenario Examples: A state implements a minimum wage increase in 2018, and we estimate its effect on employment. A placebo test checks if a pre-2018 placebo treatment also shows an effect.

```{r ex1}
# Simulate data
set.seed(123)
data <- tibble(
  state = rep(1:50, each = 10),
  year = rep(2009:2018, times = 50),
  treated = ifelse(state <= 25, 1, 0), # 25 treated states
  post_treatment = ifelse(year >= 2018, 1, 0), # Policy starts in 2018
  employment = rnorm(500, mean = 100, sd = 10) - 2 * treated * post_treatment # Simulated policy effect
)

# Run regression for actual treatment
model_actual <- lm(employment ~ treated * post_treatment + factor(state) + factor(year), data = data)
summary(model_actual)

# Placebo Test: Using 2016 as the fake treatment year
data$placebo_post <- ifelse(data$year >= 2016, 1, 0)
model_placebo <- lm(employment ~ treated * placebo_post + factor(state) + factor(year), data = data)
summary(model_placebo)

# Visualizing results
ggplot(data, aes(x = year, y = employment, color = as.factor(treated))) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~treated) +
  labs(title = "Employment Trends Before and After Policy",
       x = "Year", y = "Employment Level",
       color = "Treatment Group")
```

 *Interpretation:*
If the placebo treatment (2016) shows a significant effect, it suggests that trends or confounding factors, not the actual policy, might be driving results.
If no effect is found, it supports the credibility of the causal effect estimated from the actual policy change.

## Example 2: Placebo Test with Fake Treatment Groups

This placebo test ensures that the observed effect isn‚Äôt due to random chance by testing a fake, randomized treatment assignment.

Scenario: We analyze the effect of an education program on students' test scores, but we randomly assign treatment as a placebo test.

Expanded Explanation:

A real education program is tested to see if it improves student test scores.
We run a placebo test where treatment is randomly assigned to see if the model falsely detects an effect.
If the placebo treatment appears significant, our original causal claim is likely biased.

Why Randomly Assigning the Treatment is a Placebo Test
True Treatment vs. Fake Treatment:
In a real study, a policy or intervention (e.g., an education program) is assigned to students based on specific criteria.
In a placebo test, we replace the real assignment with a completely random assignment.
Key Idea:
If our model finds a significant effect for a randomly assigned treatment, it suggests our original results may be driven by spurious correlations or bias rather than a true causal effect.
Expected Outcome:
A valid causal model should show no significant effect for a placebo (randomly assigned) treatment.
If the placebo treatment is significant, it suggests our main analysis may be biased (e.g., due to confounding, model errors, or random chance).


```{r ex2}
# Simulate Data
set.seed(456)
data <- tibble(
  student_id = 1:1000,
  treated_real = sample(0:1, 1000, replace = TRUE),  # Real treatment
  treated_placebo = sample(0:1, 1000, replace = TRUE),  # Fake treatment
  pre_score = rnorm(1000, 50, 10),  # Pre-treatment test scores
  post_score = pre_score + treated_real * 5 + rnorm(1000, 0, 3)  # Treatment adds 5 points on avg
)

# Real treatment effect
model_real <- lm(post_score ~ treated_real + pre_score, data = data)
summary(model_real)

# Placebo test: Fake treatment group
model_placebo <- lm(post_score ~ treated_placebo + pre_score, data = data)
summary(model_placebo)
```



```{r ex4}
# Set seed for reproducibility
set.seed(456)

# Generate simulated data
data <- tibble(
  student_id = 1:1000,
  treated_real = sample(0:1, 1000, replace = TRUE),  # True treatment assignment
  treated_placebo = sample(0:1, 1000, replace = TRUE),  # Fake (random) treatment
  pre_score = rnorm(1000, 50, 10),  # Pre-treatment test scores
  post_score = pre_score + treated_real * 5 + rnorm(1000, 0, 3)  # True treatment adds 5 points
)
```
treated_real: This represents the actual education program assignment.
treated_placebo: This is a randomly assigned treatment (placebo).
pre_score: Students‚Äô scores before the program.
post_score: Students‚Äô scores after the program, where the real treatment increases scores by 5 points on average.

Step 2: Run the Real Treatment Model
```{r }
# Analyze the effect of the real treatment
model_real <- lm(post_score ~ treated_real + pre_score, data = data)
summary(model_real)
```
This regression estimates the effect of the real education program. We expect a positive, significant coefficient for treated_real, since the real program increases test scores.

Step 3: Run the Placebo Test (Fake Treatment)
```{r }
# Analyze the effect of the fake (randomly assigned) treatment
model_placebo <- lm(post_score ~ treated_placebo + pre_score, data = data)
summary(model_placebo)
```
If the placebo treatment shows no effect, the original study is likely valid.

If the placebo treatment is significant, it suggests that our main analysis may be biased (e.g., by omitted variables, incorrect assumptions, or random noise).

Step 4: Visualize Results
We can plot test scores before and after the placebo treatment to see if any patterns emerge.
```{r viz}
ggplot(data, aes(x = pre_score, y = post_score, color = as.factor(treated_placebo))) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Placebo Test: Fake Treatment vs. Test Scores",
       x = "Pre-Test Score", y = "Post-Test Score",
       color = "Fake Treatment Group")
```
If students who received the fake treatment have significantly different test scores, it suggests an issue in our model.

If the lines for placebo and non-placebo groups overlap, it means the fake treatment has no real effect, supporting the validity of our causal estimate.

Interpretation:

If the placebo treatment (randomly assigned) is statistically significant, it suggests that the main analysis may be biased.
If the placebo effect is insignificant, it strengthens the validity of the causal effect estimate.

# 2. Sensitivity Analyses in R

Sensitivity analyses check how robust our causal conclusions are to potential violations of key assumptions (e.g., unmeasured confounding, model specification).

## Example 3: Sensitivity Analysis for Unmeasured Confounding (E-value Calculation)
The E-value estimates how strong an unmeasured confounder would need to be to explain away the observed effect. We can use the E-value package.

*Scenario:* We estimate the effect of smoking on lung cancer and check how sensitive the result is to unmeasured confounding.

The E-value is a statistical tool used in causal inference to assess the robustness of an observed treatment effect to unmeasured confounding. It quantifies how strong an unmeasured confounder would have to be to fully explain away the observed association between treatment and outcome.

### 1. Why Use E-Values?

In observational studies, treatment and control groups may differ due to unmeasured confounding (e.g., socioeconomic status, genetic factors, environment).
E-values help answer:
How strong must an unmeasured confounder be to reduce our estimated effect to zero (null effect)?

### 2. How Do E-Values Work?

Suppose we estimate the effect of smoking on lung cancer and find a risk ratio (RR) of 2.5.
The E-value tells us how strong an unmeasured confounder must be (in terms of association with both treatment and outcome) to fully explain away the observed RR of 2.5.
If the E-value is large (e.g., 5.0), an extremely strong confounder would be needed to invalidate the result.
If the E-value is small (e.g., 1.5), even a weak confounder could explain away the effect.
3. Formula for E-Value Calculation

###  For a risk ratio (RR) or odds ratio (OR) estimate:


###  4. Interpreting E-Values

E-Value	Interpretation
High (e.g., >4.0)	‚úÖ Very strong confounding needed to explain away the result ‚Üí The effect is robust.
Moderate (e.g., 2.0 - 4.0)	‚ö†Ô∏è Some confounding could reduce the effect, but strong confounders are still needed.
Low (e.g., <2.0)	‚ùå A relatively weak confounder could fully explain away the observed effect ‚Üí The effect is not robust.

###  Assume an estimated risk ratio (RR) of 2.5 with a confidence interval (1.8, 3.2)
```{r } 
help(package = "EValue")
EValue::evalues.RR(2.5, lo = 1.8, hi = 3.2)
EValue::evalues.RR(est = 2.5, lo = 1.8, hi = 3.2)
```
Or do it manually
```{r } 
RR <- 2.5  # Estimated risk ratio
E_value <- RR + sqrt(RR * (RR - 1))
E_value
```

###  Interpretation:
If the E-value is high (e.g., 5+), an extremely strong unmeasured confounder would be needed to fully explain away the effect.
If the E-value is low (e.g., 1.5), the result is more sensitive to hidden confounding.


## Example 4: Sensitivity to Model Specification (Robustness Checks)
We check if the results change under different regression specifications.

Scenario: A study examines whether a new drug reduces blood pressure.

```{r } 
# Simulated Data
set.seed(789)
data <- tibble(
  patient_id = 1:500,
  treated = sample(0:1, 500, replace = TRUE),
  age = rnorm(500, 50, 10),
  bmi = rnorm(500, 25, 4),
  blood_pressure = 120 - treated * 5 + rnorm(500, 0, 5)  # Drug reduces BP by 5 units
)

# Model 1: Basic regression
model1 <- lm(blood_pressure ~ treated, data = data)
summary(model1)

# Model 2: Controlling for age
model2 <- lm(blood_pressure ~ treated + age, data = data)
summary(model2)

# Model 3: Controlling for age and BMI
model3 <- lm(blood_pressure ~ treated + age + bmi, data = data)
summary(model3)
```

Interpretation:

- If the estimated effect remains stable across models, it suggests robustness.
- If the effect changes dramatically, it indicates sensitivity to model specification (potential omitted variable bias).

## Example 5: Rosenbaum Bounds for Sensitivity to Unmeasured Confounding
We test how strong an unmeasured confounder must be to invalidate our results using the rbounds package.

Scenario: We estimate the effect of a scholarship program on college enrollment and check for unmeasured confounding.

```{r } 
# Simulated matched dataset (outcome: enrollment, treatment: scholarship)
set.seed(111)
scholarship <- sample(0:1, 500, replace = TRUE)
enrollment <- scholarship + rnorm(500, 0, 1)

# Wilcoxon signed-rank test for matching
psens(enrollment, scholarship, Gamma = 2)  # Gamma = strength of confounding
```

Interpretation:
If the effect remains significant for large values of Gamma (e.g., 2+), the results are robust.
If the effect disappears for small values of Gamma (<1.2), the findings are highly sensitive to hidden bias.

## Example X: Sensitivity Analysis Using the sensemakr Package in R
The sensemakr package is a powerful tool for sensitivity analysis in causal inference. It helps assess how unmeasured confounders might impact estimated treatment effects.

üìå Step 2: Simulate a Dataset
We create a dataset where:

wages: Worker‚Äôs wage after training
training: Binary variable (1 = received job training, 0 = no training)
education: Years of education (a potential confounder)
experience: Years of work experience (another potential confounder)

Here, we assume the true treatment effect of training on wages is $3 per hour.

üìå Step 3: Run the Initial Regression
We estimate the impact of job training on wages, controlling for education and experience.

```{r } 
# Set seed for reproducibility
set.seed(123)

# Simulate data: Job training impact on wages
n <- 500
training <- rbinom(n, 1, 0.5)  # 50% received training
education <- rnorm(n, mean = 12, sd = 2)  # Avg. 12 years of education
experience <- rnorm(n, mean = 5, sd = 2)  # Avg. 5 years of experience
wages <- 20 + 3 * training + 2 * education + 1.5 * experience + rnorm(n, sd = 5)  # Wage equation

# Create data frame
data <- data.frame(wages, training, education, experience)

# Fit linear regression model
model <- lm(wages ~ training + education + experience, data = data)

# Display model summary
summary(model)
```
Now, check the sensitivity: 

üìå Step 4: Conduct Sensitivity Analysis with sensemakr
We now check if an unmeasured confounder could explain away the observed effect of training.

```{r } 
# Perform sensitivity analysis with sensemakr
sensitivity <- sensemakr(
  model = model,
  treatment = "training",
  benchmark_covariates = c("education", "experience"),  # Use observed confounders as benchmarks
  q = 1,  # Assess robustness to omitted variable bias
  alpha = 0.05  # Significance level
)

# Print sensitivity analysis summary
summary(sensitivity)
```

üìå Step 5: Visualize Sensitivity Results
sensemakr provides a contour plot that shows how strong an unmeasured confounder would have to be to invalidate our conclusions.

üîç How to Interpret the Plot:

The x-axis represents the strength of an unmeasured confounder‚Äôs association with treatment (job training).
The y-axis represents its effect on the outcome (wages).
If the observed effect remains above the significance threshold across plausible values, the result is robust.
```{r } 
# Plot sensitivity results
plot(sensitivity)
```

Step 6: Interpret the Sensitivity Analysis Results
The summary output will show:

R-squared of omitted variable needed to fully explain the effect
How much the observed effect reduces under different confounding scenarios
Whether the treatment effect remains significant under potential omitted variables
If the sensitivity analysis shows that an extremely strong confounder is needed to nullify the effect, we can be confident in our causal estimate. If a moderate confounder could eliminate the effect, we should be cautious in our conclusions.

# Rosenbaum Bounds

Rosenbaum bounds are used in sensitivity analysis for observational studies, particularly in propensity score matching. They assess how sensitive causal inferences are to potential hidden bias due to unmeasured confounders. Since observational studies lack randomized treatment assignment, unmeasured variables may affect both the treatment and the outcome. Rosenbaum bounds estimate how strong an unmeasured confounder would have to be to alter the conclusions of a study.

Concept of Rosenbaum Bounds
The method introduces a sensitivity parameter, Gamma (Œì), which represents the degree of potential hidden bias:

If Œì = 1, the study is free from hidden bias, meaning matched pairs have the same probability of treatment assignment.
If Œì > 1, there is potential unmeasured confounding, and the higher the value of Œì, the more sensitive the results are to hidden bias.
Rosenbaum bounds use Wilcoxon signed-rank tests or Mantel-Haenszel tests to check how the significance of an estimated treatment effect changes under different levels of Œì.

### *Example:* 
I will illustrate Rosenbaum bounds with an ecological example: assessing the effect of deforestation on bird species richness. Suppose we have an observational dataset where sites with deforestation are matched to sites without deforestation based on environmental factors (e.g., elevation, precipitation).


## Introduction

This document demonstrates how to perform **Rosenbaum Bounds Sensitivity Analysis** using R, applied to an ecological study assessing the impact of deforestation on bird species richness.

## Load Required Libraries

```{r}
# Install packages if not already installed
if (!require(MatchIt)) install.packages("MatchIt", dependencies = TRUE)
if (!require(rbounds)) install.packages("rbounds", dependencies = TRUE)

# Load the libraries
library(MatchIt)
library(rbounds)
```

## Simulate Ecological Data

We create a dataset where bird species richness is compared between deforested and non-deforested sites while controlling for elevation and precipitation.

```{r}
set.seed(123)
data <- data.frame(
  deforestation = rep(c(1, 0), each = 50),
  species_richness = c(rnorm(50, mean = 10, sd = 3), rnorm(50, mean = 15, sd = 3)),
  elevation = rnorm(100, mean = 500, sd = 100),
  precipitation = rnorm(100, mean = 1000, sd = 200)
)
```

## Perform Propensity Score Matching

We match deforested and non-deforested sites based on elevation and precipitation.

```{r}
match_model <- matchit(deforestation ~ elevation + precipitation, data = data, method = "nearest")
matched_data <- match.data(match_model)
```

## Conduct Sensitivity Analysis with Rosenbaum Bounds

We check how sensitive our results are to hidden bias.

```{r}
# Extract matched treatment and control outcomes
treated_outcomes <- matched_data$species_richness[matched_data$deforestation == 1]
control_outcomes <- matched_data$species_richness[matched_data$deforestation == 0]

# Perform Rosenbaum sensitivity analysis
psens(x = treated_outcomes, y = control_outcomes, Gamma = 1.5)  # Adjust Gamma to test sensitivity
```

## Interpretation of Results

- If the p-value remains small at **Œì = 1.5**, our conclusions are robust to moderate hidden bias.
- If the p-value becomes insignificant at lower values of **Œì**, our study is highly sensitive to hidden confounders.

## Conclusion

Rosenbaum bounds allow us to assess the robustness of our ecological study results against unmeasured confounding. This is particularly useful when controlled experiments are infeasible.

### Take Aways

Rosenbaum bounds provide a way to assess how much an unmeasured confounder could influence the causal relationship between deforestation and bird species richness. In ecological studies, where controlled experiments are often impractical, this method helps gauge the reliability of observational findings.
