[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "In general, on Mondays we will have brief lectures and demonstrations in R, while on Wednesdays, we will have student-led paper discussions and/or replication exercises of published papers. In the latter half of the course, we will shift to work-shopping our own projects and discussing additional issues and considerations with applied causal inference. In this way, we will use a partially flipped classroom and strive to create a collaborative and inclusive classroom environment to discuss, ask questions, collaborate, and get feedback on your analyses.\nThe tentative schedule of content, subject to change based on student interests and availability of guest speakers, is as follows:\n\n\n\n\nMonday\nWednesday\nGoals/Topics\n\n\n\n\nWeek 1 (1/13)\nCourse Intro\nIntro to causal inference and counterfactual causality\n• Intro to course and goals  • Understand students’ goals for the course  • Set norms and expectations  • Intro to concepts in causal inference and motivation for applying it to ecology and evolution  • Clarify causal versus other research questions and aims  • Brief Introduction to two main frameworks for counterfactual causality (potential outcomes and graphical causal modeling frameworks)\n\n\nWeek 2 (1/20)\nNo class - MLK Day\nIntro to the main frameworks for counterfactual causal inference\nGuest speaker: Dr. Suchinta Arif\n• Structural Causal Models and Directed Acyclic Graphs\n\n\nWeek 3 (1/27)\nIntro to the main frameworks for counterfactual causal inference\nRandomized Controlled Experiments (or RCTs) and experimental design\n• Potential outcomes framework  • Application of potential outcomes framework to RCTs  • Review key assumptions of RCTs  • Critique experimental designs in ecology and identify solutions  • Dissect experimental design with respect to assumptions required for causal inference\n\n\nWeek 4 (2/3)\nObservational data and counterfactuals; Introduction to quasi-experimental methods\nCreating and Analyzing DAGs\n(Guest speaker: [Dr. Zach Laubach](https://laubach.github.io))\nDue: first draft of research question, or lit review proposal, and DAG analysis on 2/7\n• Articulate how and why observational data deviates from assumptions of RCTs  • Understand challenges of applying causal inference to observational data, including confounding and selection bias  • DAGs as a tool from the graphical causal modeling framework  • Principles of covariate and confounder selection \nProject/Workshopping:\n• Students create and workshop DAGs for their own research/study systems\n\n\nWeek 5 (2/10)\nPre-regression matching\nPaper discussion and replication exercise\n• Learn pre-regression matching as a method  • Demo application in R  • See & critique how it is applied in the literature\n\n\nWeek 6 (2/17)\nDifference in Difference (DiD)\nPaper discussion and replication exercise\n• Learn DiD designs, including identification assumptions and interpretation  • Demo application in R  • Compare DiD to matching and experiments  • See & critique how it is applied in the literature\n\n\nWeek 7 (2/24)\nPanel methods continued: Two way fixed effects and extensions\nPaper discussion and replication exercise - Class on zoom\nDue Draft of DAG OR literature review proposal (2 page max.)\n• Learn within estimators (two-way fixed effects) including identification assumptions and interpretation  • Compare panel designs with conditioning on observables designs and with random effect/mixed effect models in R and understand the differences in assumptions  • Compare applications in literature and the assumptions required for the conclusions drawn\n\n\nWeek 8 (3/3)\nSynthetic Control (Asia Kaiser)\nExtensions of diff-in-diff and fixed effects\n(Dr. Jarrett Brynes)\nOn zoom\n• Learn synthetic control including identification assumptions and interpretation  • Compare it to other types of designs\n\n\nWeek 9 (3/10)\nInstrumental Variables (IV)\nIV Paper discussion\n• Learn IV as a method  • Demo application in R  • See & critique how it is applied in the literature\n\n\nWeek 10 (3/17)\nOne-on-one consultations on projects on zoom\nRegressionDiscontinuity Designs (RDD)\n• Make progress on project and get feedback on proposed research design and challenges so far from Laura and your classmates\n• Learn RDD as a method\n\n\nWeek 11 (3/24)\nSpring break (no class)\nSpring break (no class)\n\n\n\nWeek 12 (3/31)\nRDD Paper discussion\nDUE: In class - brief presentation on project\n• Demo application in R  • See & critique how it is applied in the literature \n• Make progress on project and get feedback on proposed research design and challenges so far from Laura and your classmates\n\n\nWeek 13 (4/7)\nProject presentations continued; Intro to robustness checks\nComparison of study designs cont’d; Sensitivity tests\n• Articulate, compare, and parse the assumptions, estimands, and generalizability of different designs  • Intro to Sensitivity Tests\n\n\nWeek 14 (4/14)\nMediation Analysis\nPower, Inference, and Robust and Clustered Standard Errors\n• Introduction to mediation analysis  •  Articulate additional assumptions for mediation •  • Standard errors, power, and inference with quasi-experimental methods\n\n\nWeek 15 (4/21)\nSpecial topic: Generalizability (Guest speaker: Dr. Becks Spake)\nProject presentations\n• Introduction to challenges with generalizability\n•  Crititically consider Generalizability, transportability, and approaches to increase them  • Clearly communicate the application of causal inference methods to a research question in ecology OR present a topic we did not cover as a class\n\n\nWeek 16 (4/28)\nProject presentations\nMay 4th: Final Write up due by email\n• Clearly communicate the application of causal inference methods to a research question in ecology OR present a topic we did not cover as a class\n\n\n\n\nPotential additional topics/units to be voted on by the class:\n\nGeneralizability in experimental and observational studies\n\nKorell et al. 2019\nSpake et al. 2022\nSpake et al. 2021\n\n\n\nHeterogeneous treatment effects and conditional average effects\ne.g. Causal Forests\n\n\nDeeper dive into mechanisms and mediation analysis\n\n\nDeeper dive into Sensitivity Tests\n\n\nReplication/Reproducibility/Pre-registration\ne.g. Kimmel et al. 2023 Empirical evidence of widespread exaggeration bias and selective reporting in ecology",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EBIO 5460: Causal Inference in Ecology",
    "section": "",
    "text": "Laura Dee, Associate Professor, CU Boulder (laura.dee@colorado.edu)",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#instructors",
    "href": "index.html#instructors",
    "title": "EBIO 5460: Causal Inference in Ecology",
    "section": "",
    "text": "Laura Dee (laura.dee@colorado.edu)",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#class-meetings",
    "href": "index.html#class-meetings",
    "title": "EBIO 5460: Causal Inference in Ecology",
    "section": "Class Meetings",
    "text": "Class Meetings\nMondays & Wednesdays 3:35-4:50pm in Physics DUAN G135A\n\nOffice Hours\nWednesday 2:15-3:15 pm and by appointment",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "EBIO 5460: Causal Inference in Ecology",
    "section": "Course Description",
    "text": "Course Description\nHow does biodiversity effect ecosystem functioning? What are the consequences of temperature variability for parasite loads and host populations? How do plant communities respond to drought, and which ecosystems respond most? Do restoration and conservation work? How does climate change affect disturbance regimes? Cause and effect questions like these motivate much of the empirical work in basic and applied ecological and environmental sciences. Does X cause Y? If X causes Y, does it cause Y in all situations? Through what mechanisms does X cause Y? If X causes Y, how large is the effect of X on Y and how does the size compare to other causes of Y? To answer these cause-and-effect questions, a counterfactual model of causality and a unified methodological framework has been developed yet is not often taught or emphasized in ecology and EBIO more generally. Now this framework is the predominant approach for causal inference in a diversity of fields including public health, economics, and computer science (which is also referred to as program evaluation or impact evaluation).\nGiven the increasing access to spatiotemporally large observational datasets (e.g., remotely sensed data, data from long-term ecological monitoring), we have unprecedented opportunities to answer questions about causal relationships and mechanisms outside of the context of traditional ecological approaches to experimental design. Deriving causal inferences from observational data presents its own set of challenges, and this course presents statistical methods to overcome challenges for causal inference in both observational and experimental study designs.\nThis class aims to teach students to apply and interpret the counterfactual causality model and associated methods in answering empirical questions in basic and applied ecology. As a 3-credit graduate course, this course has a corresponding reading load with an emphasis on readings that elucidate the intuition and the application of the core conceptual ideas. We are firm believers that the most fundamental principles can be stated in plain English and conceptual understanding is just as important as the math. Thus the course stresses intuition (in English) over mechanics and proofs. Nevertheless, students will be expected to apply the mechanics in replication exercises in R statistical software and in a final a project related to the students’ thesis.\nWhether you are a student with substantial graduate work in empirical methods or a student with only the basic pre-requisites covered (some introductory statistics or biostatistics is required), you should expect to gain a deeper understanding of approaches to answering causal questions and of the nature of evidence. Importantly, you will see more clearly the conceptual connections among the various approaches to estimating causal effects – experiments and observational analyses. Even for students with substantial coursework in statistics, these connections are often missed.\nWhile most students who will take this class are in EBIO fields, you should expect to do readings outside of EBIO areas and to see examples from other fields – because ecology and evolutionary biology is lagging a bit behind other fields in several areas we will cover (but catching up fast!). On the bright side, reading beyond the typical EBIO papers will provide you with an entry point into the broader literature on causal inference, stemming from economics, public health, epidemiology, political science, and other fields. Thus, we see the use of broader readings and examples as intentional to meet the course’s learning objectives, and as a potential competitive advantage – and a way to expand your thinking – rather than a distraction from topics that interest you most. In our experience, gaining perspective on methods and language used in other fields will also provide you with skills to foster collaborations across disciplines. We will also invite guest speakers who are experts to showcase applications of the theory and approaches to questions and data in ecology, conservation, and evolution/behavior – and present some of our in-progress research using causal inference approaches applied to EBIO.\nWe have outlined a provisional syllabus below, but we can adapt it based on student interests and background. The main emphasis of the course is like any other graduate course: to encourage students to think critically, to speak and write simply and clearly, collaborate, to own and use a body of facts and ideas that are widely known, to detect errors and fallacies, to resolve intellectual problems, and to advance our collective knowledge through independent research and applied statistics.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#course-prerequisites",
    "href": "index.html#course-prerequisites",
    "title": "EBIO 5460: Causal Inference in Ecology",
    "section": "Course Prerequisites",
    "text": "Course Prerequisites\nSome introductory statistics or biostatistics that covers hypothesis testing and regression is required. Some familiarity with R as a programming language is also recommended. This course will be most beneficial to students who have started their thesis work and can incorporate an empirical analysis into the class project, but it is open to all students who meet the pre-requisites. Contact the professors if you are unsure whether your background is sufficient for the course.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#course-objectives",
    "href": "index.html#course-objectives",
    "title": "EBIO 5460: Causal Inference in Ecology",
    "section": "Course Objectives",
    "text": "Course Objectives\nThrough this course, students will gain:\n\nAn understanding of the main frameworks for counterfactual causal inference and how causal inference differs from other empirical research aims\nFamiliarity with how causal inference is applied in experimental and quasi-experimental study designs in ecology and evolution\nExperience reading the published literature in ecology and evolutionary biology with a critical eye towards appropriate use of methods for identifying causal relationships and mechanisms.\n\nThis course aims for students to learn how to:\n\nAsk better causal questions\nSummarize key threats to causal inference and identify these threats when evaluating their own and published study designs\nApply causal inference methods to real world research questions and datasets – either through applications to their thesis work or to other datasets – to mitigate threats to causal inference through research design\nIdentify the most appropriate study design(s) and methodology in experimental and non-experimental settings in light of the available data and the research question\nIdentify the target population or subpopulation for which you want to make inferences\nImplement these designs and methods in R and appropriately interpret the results and their potential biases and assumptions\nConduct robustness checks and sensitivity analysis because our methods are often imperfect and assumptions can be violated\nWrite and speak clearly about these methods, the assumptions they need for causal inference, and the results they yield.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#code-data",
    "href": "index.html#code-data",
    "title": "EBIO 5460: Causal Inference in Ecology",
    "section": "Code & Data",
    "text": "Code & Data\nI will make RMarkdown Files, datasets, and code available on this website and through a shared (private) Google Drive for data I don’t have permissions to post.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#a-note-on-inclusion",
    "href": "index.html#a-note-on-inclusion",
    "title": "EBIO 5460: Causal Inference in Ecology",
    "section": "A Note on Inclusion",
    "text": "A Note on Inclusion\nSome of the texts that we will read in class use examples to illustrate their points that are problematic (e.g., treating gender as a binary or studying post-colonial economic development without considering the violence of colonialism). We do not agree with the assumptions underlying these examples and we acknowledge the problems with them. The descriptions of methods are still some of the easiest to read out there. At the same time, policy decisions are being made based on these and similar analyses, so we need to train people who can use these methods for causal inference and approach this work through a lens of diversity, equity, inclusion, and justice.\nIn this class, we aim to foster discussions of how bias shapes causal models in science, the questions we ask, the analyses we do, who does them, and our interpretation of these results. We are also always open to feedback on these subjects.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "exercises/matching.html",
    "href": "exercises/matching.html",
    "title": "Matching",
    "section": "",
    "text": "In class on Monday, we will walk through a demonstration of how to use matching in R.\nYou can access the R Markdown file for this coding demonstration if you’d like to follow along."
  },
  {
    "objectID": "exercises/matching.html#matching-demo-in-r",
    "href": "exercises/matching.html#matching-demo-in-r",
    "title": "Matching",
    "section": "",
    "text": "In class on Monday, we will walk through a demonstration of how to use matching in R.\nYou can access the R Markdown file for this coding demonstration if you’d like to follow along."
  },
  {
    "objectID": "exercises/Panel_demo/RMarkdown_panel_designs_Dee_and_Severen.html",
    "href": "exercises/Panel_demo/RMarkdown_panel_designs_Dee_and_Severen.html",
    "title": "Panel Data and Fixed Effects Designs for Ecology",
    "section": "",
    "text": "The data used in this tutorial comes from the Nutrient Network. It has been cleaned and processed and is ready to use. The data correspond to what we use in the paper for the main analysis, and consists of control plots from 43 NutNet sites. Each site includes one or more control plots that we use. We only use plots for which we have at least 5 years of data.\nWe’ll primarily use the fixest package for analysis; the other packages (tidyverse, data.table, lme4) aid data wrangling and comparing research designs.\n\n\nCode\n# library(tidyverse) #v 1.3.2\n# library(data.table) # v 1.14.6\n# library(fixest) # v 0.11\n# library(lme4)  # Need at least version 1.1-26\n# library(knitr) #v1.41\n# library(ggplot2) # v 3.4.0\n# \n# cdir &lt;- getwd()\n# comb &lt;- fread(paste(cdir,\"/cleaned_comb_data.csv\",sep=\"\"),na.strings='NA')\n# comb$V1 = NULL\n# comb$site &lt;- comb$site_code\n# ## Make dummy variables for panel regression ##\n# # make year a character, to be a dummy variable: \n#  comb$year &lt;- as.character(comb$year)\n# #make a factor that is site by year\n# comb[, site.by.yeardummy := paste(site_code, year, sep = \"_\")]\n\n\nThe comb dataset contains a wide number of variables; we’ll mainly be using the variables live_mass and rich. Also important are the site_code, plot, and year variables, which we use to create fixed effects (site-by-year fixed were created with \"r comb[, site.by.yeardummy := paste(site_code, year, sep = \"_\")]\")."
  },
  {
    "objectID": "exercises/Panel_demo/RMarkdown_panel_designs_Dee_and_Severen.html#simple-correlations-in-single-years",
    "href": "exercises/Panel_demo/RMarkdown_panel_designs_Dee_and_Severen.html#simple-correlations-in-single-years",
    "title": "Panel Data and Fixed Effects Designs for Ecology",
    "section": "Simple correlations in single years",
    "text": "Simple correlations in single years\nLet’s begin by looking at simple correlations. We will examine both the overall correlation and correlations within a single year (i.e., using cross-sectional variation). We proceed with a linear regression framework @ref(eq:eq1), using the log of productivity (measured as live aboveground biomass) as the outcome and the log species richness as the explanatory variable. We’ve found that a log-log specification captures the nature of the relationship in our data well (see Figure @ref(fig:graphbiomasssp) below and the paper for details).\nWe initially estimate and report \\(\\beta\\) in: \\[\\begin{equation}\n\\ln(\\text{Live Mass}_{pst}) = \\alpha + \\beta \\ln(\\text{Richness}_{pst}) + e_{pst}\n(\\#eq:eq1)\n\\end{equation}\\] where \\(p\\) indexes plots, \\(s\\) indexes sites, and \\(t\\) indexes years. The unobserved error term is \\(e_{pst}\\). We include a constant \\(\\alpha\\), but do not report estimates of it as it tells us little.\nWe report results below using all years of data first, and then report results using two individual years: 2012 and 2013. Here, as everywhere below, we cluster standard errors by plot to reflect serial correlation in errors terms within a plot across years (we do not assume that errors are iid). Note that when we use only a single year of data, this is equivalent to using heteroskedasticity-robust errors.\n\n\nCode\n# SimpleCorrAll &lt;- feols(log(live_mass) ~ log(rich), comb, cluster = \"newplotid\") \n# \n# SimpleCorr2012 &lt;- comb %&gt;%\n#   filter(year==2012) %&gt;%\n#   feols(log(live_mass) ~ log(rich), ., cluster = \"newplotid\") \n# \n# SimpleCorr2013 &lt;- comb %&gt;%\n#   filter(year==2013) %&gt;%\n#   feols(log(live_mass) ~ log(rich), ., cluster = \"newplotid\") \n# \n# etable(SimpleCorrAll, SimpleCorr2012, SimpleCorr2013, \n#           cluster = \"newplotid\", \n#           drop = \"Intercept\", \n#           subtitles = c(\"Data All Years\", \"Data in 2012\", \"Data in 2013\"))  \n\n\nAs you can see, using all years of data gives a non-significant positive relationship between productivity and richness. In just the 2012 data, the coefficient is larger in magnitude, but still not significant. Finally, just using 2013 data, the coefficient switches signs and becomes significant.\nSo… which one to believe? Well, probably none of these, because they likely do not identify the true causal effect of richness on productivity. Their variability highlights that these estimates, which rely on non-experimental cross-sectional data, are likely contaminated by omitted variable bias.\nWhen does \\(\\hat{\\beta}\\) capture a causal relationship? When there are no unobservables that are correlated with richness that also influence productivity: \\(\\mathbb{E}[e_{pst} \\times \\ln(\\text{Richness}_{pst})]=0\\) (i.e., \\(e_{pst}\\) and \\(\\ln(\\text{Richness}_{pst})\\) aren’t correlated). In the above results, there’s probably stuff in \\(e\\) that is correlated with richness, like precipitation, disturbance, land-use history, soil characteristics, and other characteristics of sites and plots.\nA Directed Acyclic Diagram (DAG) can help us see the challenges of omitted, and potentially confounding, variables more clearly. In the above analyses from equation @ref(eq:eq1), \\(\\beta\\) is only identified if we assume that any variables that matter but that we omitted are uncorrelated with richness. One benefit of a DAG is that it makes transparent the assumptions on which one relies for making causal claims from observable data. A DAG therefore allows the researcher and the reader to better judge the credibility of the causal claims from a specific research design. Another way to view this benefit is that a causal graph helps identify the sources of variation in a causal variable and in its outcome, thereby emphasizing potential sources of bias that must be addressed in a research design and pointing to designs that can address these sources of statistical bias [@morganCounterfactualsCausalInference2014].\nIn our case, many variables that are correlated with biodiversity can also drive productivity (Fig. 1B – main text), creating the possibility for rival explanations and biased estimates for estimated effects from observations. For example, climatic conditions, soil nutrients, evolutionary history, and historic contingency during community assembly are all related to both productivity and biodiversity [@Loreau1998;@fukamiCommunityAssemblyAlternative2011;@graceIntegrativeModellingReveals2016]. With a common driver of both variables that is not included in a model, two variables (i.e., biodiversity and an ecosystem function) may be correlated, even when there is no causal relationship between them. Similarly, no correlation between variables does not imply a lack of causation. Causal relationships can also be masked when examining correlations, due to an omitted variable (e.g., nitrogen addition), which positively affects productivity but negatively affects richness [@isbellNutrientEnrichmentBiodiversity2013]. Models that do not control for that common driver will consequently tend to give estimates that do not correspond to causal effects of biodiversity on productivity (or vice versa).\n\n  \n\nFigure 1B (main text – right panel) is known as a directed acyclic causal graph (DAG) and is a visualization of qualitative causal assumptions ([@pearl2009causality; @pearl2011aspects;@fieberg2012understanding; @schoolmaster2013causal; @schoolmaster2020graphical; @Grace2020]). A DAG encodes knowledge and beliefs about how a system works. The graphical relations depicted in the DAG encode causal claims – not just representations of associations. A directed edge (e.g., R –&gt; P) depicts a claim about the results of many hypothetical experiments, whereby if every other variable represented in the graph is held fixed, R and P will covary if R if manipulated, but not if P is manipulated (note, a DAG assumes that one can isolate the effect of R on P, but does not imply that P can never affect R; another DAG may represent the reverse direction, P –&gt; R).\n\nTAKEAWAY:\nOmitted variable bias is a pervasive feature in observational analysis, and the assumptions that permit identification of causal effects are unlikely to hold when using cross-sectional variation."
  },
  {
    "objectID": "exercises/Panel_demo/RMarkdown_panel_designs_Dee_and_Severen.html#common-ecological-design---multivariate-regression",
    "href": "exercises/Panel_demo/RMarkdown_panel_designs_Dee_and_Severen.html#common-ecological-design---multivariate-regression",
    "title": "Panel Data and Fixed Effects Designs for Ecology",
    "section": "Common Ecological Design - Multivariate Regression",
    "text": "Common Ecological Design - Multivariate Regression\nOf course, in the above correlations, we include plots in sites from across the world, implicitly comparing grasslands in warmer climates with those in cooler ones, or wetter with dryer, or Europe with the Americas. There are a lot of differences between these places!\nA common response to this problem is to try to measure these differences and include them in the model. In the causal inference literature, this is known as “conditioning on observables” or Pearl’s back-door criteria. Conditioning on observables is convenient but makes strong assumptions for causal inference, namely the “Selection on Observables” Assumption. Informally, this assumption implies that confounding variables that could introduce bias into a design are known and observable to the researcher. The bias they introduce into an estimator can be eliminated (controlled, blocked) by conditioning strategies, such as regression, matching, or stratification methods. To read more, see [@morganCounterfactualsCausalInference2014]. We can visualize this assumption by modifying our DAG, and using some examples in R:\n\n  \n\nTo explore the consequences of adding in covariates, we show the results of five models below. The first column repeats the first column from above. The second column adds in soil chemistry covariates, the third column instead adds weather covariates, and the fourth instead adds management variables plus habitat. The last columns adds in everything. For the purposes of this tutorial, we only show coefficient estimates for richness in the following table, even though the other terms are included in the model.\n\n\nCode\n# SoilCovars &lt;- feols(log(live_mass) ~ log(rich) +\n#                       pct_C + pct_N + ppm_P + ppm_K + ppm_Na + ppm_Mg + ppm_S + ppm_Na + ppm_Zn +  ppm_Mn +  ppm_Fe + ppm_Cu + ppm_B +\n#                       pH + PercentSand + PercentSilt + PercentClay, \n#                     comb, cluster = \"newplotid\") \n# \n# WeatherCovars &lt;- feols(log(live_mass) ~ log(rich) +\n#                          elevation + TEMP_VAR_v2 + MIN_TEMP_v2 + MAX_TEMP_v2 + TEMP_WET_Q_v2 + TEMP_DRY_Q_v2 + TEMP_WARM_Q_v2 + \n#                          TEMP_COLD_Q_v2, \n#                        comb, cluster = \"newplotid\") \n# \n# MgmtCovars &lt;- feols(log(live_mass) ~ log(rich) +\n#                       as.factor(habitat) + managed + burned + grazed + anthropogenic, \n#                     comb, cluster = \"newplotid\") \n# \n# AllCovars &lt;- feols(log(live_mass) ~ log(rich) +\n#                      pct_C + pct_N + ppm_P + ppm_K + ppm_Na + ppm_Mg + ppm_S + ppm_Na + ppm_Zn +  ppm_Mn +  ppm_Fe + ppm_Cu + ppm_B +\n#                      pH + PercentSand + PercentSilt + PercentClay +\n#                      elevation + TEMP_VAR_v2 + MIN_TEMP_v2 + MAX_TEMP_v2 + TEMP_WET_Q_v2 + TEMP_DRY_Q_v2 + TEMP_WARM_Q_v2 +\n#                      TEMP_COLD_Q_v2 + as.factor(habitat) + managed + burned + grazed + anthropogenic, \n#                    comb, cluster = \"newplotid\") \n# \n# etable(SimpleCorrAll, SoilCovars, WeatherCovars, MgmtCovars, AllCovars,\n#           cluster = \"newplotid\", \n#           drop = \"!rich\", \n#           subtitles = c(\"Data All Years\", \"+ Soil\", \"+ Weather\", \"+ Management\", \"+ All\")) \n\n\nEstimates jump around depending on which covariates are used! This is likely a sign of some sort of omitted variables bias. Even though we consecutively explain more and more of the variation in the data, we are not necessarily any closer to a causal relationship.\nThere are a few other points to make about the above results that speak the practice of science.\n1.First, the number of observations changes based on which controls we use. Columns 2 and 5, where we only have 675 observations, might be on highly selected and not representative of the overall populations under study. A careful analysis could do any of a number of approaches to control for that, but far too often we just say ``This is what I have (all the) data for, so this what I estimate.’’ It would be nice if there were a way to move forward even if we were unable to collect data on all the variables we wanted.\n2.Second, and more perniciously, is specification hunting. What’s to keep from only displaying results in columns 2 and 5 above, with no mention of the other results. P-hacking is something we should all be concerned about, but it’s really easy to twiddle and play until one gets just the right set of results that agree with ones hypothesis.\n3.Lastly, we assumed that all the covariates entered linearly and did not interact. What if what really matters is the interaction of nitrogen and precipitation? How do we capture that? We could include interactions and quadratics, or splines, but at some point, the number of covariates will exceed our sample size and nothing is identified.\n\nTAKEAWAY:\nIt is hard to assume the we observe and correctly control for all confounding variables when analyzing cross-sectional data"
  },
  {
    "objectID": "exercises/Panel_demo/RMarkdown_panel_designs_Dee_and_Severen.html#plot-fixed-effects",
    "href": "exercises/Panel_demo/RMarkdown_panel_designs_Dee_and_Severen.html#plot-fixed-effects",
    "title": "Panel Data and Fixed Effects Designs for Ecology",
    "section": "Plot Fixed Effects",
    "text": "Plot Fixed Effects\nLet’s ignore sites for a minute, and just think about the plots that lie in a single site. We’re going to estimate the following model: \\[\\begin{equation}\n\\ln(\\text{Live Mass}_{pt}) = \\beta \\ln(\\text{Richness}_{pt}) + \\delta_p + \\mu_t + e_{pt}\n\\end{equation}\\] where we’ve added the term \\(\\delta_p\\). This represents a vector of plot-specific fixed effect—a dummy variable for each plot. We also add time fixed effects (\\(\\mu_t\\), a dummy for each year) to control for the common differences to all plots in a year (in a site). We’ll touch on that more later, but really, the plot fixed effects are of greatest consequence.\nWhat does adding this vector of plot dummy variables do? Two big things. First, it controls for any and all time-invariant features of the plot, whether we observe them or not!!! To see this, imagine putting in a variable \\(x_p\\) into the above equation linearly with the coefficient \\(\\gamma\\). We wouldn’t actually be able to estimate \\(\\gamma x_p\\); it’s already a component of \\(\\delta_p\\). Don’t know what functional for you should use for \\(x_p\\) or whether it should be interacted with another variable? That’s fine, that’s already included in \\(\\delta_p\\)! We get a whole lot for the inclusion of this variable. Let’s see our updated DAG: now we have removed confounding effects from plot-level attributes whether we can measure them or not!\n\n  \n\nSecond, and most importantly conceptually, is that we are no longer directly comparing different plots; we aren’t using cross-sectional variation any more. Instead, we are using variation in richness and productivity within the same plot over time. So, we’re implicitly comparing a plot in year \\(t\\) with this same plot in year \\(t+k\\) for some \\(k\\). Another way to see this is that we could write a very similar equation in differences (ignore the \\(\\mu_t\\) for a moment): \\[\\begin{equation}\n\\left(\\ln(\\text{Live Mass}_{pt})-\\ln(\\text{Live Mass}_{pt-1}) \\right)= \\beta \\left( \\ln(\\text{Richness}_{pt}) - \\ln(\\text{Richness}_{pt-1}) \\right) + \\left( e_{pt} - e_{pt-1}\\right)\n\\end{equation}\\] Where did \\(\\lambda_p\\) go? Well, \\(\\lambda_p-\\lambda_p=0\\), so we don’t need it. (NB: We could also subtract the mean of each variable over time within each plot and arrive at a similar estimator. There are subtle differences between the two approaches that depend on the nature of the error terms \\(e\\), but they draw on the same source of variation).\nWhat do we have to assume for a causal interpretation? There are a couple of different assumptions we could choose; I think it’s easiest to frame it like this: \\(\\mathbb{E}[ (e_{pt} - e_{pt-1}) \\times (\\ln(\\text{Richness}_{pt}) - \\ln(\\text{Richness}_{pt-1}))]=0\\). That is, changes in richness are uncorrelated with changes in unobserved determinants of richness. Because time-invariant unobservable variables do not change, they are no longer a concern! Instead, we’re concerned if movements in some unobserved factor could both be driving our outcome variable and be correlated with richness.\nWhat’s the cost? Well, there are a few to consider, but some really aren’t much of a restriction:\n\nWe use to worry about computational issues. Instead of differencing, we could have including the additional fixed effects as regressors. Were this a large vector, computation could have become difficult. This is rarely a concern thanks to better computers and better techniques.\nWe need longitudinal (panel) data (or repeated cross sections in some special circumstances). This is why economists get so much use out of administrative data! This data is rarer in ecology, but with the growth in LTERs and other multi-year sites, this will be less of a constraint.\nOur target of interest has to be time-varying. Any interesting time-invariant factors have been removed from the equation (literally).\n\nFigures @ref(fig:graphrawvary) and @ref(fig:graphdeplotFE) illustrate graphically what the plot fixed effects do to the outcome variable (productivity). Figures @ref(fig:graphrawvary) is just the raw data, and shows log(live mass) in four plots split between two sites (at the Sedgwick Reserve [sedg.us] and at the Sevilleta LTER [sevi.us]). Sedgwick has higher productivity on average. The productivity at these sites also appear to be following different trajectories through time (e.g., note the dip in productivity at Sevilleta in 2009). The plot fixed effects remove the average productivity in each site, as shown in Figure @ref(fig:graphdeplotFE). They do not remove site-and-year specific sources of confounding variation (e.g., if a more extreme drought happened at Sevilleta than at Sedgwick in 2009 affecting both productivity and richness); we turn to eliminating site and year specific confounding variables below in @ref(fig:graphdeplotsiteyearFE).\nTo the statistical model: We’re first going to estimate the following equation site-by-site on the five sites with the largest number of observations (in terms of the number plot-years we observe; see Table S1). \\[\\begin{equation}\n\\ln(\\text{Live Mass}_{pt}) = \\beta \\ln(\\text{Richness}_{pt}) + \\delta_p + \\mu_t + e_{pt}\n\\end{equation}\\] The year fixed effects \\(\\mu_t\\) control for time-varying factors (observed or unobserved) that affect all plots at the site under consideration. For example, suppose 2007 was a particularly damp and rainy year at the site; \\(\\mu_t\\) controls for the average impact of that across all plots. Because what happens at one site in a year is probably very different from what happens at a different site in the same year, we estimate these separately for each site. This will make the point estimates for each site less precise (especially because we’re clustering by plot), but this is just for illustration’s sake.\n\n\nCode\n# PlotFE_1 &lt;- comb %&gt;%\n#   filter(site_code==\"cdcr.us\") %&gt;%\n#   feols(log(live_mass) ~ log(rich) | newplotid + year, ., cluster = \"newplotid\")\n# \n# PlotFE_2 &lt;- comb %&gt;%\n#   filter(site_code==\"cdpt.us\") %&gt;%\n#   feols(log(live_mass) ~ log(rich) | newplotid + year, ., cluster = \"newplotid\")\n# \n# PlotFE_3 &lt;- comb %&gt;%\n#   filter(site_code==\"koffler.ca\") %&gt;%\n#   feols(log(live_mass) ~ log(rich) | newplotid + year, ., cluster = \"newplotid\")\n# \n# PlotFE_4 &lt;- comb %&gt;%\n#   filter(site_code==\"sedg.us\") %&gt;%\n#   feols(log(live_mass) ~ log(rich) | newplotid + year, ., cluster = \"newplotid\")\n# \n# PlotFE_5 &lt;- comb %&gt;%\n#   filter(site_code==\"sier.us\") %&gt;%\n#   feols(log(live_mass) ~ log(rich) | newplotid + year, ., cluster = \"newplotid\")\n# \n# etable(SimpleCorrAll, PlotFE_1, PlotFE_2, PlotFE_3, PlotFE_4, PlotFE_5,\n#           cluster = \"newplotid\", \n#           drop = \"!rich\", \n#           subtitles = c(\"Data All Years\",\"US - CDCR\", \"US - CDPT\", \"CA - Koffler\", \"US - SEDG\", \"US - SIER\" )) \n\n\nWe again show the bivariate correlation on all sites first (SimpleCorrAll), and then the estimate for each site. Whoa! Now we’re getting some negative coefficients (though mostly insignificant due to smaller effective sample sizes). We’re controlling for lots and lots of things that we couldn’t control for before, either because we didn’t think to include them or we couldn’t collect data on them. The R-squared values confirm that this is the case; we’re generally explaining much more of the data than before (but note: R-squared values are NOT important for causal interpretations generally). Note that, with the plot fixed effects, we do not have much statistical power estimating sites individually.\n\nTAKEAWAY:\nUsing unit fixed effects in panel data shifts the identifying variation from across units to within units over time."
  },
  {
    "objectID": "exercises/Panel_demo/RMarkdown_panel_designs_Dee_and_Severen.html#bringing-it-all-together-with-site-by-year-fixed-effects",
    "href": "exercises/Panel_demo/RMarkdown_panel_designs_Dee_and_Severen.html#bringing-it-all-together-with-site-by-year-fixed-effects",
    "title": "Panel Data and Fixed Effects Designs for Ecology",
    "section": "Bringing it all Together with Site-by-Year Fixed Effects",
    "text": "Bringing it all Together with Site-by-Year Fixed Effects\nWe now combine all sites together to give us more statistical power to detect effects. We do want to account for the fact that different sites experience different conditions in different years. To do so in a flexible way, we include site-by-year fixed effects, \\(\\mu_{st}\\). \\[\\begin{equation}\n\\ln(\\text{Live Mass}_{pst}) = \\beta \\ln(\\text{Richness}_{pst}) + \\delta_p + \\delta_{st} +  e_{pst}\n\\end{equation}\\] These additional fixed effects control for all time-varying effects that impact the site as whole (i.e., that apply to all the plots equally). Thus, they capture the first order effects of weather, among other factors that could shift outcomes for the site as whole. This gives us sufficient power to conduct conservative inference on our estimated average treatment effect.\nTo get a sense for what these site-by-year effects do, first recall Figure @ref(fig:graphdeplotFE). Plots that are in the same site seem to have similar movements in productivity over time, even after controlling for plot fixed effects. The site-by-year fixed effects remove the average of everything that happens across the site in the data in a year (e.g., a drought at a site). Figure @ref(fig:graphdeplotsiteyearFE) removes this variation; see how the big drop in Sevilleta live mass in 2009 is much less Figure in @ref(fig:graphdeplotsiteyearFE).\nTo provide confidence that the results are robust, we will also include a couple of time-varying controls, evenness and lagged richness. NB: To make sure we don’t drop locations with values of zero evenness, we use the inverse hyperbolic sine instead of the natural log. Note that we don’t need to worry about that for productivity or richness because they never take a zero value.\n\n\nCode\n# ihs &lt;- function(x) {\n#   y &lt;- log(x+sqrt(x^2 + 1))\n# }\n# \n# \n# MainMod_Rich     &lt;- feols(log(live_mass) ~ log(rich)  | newplotid + site.by.yeardummy, comb) \n# MainMod_RichEven &lt;- feols(log(live_mass) ~ log(rich) + ihs(even) | newplotid + site.by.yeardummy, comb) \n# MainMod_RichLag  &lt;- feols(log(live_mass) ~ log(rich) + log(laggedrich) | newplotid + site.by.yeardummy, comb) \n# MainMod_RichEvenLag &lt;- feols(log(live_mass) ~ log(rich) + log(laggedrich) + ihs(even) | newplotid + site.by.yeardummy, comb)\n# \n# etable(MainMod_Rich, MainMod_RichEven, MainMod_RichLag, MainMod_RichEvenLag,\n#           cluster = \"newplotid\")\n\n\nAs you can see, estimate on log richness are relatively stable across different specifications. Of special note: the coefficient on lagged richness (richness from the year before) is small and insignificant, given us confidence that our results reflect contemporaneous movement in richness, and not some factor that is also correlated with last year’s richness.\n\nTAKEAWAY:\nFixed effects can alter the research design and control for a wide variety of potentially confounding factors!\n\nOur updated DAG, including both plot and site-by-year fixed effects, show that we can control more flexibly for a broader set of confounding variables in space and time – critically, whether they are measured or not!"
  },
  {
    "objectID": "exercises/matching_demo.html",
    "href": "exercises/matching_demo.html",
    "title": "Matching in R",
    "section": "",
    "text": "Code to demonstrate matching in R. Adapted from the supplementary materials from Butsic, V. et al. (2017): Quasi-experimental methods enable stronger inferences from observational data in ecology. (c) Matthias Baumann (2017-01-10).\nIn the Butsic et al. paper, they used the example of the impact of wildfire on species richness. Here, we will simulate data with a known treatment effect of fire on species richness. We will then compare the estimated effect we get through a naive ordinary least squares (OLS) regression approach to the effect we estimate when we use matching methods to control for observable confounding variables."
  },
  {
    "objectID": "exercises/matching_demo.html#description",
    "href": "exercises/matching_demo.html#description",
    "title": "Matching in R",
    "section": "",
    "text": "Code to demonstrate matching in R. Adapted from the supplementary materials from Butsic, V. et al. (2017): Quasi-experimental methods enable stronger inferences from observational data in ecology. (c) Matthias Baumann (2017-01-10).\nIn the Butsic et al. paper, they used the example of the impact of wildfire on species richness. Here, we will simulate data with a known treatment effect of fire on species richness. We will then compare the estimated effect we get through a naive ordinary least squares (OLS) regression approach to the effect we estimate when we use matching methods to control for observable confounding variables."
  },
  {
    "objectID": "exercises/matching_demo.html#set-up",
    "href": "exercises/matching_demo.html#set-up",
    "title": "Matching in R",
    "section": "Set up",
    "text": "Set up\nLoad required packages. In this demo, we will use the package “MatchIt” for the matching process."
  },
  {
    "objectID": "exercises/matching_demo.html#simulate-data",
    "href": "exercises/matching_demo.html#simulate-data",
    "title": "Matching in R",
    "section": "Simulate data",
    "text": "Simulate data\nSimulated data is handy because we know the true effect of the treatment variable. Here, we’ll write a function to simulate a dataset where we know the true effect of fire on species richness.\n\n\nCode\n### Function to simulate data and write it as a dataframe\nsimulate_data &lt;- function(){\n  \n  ### Create variables in a dataframe\n  \n  ### Make column for observation ID\n  df &lt;- data.frame(id = seq(1,1000),\n                   \n                   ### Add columns for explanatory variables\n                   \n                   ### Add column for treatment variable\n                   fire = c(rep(0,500), rep(1,500)),\n                   \n                   ### And the rest of the covariates\n                   slope = c(runif(500, min = 50, max = 90), \n                             runif(500, min = 65, max = 150)),\n                   elevation = c(runif(500, min = 150, max = 185), \n                                 runif(500, min = 165, max = 200)),\n                   stream = runif(1000, min = 0, max = 1),\n                   \n                   ### And the error term\n                   error = rnorm(1000, mean = 0, sd = 5))\n  \n  ### Add a slope*slope variable\n  df &lt;- df %&gt;%\n    mutate(slope2 = slope^2)\n  \n  ### Make column for outcome variable (species richness) \n  df &lt;- df %&gt;%\n    mutate(species_richness = 1 + 5*fire + 0.07*slope + 0.05*elevation + 2*stream - 0.005*slope2 + error)\n  return(df)\n}\n\n\nWe know that the true effect of the treatment variable (fire) is a 5x increase in the response variable (species richness)."
  },
  {
    "objectID": "exercises/matching_demo.html#estimate-the-effect-using-ordinary-least-squares",
    "href": "exercises/matching_demo.html#estimate-the-effect-using-ordinary-least-squares",
    "title": "Matching in R",
    "section": "Estimate the effect using ordinary least squares",
    "text": "Estimate the effect using ordinary least squares\n\n\nCode\n### Write a function to generate data and analyze using OLS \nols_fun &lt;- function(){\n  \n  ### Simulate the dataset\n  data &lt;- simulate_data()\n  \n  ### Run OLS regression\n  ols &lt;- lm(species_richness ~ fire + slope + elevation + stream, \n            data = data)\n  \n  ### Extract model coefficients and standard error\n  fire_coeff &lt;- coef(summary(ols))[\"fire\", \"Estimate\"]\n  fire_se &lt;- coef(summary(ols))[\"fire\", \"Std. Error\"]\n  list &lt;- list(fire_coeff, fire_se)\n}\n\n### Apply the function to 100 replicates\nols_sim &lt;- replicate(100, ols_fun())\n\n### Extract the model estimates\nols_fire_est &lt;- unlist(ols_sim[1, ])\n\n### Print mean, standard deviation, minimum, and maximum values for coefficient estimates\nc(mean(ols_fire_est), sd(ols_fire_est), \n  min(ols_fire_est), max(ols_fire_est))\n\n\n[1] 7.0279435 0.6065381 5.6262475 8.3778838\n\n\nCode\n### Extract the standard deviations\nols_fire_sd &lt;- unlist(ols_sim[2, ])\n\n### Print mean, standard deviation, minimum, and maximum standard deviation of coefficient estimates\nc(mean(ols_fire_sd), sd(ols_fire_sd), \n  min(ols_fire_sd), max(ols_fire_sd))\n\n\n[1] 0.60583817 0.01842916 0.56454731 0.64972060\n\n\nThe effect estimated by OLS is incorrect– it should be 5."
  },
  {
    "objectID": "exercises/matching_demo.html#use-pre-regression-matching-then-run-the-regression",
    "href": "exercises/matching_demo.html#use-pre-regression-matching-then-run-the-regression",
    "title": "Matching in R",
    "section": "Use pre-regression matching, then run the regression",
    "text": "Use pre-regression matching, then run the regression\n\n\nCode\n### Write a function to generate the data, use matching to subset the data, and run a regression on the matched data\npps_fun &lt;- function(){\n  \n  ### Simulate the dataset\n  data &lt;- simulate_data()\n  \n  ### Match the data on the observed covariates\n  match &lt;- matchit(fire ~ slope + \n                     elevation + \n                     stream, \n                   \n                   ### set method to use for matching\n                   method = \"nearest\", \n                   \n                   ### tell it what data source to draw matches from\n                   data = data, \n                   \n                   ### tell it to use logistic regression for the matching\n                   distance = \"glm\", \n                   link = \"probit\",\n                   \n                   ### specify which order to draw potential points from the full dataset\n                   m.order = \"random\",\n                   \n                   ### set a maximum distance for the matches\n                   caliper = 0.10)\n  \n  ### Extract the matched data from the full dataset\n  matched_data = match.data(match)\n  \n  ### Run OLS on the matched dataset\n  ols &lt;- lm(species_richness ~ fire + slope + \n              elevation + stream, \n            data = matched_data)\n  \n  ### Extract model coefficients\n  fire_coeff &lt;- coef(summary(ols))[\"fire\", \"Estimate\"]\n  fire_se &lt;- coef(summary(ols))[\"fire\", \"Std. Error\"]\n  list &lt;- list(fire_coeff, fire_se) \n}\n\n### Apply the function to 100 replicates\npps_sim &lt;- replicate(100, pps_fun())\n\n### Extract the model estimates\npps_fire_est &lt;- unlist(pps_sim[1,])\n\n### Print mean, standard deviation, minimum, and maximum values for coefficient estimates\nc(mean(pps_fire_est), sd(pps_fire_est), \n  min(pps_fire_est), max(pps_fire_est))\n\n\n[1] 4.8386337 0.7223976 3.5179257 6.8933651\n\n\nCode\n### Extract the standard deviations\npps_fire_sd &lt;- unlist(pps_sim[2,])\n\n### Print mean, standard deviation, minimum, and maximum standard deviation of coefficient estimates\nc(mean(pps_fire_sd), sd(pps_fire_sd), \n  min(pps_fire_sd), max(pps_fire_sd))\n\n\n[1] 0.67701326 0.04671819 0.55414637 0.80191143"
  },
  {
    "objectID": "exercises/matching_demo.html#take-a-closer-look-at-the-matching-process",
    "href": "exercises/matching_demo.html#take-a-closer-look-at-the-matching-process",
    "title": "Matching in R",
    "section": "Take a closer look at the matching process",
    "text": "Take a closer look at the matching process\n\nPre-matching data\n\n\nCode\n### Simulate a dataset\ndata_for_match &lt;- simulate_data()\n\n### Make fire a factor variable\ndata_for_match &lt;- data_for_match %&gt;%\n  mutate_at(vars(fire), \n            funs(factor))\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nCode\n### Take a look at the balance of the covariates before matching\ndata_for_match %&gt;%\n  dplyr::select(id, fire, slope, elevation, stream) %&gt;%\n  gather(variable, value, slope:stream, factor_key = TRUE) %&gt;%\n  ggplot(aes(x = variable, y = value, color = fire)) +\n  geom_boxplot() +\n  xlab(\"Variable\") + ylab(\"Value\")\n\n\n\n\n\n\n\n\n\nCode\n### fires: 0 = unburned, 1 = burned\n\n### You can also look at it in table form\ncovariate_summ &lt;- data_for_match %&gt;%\n  group_by(fire) %&gt;%\n  summarise(slope_mean = mean(slope),\n            slope_sd = sd(slope),\n            elevation_mean = mean(elevation),\n            elevation_sd = sd(elevation),\n            stream_mean = mean(stream),\n            stream_sd = sd(stream))\n\n\nWe can see that there are issues with the balance between the burned and unburned sample units. The burned areas are on steeper slopes and higher elevations, on average.\n\n\nMatch the data\n\n\nCode\n### Match the data on the observed covariates\nmatch &lt;- matchit(fire ~ slope + \n                   elevation + \n                   stream, \n                 \n                 ### set method to use for matching\n                 method = \"nearest\", \n                 \n                 ### tell it what data source to draw matches from\n                 data = data_for_match, \n                 \n                 ### tell it to use logistic regression for calculating the propensity scores \n                 distance = \"glm\", \n                 link = \"probit\",\n                 \n                 ### specify which order to draw potential points from the full dataset\n                 m.order = \"random\",\n                 \n                 ### set a maximum distance for the matches\n                 caliper = 0.10)\n\n### Take a look at the quality of the matches\nmatch_quality &lt;- summary(match, \n                         standardize = TRUE)\n\n### Let's see how many points were matched\nmatch_quality_nn &lt;- as.data.frame(match_quality$nn)\n\n### Let's look at the pre-match covariate balance\nmatch_quality_unmatched &lt;- as.data.frame(match_quality$sum.all)\n\n### What does the covariate balance look like after matching?\nmatch_quality_summary &lt;- as.data.frame(match_quality$sum.matched)\n### Ideally, you want the standardized mean differences in the matched dataset to be &lt; 0.25 (reference: Schleicher et al. 2020. Statistical matching for conservation science. Conserv. Biol. 34:538–549. https://doi.org/10.1111/cobi.13448).\n\n### You can also look at the amount of bias *reduction achieved through matching\n# match_quality_reduction &lt;- as.data.frame(match_quality$reduction)\n\n\n\nVisualizing the match quality\n\n\nCode\n### You can also use a fun interactive command to visualize the pre- and post-match covariate spread\n# plot(match, interactive = FALSE)\n\n### And you can compare the propensity scores visually\n# plot(match, type = \"jitter\", interactive = FALSE)\n\n\n\n\n\nAnalyzing the matched subset of data\n\n\nCode\n### First, extract the matches from the full dataset\ndemo_matched &lt;- match.data(match)\n\n### Then run the regression, including the covariates in the model\nfit_matched &lt;- lm(species_richness ~ fire + slope + \n                    elevation + stream, \n                  data = demo_matched)"
  },
  {
    "objectID": "exercises/rdd_demo.html",
    "href": "exercises/rdd_demo.html",
    "title": "Regression discontinuity design in R",
    "section": "",
    "text": "Code to demonstrate matching in R. Adapted from the supplementary materials from Butsic, V. et al. (2017): Quasi-experimental methods enable stronger inferences from observational data in ecology. (c) Matthias Baumann (2017-01-10).\nIn the Butsic et al. paper, they used the example of the impact of wildfire on species richness. Here, we will simulate data with a known treatment effect of fire on species richness. We will then compare the estimated effect we get through a naive ordinary least squares (OLS) regression approach to the effect we estimate when we use RDD methods ."
  },
  {
    "objectID": "exercises/rdd_demo.html#description",
    "href": "exercises/rdd_demo.html#description",
    "title": "Regression discontinuity design in R",
    "section": "",
    "text": "Code to demonstrate matching in R. Adapted from the supplementary materials from Butsic, V. et al. (2017): Quasi-experimental methods enable stronger inferences from observational data in ecology. (c) Matthias Baumann (2017-01-10).\nIn the Butsic et al. paper, they used the example of the impact of wildfire on species richness. Here, we will simulate data with a known treatment effect of fire on species richness. We will then compare the estimated effect we get through a naive ordinary least squares (OLS) regression approach to the effect we estimate when we use RDD methods ."
  },
  {
    "objectID": "exercises/rdd_demo.html#set-up",
    "href": "exercises/rdd_demo.html#set-up",
    "title": "Regression discontinuity design in R",
    "section": "Set up",
    "text": "Set up\nLoad required packages. In this demo, we will use the package “rdrobust” for the matching process."
  },
  {
    "objectID": "exercises/rdd_demo.html#simulate-data",
    "href": "exercises/rdd_demo.html#simulate-data",
    "title": "Regression discontinuity design in R",
    "section": "Simulate data",
    "text": "Simulate data\nSimulated data is handy because we know the true effect of the treatment variable. Here, we’ll write a function to simulate a dataset where we know the true effect of fire on species richness.\n\n\nCode\n### Function to simulate data and write it as a dataframe\nsimulate_data &lt;- function(){\n  \n  ### Create variables in a dataframe\n  \n  ### Make column for observation ID\n  df &lt;- data.frame(id = seq(1, 1000),\n                   \n                   ### Add columns for explanatory variables\n                   \n                   ### Add column for treatment variable\n                   fire = c(rep(0, 500), rep(1, 500)),\n                   \n                   ### And the rest of the covariates\n                   distance = c(runif(500, min = -100, max = 0), \n                             runif(500, min = 0, max = 100)),\n                   slope = runif(1000, min = 50, max = 100),\n                   elevation = runif(1000, min = 150, max = 185),\n                   stream = runif(1000, min = 0, max = 1),\n                   \n                   ### And the error term\n                   error = rnorm(1000, mean = 0, sd = 2))\n  \n  ### Add land use history variable, depends on outcomes of distance\n  df &lt;- df %&gt;%\n    mutate(land_use = ifelse(distance &gt; -50 & distance &lt; 5, \n                             1, \n                             0))\n  \n  \n  \n  ### Make column for outcome variable (species richness) \n  df &lt;- df %&gt;%\n    mutate(species_richness = 1 + 5*fire + 0.7*slope + 0.05*elevation + 2*stream + 5*land_use + error)\n  return(df)\n}\n\n\nWe know that the true effect of the treatment variable (fire) is a 5x increase in the response variable (species richness)."
  },
  {
    "objectID": "exercises/rdd_demo.html#estimate-the-effect-using-ordinary-least-squares",
    "href": "exercises/rdd_demo.html#estimate-the-effect-using-ordinary-least-squares",
    "title": "Regression discontinuity design in R",
    "section": "Estimate the effect using ordinary least squares",
    "text": "Estimate the effect using ordinary least squares\n\n\nCode\n### Write a function to generate data and analyze using OLS \nols_fun &lt;- function(){\n  \n  ### Simulate the dataset\n  data &lt;- simulate_data()\n  \n  ### Run OLS regression\n  ols &lt;- lm(species_richness ~ fire + slope + elevation + stream, \n            data = data)\n  \n  ### Extract model coefficients and standard error\n  fire_coeff &lt;- coef(summary(ols))[\"fire\", \"Estimate\"]\n  fire_se &lt;- coef(summary(ols))[\"fire\", \"Std. Error\"]\n  list &lt;- list(fire_coeff, fire_se)\n}\n\n### Apply the function to 100 replicates\nols_sim &lt;- replicate(100, ols_fun())\n\n### Extract the model estimates\nols_fire_est &lt;- unlist(ols_sim[1, ])\n\n### Print mean, standard deviation, minimum, and maximum values for coefficient estimates\nc(mean(ols_fire_est), sd(ols_fire_est), \n  min(ols_fire_est), max(ols_fire_est))\n\n\n[1] 2.7531093 0.1720598 2.3303118 3.1750195\n\n\nCode\n### Extract the standard deviations\nols_fire_sd &lt;- unlist(ols_sim[2, ])\n\n### Print mean, standard deviation, minimum, and maximum standard deviation of coefficient estimates\nc(mean(ols_fire_sd), sd(ols_fire_sd), \n  min(ols_fire_sd), max(ols_fire_sd))\n\n\n[1] 0.176375519 0.003588894 0.168985998 0.186028617\n\n\nThe effect estimated by OLS is incorrect– it should be 5."
  },
  {
    "objectID": "exercises/rdd_demo.html#use-regression-discontinuity",
    "href": "exercises/rdd_demo.html#use-regression-discontinuity",
    "title": "Regression discontinuity design in R",
    "section": "Use regression discontinuity",
    "text": "Use regression discontinuity\n\n\nCode\n### Simulate the dataset\ndata_rdd &lt;- simulate_data()\n\n### look at the distribution of sample points along the running variable\n# ggplot(data = data_rdd, aes(x = distance)) + \n#   geom_histogram() +\n#   geom_vline(xintercept = 0)\n# \n# ### look at the distribution of the outcome variable along the running variable\n# ggplot(data = data_rdd, aes(x = distance, y = species_richness)) +\n#   geom_point() +\n#   geom_vline(xintercept = 0)"
  },
  {
    "objectID": "exercises/index.html",
    "href": "exercises/index.html",
    "title": "Exercises in R",
    "section": "",
    "text": "This section has link to the files for the demos and replication exercises that we will do in class. Here, you will find link to files with annotated code and csv files with data for the replication exercises.",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/index.html#monday-220",
    "href": "exercises/index.html#monday-220",
    "title": "Exercises in R",
    "section": "Monday 2/20",
    "text": "Monday 2/20\nMatching demo in R. I couldn’t figure out a cute way for you to be able to download this directly as a RMarkdown file (thwarted by quarto…). But this link will take you to the page in the course’s GitHub repository where the RMarkdown file is stored. Click on “Raw,” then you can copy and paste the code into an RMarkdown on your computer.",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/index.html#wednesday-222",
    "href": "exercises/index.html#wednesday-222",
    "title": "Exercises in R",
    "section": "Wednesday 2/22",
    "text": "Wednesday 2/22\nMatching demo with simulated data\nMatching and Weighting replication exercise\nIn this replication exercise, you will use a subset of the data in the data folder found in the above link; the data is from Siegel et al. 2022 to play around with matching and weighting using a real dataset. On the Github, there are two csv files of data: colo_dat_full.csv and colo_data_for_matching.csv. colo_dat_full.csv has the entire time series of data for the full (unmatched) dataset of federal and private forests in Colorado. colo_data_for_matching.csv is a file that’s ready for the matching process without additional pre-processing: it has a row for each sample point in Colorado and five-year averages for the climate variables.",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/index.html#monday-227",
    "href": "exercises/index.html#monday-227",
    "title": "Exercises in R",
    "section": "Monday 2/27",
    "text": "Monday 2/27\nDiff-in-Diff regression demo in R Application and interpretation of differences-in-differences to examine effects of the Marshall Fire on vegetation in Boulder, CO. Data file is on GDrive.\nOptional: Review of difference-in-difference covered in lecture in RMarkdown, see here",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/index.html#monday-36",
    "href": "exercises/index.html#monday-36",
    "title": "Exercises in R",
    "section": "Monday 3/6",
    "text": "Monday 3/6\nPanel regression demo in R This tutorial replicates the main analyses in Dee et al, contrasted with common approaches in ecology (mixed effect models and conditioning on observable covariates. For the rest of the supplemental analyses in Dee et al, see L.Dee’s GitHub",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/index.html#wednesday-315",
    "href": "exercises/index.html#wednesday-315",
    "title": "Exercises in R",
    "section": "Wednesday 3/15",
    "text": "Wednesday 3/15\nInstrumental variables demo in R. Link to an RMarkdown file that walks through the process of coding an analysis using an instrumental variable.",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/index.html#wednesday-322",
    "href": "exercises/index.html#wednesday-322",
    "title": "Exercises in R",
    "section": "Wednesday 3/22",
    "text": "Wednesday 3/22\nRegression discontinuity design demo in R. Link to an RMarkdown file that walks through the process of coding an analysis using regression discontinuity analysis.",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/matching_replication.html",
    "href": "exercises/matching_replication.html",
    "title": "Replication exercise for matching methods",
    "section": "",
    "text": "In this replication exercise, you will use some of the data from Siegel et al. 2022 (https://doi.org/10.1007/s10113-022-01950-y). The dataset for the entire western US is very large and unwieldy, so you’ll work with data from a single state: Colorado. Note: all the code is commented out in this Rmd – you’ll need to uncomment the parts you want to use."
  },
  {
    "objectID": "exercises/matching_replication.html#description",
    "href": "exercises/matching_replication.html#description",
    "title": "Replication exercise for matching methods",
    "section": "",
    "text": "In this replication exercise, you will use some of the data from Siegel et al. 2022 (https://doi.org/10.1007/s10113-022-01950-y). The dataset for the entire western US is very large and unwieldy, so you’ll work with data from a single state: Colorado. Note: all the code is commented out in this Rmd – you’ll need to uncomment the parts you want to use."
  },
  {
    "objectID": "exercises/matching_replication.html#set-up",
    "href": "exercises/matching_replication.html#set-up",
    "title": "Replication exercise for matching methods",
    "section": "Set up",
    "text": "Set up"
  },
  {
    "objectID": "exercises/matching_replication.html#the-data",
    "href": "exercises/matching_replication.html#the-data",
    "title": "Replication exercise for matching methods",
    "section": "The data",
    "text": "The data\nThere are two data files you can play around with for this exercise: colo_dat_full.csv and colo_data_for_matching.csv. colo_dat_full.csv has the entire time series of data for the full (unmatched) dataset of federal and private forests in Colorado. colo_data_for_matching.csv is a file that’s ready for the matching process without additional pre-processing: it has a row for each sample point in Colorado and five-year averages for the climate variables.\n\nVariable names in colo_dat_full\n\nstate: the state the sample is from (it should always be Colorado in this file)\n\nUID: a unique identifier for each sample point\n\nyear: the year that the fire and climate data is from\n\nburned: whether or not the site burned in that year (0 = unburned, 1 = burned)\n\nprot_cat_recl: the ownership class. 0 = private, 1 = federal\n\ndist_rds_km: distance to the nearest road, in km\n\nslope: slope, in degrees\n\naspect_srai: aspect\n\nelev_km: elevation, in 1000 m\n\nlon: longitude\n\nlat: latitude\n\nlightning: county-level lightning strikes\n\npdsi_avg_season: seasonal average Palmer Drought Severity Index value\n\nsoil_avg_season: seasonal average soil moisture\n\ntmmn_avg_season: seasonal average minimum temperature\ntmmx_avg_season: seasonal average maximum temperature\n\nvs_max_season: seasonal average maximum wind speed\n\ntotal_precip_season: total seasonal precipitation\n\nprev_yr_precip: total precipitation in the previous year\n\n\n\nVariable names in colo_data_for_matching\nAll the climate variables are variablename_5 to indicate that they are 5-year average values * UID: a unique identifier for each sample point\n* state: the state the sample is from\n* prot_cat_recl: the ownership class. 0 = private, 1 = federal\n* lightning_5: 5 year average for lightning strikes\n* vs_max_season: seasonal average maximum wind speed\n* pr_total_season: total seasonal precipitation\n* tmmx_avg_season: seasonal average maximum temperature\n* tmmn_avg_season: seasonal average minimum temperature * pdsi_avg_season: seasonal average Palmer Drought Severity Index value\n* soil_avg_season: seasonal average soil moisture\n* slope: slope, in degrees\n* aspect_srai: aspect\n* elev_km: elevation, in 1000 m\n* lon: longitude\n* lat: latitude\n* dist_rds_km: distance to the nearest road, in km\n* popdens_1990: population density (per km2) in 1990\n* popdens_2000: population density (per km2) in 2000\n* popdens_2010: population density (per km2) in 2010"
  },
  {
    "objectID": "exercises/matching_replication.html#prep-the-data",
    "href": "exercises/matching_replication.html#prep-the-data",
    "title": "Replication exercise for matching methods",
    "section": "Prep the data",
    "text": "Prep the data"
  },
  {
    "objectID": "exercises/matching_replication.html#check-out-the-full-unmatched-dataset",
    "href": "exercises/matching_replication.html#check-out-the-full-unmatched-dataset",
    "title": "Replication exercise for matching methods",
    "section": "Check out the full, unmatched dataset",
    "text": "Check out the full, unmatched dataset\nYou might want to make the prot_cat_recl variable a factor.\n\n\nCode\n### See how many treated (federal) and control (private) sample points we have\n\n\n### You can take a look at the balance of the covariates before matching via data visualizations (boxplots, histograms, density plots)\n\n\n### You can also look at it in table form"
  },
  {
    "objectID": "exercises/matching_replication.html#match-the-data",
    "href": "exercises/matching_replication.html#match-the-data",
    "title": "Replication exercise for matching methods",
    "section": "Match the data",
    "text": "Match the data\nMatch the data on the observed covariates, using the MatchIt package. You can play around with the settings to see how it affects the matched data you end up with.\n\n\nCode\n### Match the data \n\n\n\nAssess match quality\nTake a look at the quality of the matches: how many points were matched? what was the pre-matching covariate balance? what was the covariate balance after matching?\n\n\nVisualize the match quality\n\n\nCode\n### Some easy visualizations through MatchiIt\n# plot(match, interactive = FALSE)\n# plot(match, type = \"jitter\", interactive = FALSE)\n\n\n\n\nIterate, if necessary\nIf you’re satisfied with the quality of your matches, you can move on to the analysis. Otherwise, try tweaking the parameters to see what happens."
  },
  {
    "objectID": "exercises/matching_replication.html#analyze-the-matched-dataset",
    "href": "exercises/matching_replication.html#analyze-the-matched-dataset",
    "title": "Replication exercise for matching methods",
    "section": "Analyze the matched dataset",
    "text": "Analyze the matched dataset\n\nExtract the matched data\nFirst, you’ll need to extract the matched data and use the UIDs from the matched data to subset the full dataset for analysis. Here is some code to demonstrate that (you’ll likely need to modify this)\n\n\nCode\n# ### Extract the matches from the MatchIt object\n# matched &lt;- match.data(match)\n# \n# ### Open the full data (entire time series) \n# full_data &lt;- read.csv(\"file_path/colo_dat_full.csv\")\n# \n# ### Filter full_data to just include the UIDs of the matched subset of the data\n# full_data &lt;- full_data %&gt;%\n#   filter(UID %in% matched$UID)\n\n\n\n\nAdd time lag variables\nIn my analyses, I used time lag variables to account for recent fire history: if a site burned last year, it usually won’t burn this year. So we’ll make a variable that accounts for whether or not a site burned in the previous five years.\nSince we used a lag variable, we won’t have complete data for the first few years of the time series, so we’ll also filter the dataset to just include the years with complete data.\n\n\nCode\n# ### Add lag variable\n# full_data &lt;- full_data %&gt;%\n#   \n#   ### Group by the UID so you can make a site-specific lag variable\n#   group_by(UID) %&gt;%\n#   \n#   ### Add the lag variable\n#   mutate(burn_prev_5_yr = dplyr::lag(burned, \n#                                      n = 5, \n#                                      default = NA,\n#                                      order_by = year)) %&gt;%\n#   ungroup()\n# \n# ### Drop first years of time series\n# full_data &lt;- full_data %&gt;%\n#   filter(year &gt; 1988)\n\n\n\n\nCheck for correlated variables\nSome of the continuous variables are highly correlated, so we don’t want to include them in the regression model.\n\n\nCode\n### Assess level of correlation\n\n### if you want to use it, here's a function that returns a table with the correlation coefficient and p-value between each pair of variables:\n# cor.prob &lt;- function (X, dfr = nrow(X) - 2) {\n#   R &lt;- cor(X, use=\"pairwise.complete.obs\")\n#   above &lt;- row(R) &lt; col(R)\n#   r2 &lt;- R[above]^2\n#   Fstat &lt;- r2 * dfr/(1 - r2)\n#   R[above] &lt;- 1 - pf(Fstat, 1, dfr)\n#   R[row(R) == col(R)] &lt;- NA\n#   R\n# }\n# flattenSquareMatrix &lt;- function(m) {\n#   if( (class(m) != \"matrix\") | (nrow(m) != ncol(m))) stop(\"Must be a square matrix.\") \n#   if(!identical(rownames(m), colnames(m))) stop(\"Row and column names must be equal.\")\n#   ut &lt;- upper.tri(m)\n#   data.frame(i = rownames(m)[row(m)[ut]],\n#              j = rownames(m)[col(m)[ut]],\n#              cor=t(m)[ut],\n#              p=m[ut])\n# }\n\n### Determine which variables to drop\n\n\n\n\nModel the effect of ownership/management on wildfire probability for a given site in a given year\nThis is the exciting part! Here’s a basic logistic regression model, but feel free to tweak it.\n\n\nCode\n# ### Model\n# colo_model &lt;- glmer(burned ~ # covariates you're including + \n#                       \n#                       ### include an interaction between year and ownership \n#                       ### don't forget to tell R that prot_cat_recl is a factor\n#                       prot_cat_recl*year + \n#                       \n#                       ### add the lag variable\n#                       burn_prev_5_yr +\n#                       \n#                       ### and a site-level effect to account for unobservable \n#                       ### factors unique to each site\n#                       (1|UID),\n#                     \n#                     ### tell it where to get the data from\n#                     data = full_data, \n#                     \n#                     ### model parameters\n#                     family = binomial(link = \"logit\"),\n#                     nAGQ = 0,\n#                     control = glmerControl(optimizer = \"nloptwrap\"))\n\n### Extract the model's coefficient estimates"
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "EBIO 5460: Causal Inference in Ecology",
    "section": "",
    "text": "Laura Dee, Associate Professor, CU Boulder (laura.dee@colorado.edu)",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "coding_resources.html",
    "href": "coding_resources.html",
    "title": "Coding Resources",
    "section": "",
    "text": "Programming in R will be a key part of this course, and as most of you have probably experienced, R can cause headaches and stress! Files go missing, package dependencies spiral out of control, and minor code errors can cost us hours of troubleshooting. The point of this course is not to become an R whiz, and I don’t want code errors or coding anxiety to get in the way of your learning and research progress.\nI encourage you to use online resources for coding help if/when you get stuck. StackOverflow is a great resource for finding solutions to coding issues or asking a specific question, as is the RStudio Community. BlueSky can also be a great resource: if you post a question with the hashtag #rstats, a kind soul might give you an answer. If you’re looking for help via Google, we recommend including in your query the package you’re using and if applicable, the error message you’re getting. Finally, if you are a visual learner, Allison Horst has some great learning tools for various R functions.\nIf you could use some more training and exposure to R, here are some resources to get started:\n\nswirl teaches you R programming and data science interactively, at your own pace, and right in the R console! On this page, swirl will walk you through each of the steps required to begin using R and swirl today!\n\nR for data science (R4DS). R4DS teaches you how to do data science with R: You’ll learn how to get your data into R, get it into the most useful structure, transform it, visualize it and model it. The book is available online, where you can find a practicum of skills for data science from visualization to wrangling and writing functions.\n\nHave a favorite resource? Please send it to Laura to post!\n\nIf you use Python instead, many of the free, online books we will use (The Effect, Causal MixTape, etc.) have code snippets in both R and Python",
    "crumbs": [
      "Coding Resources"
    ]
  },
  {
    "objectID": "readings.html",
    "href": "readings.html",
    "title": "Readings by Class & Additional Materials",
    "section": "",
    "text": "Note: book chapters can be found on the private course Google Drive, as well as pdfs of the papers. Demos will be in RMarkdown and on the course GitHub page in the exercises folder.",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#recommended-textbooks-not-required",
    "href": "readings.html#recommended-textbooks-not-required",
    "title": "Readings & Additional Materials",
    "section": "",
    "text": "We do not expect you to buy any/all of these books. We will provide the sections that are assigned for class. But if you want to explore topics of causal inference further, we recommend these books!\nNote: Some of these texts use examples to illustrate their points that are problematic (e.g., treating gender as a binary or studying post-colonial economic development without considering the violence of colonialism). We do not agree with the assumptions underlying these examples and we acknowledge the problems with them. The descriptions of methods are still some of the easiest to read out there.\n\nMorgan, SL and C Winship. 2007. Counterfactuals and Causal Inference: methods and principles for social research.\n\nGerber, AS and DP Green. 2012. Field Experiments: design, analysis and interpretation. (especially useful if you plan to do field experiments in your research)\n\nCunningham, S. 2021. Causal Inference: The Mixtape (New Haven, CT: Yale University Press). Available online for free\n\nAngrist, JD and JS Pischke. 2015. Mastering ’Metrics: The Path from Cause to Effect (Princeton, NJ: Princeton University Press).\n\nAngrist, JD and JS Pischke. 2008. Mostly Harmless Econometrics: an empiricist’s companion. (Princeton, NJ: Princeton University Press).\n\nRosenbaum, P. 2010. Observational Studies. Springer.\n\nPearl, J. and D. Mackenzie. 2018. The Book of Why. (New York, NY: Basic Books, Inc.) (A more popular science book)\n\nFor more technical discussions of causality:\n\nHolland 1986, JASA\n\nHeckman 2000, QJE\n\nPearl, J. 2009. Causality. (Cambridge, UK: Cambridge University Press).",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-123",
    "href": "readings.html#due-mon.-123",
    "title": "Readings & Additional Materials",
    "section": "Due Mon. 1/23",
    "text": "Due Mon. 1/23\n\nHernan et al. 2019\nAngrist & Pischke 2008, Chapter 1\n\nGerber & Green, Chapter 1 (you can just skim this one)",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-125",
    "href": "readings.html#due-wed.-125",
    "title": "Readings & Additional Materials",
    "section": "Due Wed. 1/25",
    "text": "Due Wed. 1/25\n\nAngrist & Pischke 2008, Chapter 2\n\nHernan 2016",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-130",
    "href": "readings.html#due-mon.-130",
    "title": "Readings & Additional Materials",
    "section": "Due Mon. 1/30",
    "text": "Due Mon. 1/30\nGuest speaker: Dr. Zach Laubach\n\nRohrer 2018\nLaubach et al. 2021",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-21",
    "href": "readings.html#due-wed.-21",
    "title": "Readings & Additional Materials",
    "section": "Due Wed. 2/1",
    "text": "Due Wed. 2/1\nCome to class prepared to present and discuss your DAG.\n\nArif & MacNeil 2022\n\nWatch Hernan video: “3. Elements of DAGs” on Canvas\nCheck out DAG software\n\n\nOptional\nFor additional background on DAGs:\n4) Morgan & Winship (2007) pgs. 29-34 and Ch. 3 - see Canvas 5) Cunningham (2021) Ch. 3\nOther software for making DAGs:\na) ggdag is a nice R package based on dagitty but tidyverse-compatible and with much better plotting functionality\nb) shinydag is another GUI aimed at visualizing DAGs and exporting them in different publication-ready formats\nc) TETRAD\nd) DAG program\ne) dagR",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-26",
    "href": "readings.html#due-mon.-26",
    "title": "Readings & Additional Materials",
    "section": "Due Mon. 2/6",
    "text": "Due Mon. 2/6\n\nGerber & Green, Chapter 2\nKimmel et al. 2021",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-28-casey",
    "href": "readings.html#due-wed.-28-casey",
    "title": "Readings & Additional Materials",
    "section": "Due Wed. 2/8 [Casey]",
    "text": "Due Wed. 2/8 [Casey]\n\nNo readings today!",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-213",
    "href": "readings.html#due-mon.-213",
    "title": "Readings & Additional Materials",
    "section": "Due Mon. 2/13",
    "text": "Due Mon. 2/13\n\nWatch Imbens video: 2022 Nobel Prize lecture\nFerraro 2009",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-215-brendan-henry",
    "href": "readings.html#due-wed.-215-brendan-henry",
    "title": "Readings & Additional Materials",
    "section": "Due Wed. 2/15 [Brendan & Henry]",
    "text": "Due Wed. 2/15 [Brendan & Henry]\n\nButsic et al. 2017\n\nLarsen et al. 2019",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-220",
    "href": "readings.html#due-mon.-220",
    "title": "Readings & Additional Materials",
    "section": "Due Mon. 2/20",
    "text": "Due Mon. 2/20\n\nRamsey et al. 2018\nStuart 2010",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-222-alec-sam",
    "href": "readings.html#due-wed.-222-alec-sam",
    "title": "Readings & Additional Materials",
    "section": "Due Wed. 2/22 [Alec & Sam]",
    "text": "Due Wed. 2/22 [Alec & Sam]\n\nSiegel et al. 2022\n\nXu et al. 2022\n\nSiegel et al., submitted (available on Canvas)",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-227",
    "href": "readings.html#due-mon.-227",
    "title": "Readings & Additional Materials",
    "section": "Due Mon. 2/27",
    "text": "Due Mon. 2/27\n\nAngrist & Pischke 2015, Chapter 5 (note: this is in the book Mastering ’Metrics)",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-31-anna-hilary",
    "href": "readings.html#due-wed.-31-anna-hilary",
    "title": "Readings & Additional Materials",
    "section": "Due Wed. 3/1 [Anna & Hilary]",
    "text": "Due Wed. 3/1 [Anna & Hilary]\n\nSimler-Williamson & Germino 2022",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-36",
    "href": "readings.html#due-mon.-36",
    "title": "Readings & Additional Materials",
    "section": "Due Mon. 3/6",
    "text": "Due Mon. 3/6\n\nAngrist & Pischke 2008, Chapter 5",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-38-meghan-tom",
    "href": "readings.html#due-wed.-38-meghan-tom",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 3/8 [Meghan & Tom]",
    "text": "Due Wed. 3/8 [Meghan & Tom]\n\nDee et al., in press, and RMarkdown\n\nByrnes & Dee, in prep and RShiny\n\n\nOptional\n\nMeehan et al. 2011 vs. Larsen 2013\n\nGrace et al. 2016 vs. Dee et al., in press\nDudney et al. 2021",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-313",
    "href": "readings.html#due-mon.-313",
    "title": "Readings & Additional Materials",
    "section": "Due Mon. 3/13",
    "text": "Due Mon. 3/13\n\nAngrist & Pischke 2015, Chapter 3 (note: this is in Mastering ’Metrics)\n\nKendall book chapter (on Canvas)",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-315-max-tyler",
    "href": "readings.html#due-wed.-315-max-tyler",
    "title": "Readings & Additional Materials",
    "section": "Due Wed. 3/15 [Max & Tyler]",
    "text": "Due Wed. 3/15 [Max & Tyler]\n\nSims 2010\n\nMacDonald & Mordecai 2020",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-320",
    "href": "readings.html#due-mon.-320",
    "title": "Readings & Additional Materials",
    "section": "Due Mon. 3/20",
    "text": "Due Mon. 3/20\n\nAngrist & Pischke 2015, Chapter 4 (note: this is in Mastering ’Metrics)",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-322-aly-john",
    "href": "readings.html#due-wed.-322-aly-john",
    "title": "Readings & Additional Materials",
    "section": "Due Wed. 3/22 [Aly & John]",
    "text": "Due Wed. 3/22 [Aly & John]\n\nEnglander 2019\n\nNoack et al. 2022\n\nOptional additional paper: Burgess et al. 2019",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-43",
    "href": "readings.html#due-mon.-43",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 4/3",
    "text": "Due Mon. 4/3\nNo readings - come prepared with your causal question to begin your project. Check your email for a zoom link for the class.\nIn class, start your projects. We have provided a template to provide some structure and facilitate your small group discussions.",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-45",
    "href": "readings.html#due-wed.-45",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 4/5",
    "text": "Due Wed. 4/5\n1:1 consultations with Laura in lieu of class. Schedule to sign up here",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-410",
    "href": "readings.html#due-mon.-410",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 4/10",
    "text": "Due Mon. 4/10\nNo readings - Presenting or discussing your project question and DAG for feedback",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-412",
    "href": "readings.html#due-wed.-412",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 4/12",
    "text": "Due Wed. 4/12\nNo readings - Presenting or discussing your project question and DAG for feedback",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-fri.-414",
    "href": "readings.html#due-fri.-414",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Fri. 4/14",
    "text": "Due Fri. 4/14\nDraft of DAG OR literature review proposal (1 page max.)",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-417",
    "href": "readings.html#due-mon.-417",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 4/17",
    "text": "Due Mon. 4/17\nBrief presentation (3-5 minutes) of your project: the question you are addressing, its applications, your DAG (if applicable) and your proposed method for feedback and Q&A",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-419",
    "href": "readings.html#due-wed.-419",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 4/19",
    "text": "Due Wed. 4/19\nNo assignment – continue working on projects",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#fri.-421",
    "href": "readings.html#fri.-421",
    "title": "Readings by Class & Additional Materials",
    "section": "Fri. 4/21",
    "text": "Fri. 4/21\nRevised DAG + proposed methods OR outline of literature review due",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-424",
    "href": "readings.html#due-mon.-424",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 4/24",
    "text": "Due Mon. 4/24\nNo assignment – continue working on projects",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-426",
    "href": "readings.html#due-wed.-426",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 4/26",
    "text": "Due Wed. 4/26\n\nGeneralizability in experimental and observational studies\nVisit from Dr. Rebecca Spake\n\nSpake et al. 2022 Improving quantitative synthesis to achieve generality in ecology\n\nOptional\n\nKorell et al. 2019\n\nSpake et al. 2021",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-51",
    "href": "readings.html#due-mon.-51",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 5/1",
    "text": "Due Mon. 5/1",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-53",
    "href": "readings.html#due-wed.-53",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 5/3",
    "text": "Due Wed. 5/3\nProject presentations, continued",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#fri.-55",
    "href": "readings.html#fri.-55",
    "title": "Readings by Class & Additional Materials",
    "section": "Fri. 5/5",
    "text": "Fri. 5/5\nFinal project write-ups due (see Assignments tab for descriptions of the requirements for the write-up)",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#potential-additional-units",
    "href": "readings.html#potential-additional-units",
    "title": "Readings by Class & Additional Materials",
    "section": "Potential additional units",
    "text": "Potential additional units\n\nGeneralizability in experimental and observational studies\n\nKorell et al. 2019\n\nSpake et al. 2022\n\nSpake et al. 2021\n\n\n\nHeterogeneous treatment effects\n\n\nReplication/Reproducibility/Pre-registration\n\nKimmel et al., in press\n\n\n\nDeeper dive into mechanisms and mediation analysis",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments & Evaluation",
    "section": "",
    "text": "As a 3-credit graduate course, my expectation is that you are here to learn, which involves following along with the course through doing the reading and attending class meetings. I am less interested in grades and more interested in providing a venue to hold you accountable for learning the material. I will provide many opportunities for feedback and workshopping, so completing your assignments and making progress on your project will benefit your learning (and hopefully contribute to your progress on your thesis/dissertation). As such, attendance and participation is 25%, leading of paper discussion/presentation is 25%, and the final report and presentation are worth the remaining 50%.",
    "crumbs": [
      "Assignments & Evaluation"
    ]
  },
  {
    "objectID": "assignments.html#overview",
    "href": "assignments.html#overview",
    "title": "Assignments & Evaluation",
    "section": "",
    "text": "As a 3-credit graduate course, my expectation is that you are here to learn, which involves following along with the course through doing the reading and attending class meetings. I am less interested in grades and more interested in providing a venue to hold you accountable for learning the material. I will provide many opportunities for feedback and workshopping, so completing your assignments and making progress on your project will benefit your learning (and hopefully contribute to your progress on your thesis/dissertation). As such, attendance and participation is 25%, leading of paper discussion/presentation is 25%, and the final report and presentation are worth the remaining 50%.",
    "crumbs": [
      "Assignments & Evaluation"
    ]
  },
  {
    "objectID": "assignments.html#main-assignment",
    "href": "assignments.html#main-assignment",
    "title": "Assignments & Evaluation",
    "section": "Main assignment",
    "text": "Main assignment\nThe main assignment will be either to complete and write up a data analysis or to do a literature review and deep dive on a particular subtopic (ideas listed below). The project or literature review will culminate in a presentation the last week of classes and a final written report.\nMany students in the past have used this course as an opportunity to advance their thesis research or pursue and publish a paper based on their analyses or literature review!\n\nFinal Project Option 1: Data analysis project\nFor the final project that takes the form of a data analysis, each student will complete a project according to the following process: identifying a research question, creating a DAG, choosing and applying a data analysis, and writing up a methods and results section that 1) articulates the causal relationship of interest, 2) describes the causal identification strategy and choice of study design/methods, 3) describes the implicit assumptions on which this strategy rests, and 4) interprets the results and their potential biases. I encourage students to base this analysis on their thesis research to get the most out of the class.\n\n\nFinal Project Option 2: Literature review\nAlternatively, students can complete a literature review that provides a deeper dive on a topic we do not cover in detail in this course (*some ideas are listed below). This direction may be more appropriate for students who are not at a stage of performing analyses for their thesis, who do not have a relevant project that is tackling an empirical causal question, or who are interested in learning more about a topic not covered under our guidance.\n\nFinal project presentation\nGuidelines for all presentations:\n\n8 minute presentation + 2 minutes for questions\n\nIntroduce the question you address and the motivation for your project\n\nSpecific guidelines for data analysis project presentations:\n\nDescribe the causal inference approach you took and why you chose that approach\n\nBriefly present your DAG and describe the data you used\n\nShare your results AND contextualize them with the important limitations and assumptions of the method(s) you used\n\nOutline possible next steps you could take to make your analysis more robust in the future\nSpecific guidelines for literature review presentations:\nDescribe the challenge for causal inference\n\nDescribe and critique the current approaches people are taking\n\nPresent your ideas for how the field could better incorporate causal inference methods\n\n\n\nFinal project write up\nFor data analysis projects: ~ 5 pages (not including tables and figures, but don’t stress over the page count)\nFor literature review projects: ~ 10 pages, but don’t stress over the page count – focus on what you need to include to write a compelling paper",
    "crumbs": [
      "Assignments & Evaluation"
    ]
  },
  {
    "objectID": "assignments.html#additional-assignments",
    "href": "assignments.html#additional-assignments",
    "title": "Assignments & Evaluation",
    "section": "Additional assignments",
    "text": "Additional assignments\nIn addition to the project/literature review, each student will also lead at least one paper discussion during the semester, which will be either a discussion of a paper describing or applying a study design or a critique of a paper aiming to address a causal question. When leading the paper discussions, we recommend starting off with drawing a diagram of the causal relationships in question in the paper.\nWhy is this important? Part of learning this material well is learning it to the point where you can explain it to other people. This is true in general in graduate or post-graduate school as you move towards becoming experts and in this class when presenting technical things. The presentations offer an opportunity to practice these skills in a friendly setting.\nPresentation Sign Ups",
    "crumbs": [
      "Assignments & Evaluation"
    ]
  },
  {
    "objectID": "assignments.html#readings",
    "href": "assignments.html#readings",
    "title": "Assignments & Evaluation",
    "section": "Readings",
    "text": "Readings\nThe course reading list will include journal articles and book chapters that we will make available to you on our course website. Throughout the semester, the class will compile a running glossary of “jargon” and terms and concepts in causal inference to provide notes as reference. We (the instructors) will provide a bibliography of additional readings and coding resources on various topics for students as reference.\nThe readings differ in their depth and use of math and notation. You do not need to understand all the equations and notation to gain intuition about the material and concepts. Try to learn it but try not to be too stressed by it.",
    "crumbs": [
      "Assignments & Evaluation"
    ]
  },
  {
    "objectID": "assignments.html#course-glossary",
    "href": "assignments.html#course-glossary",
    "title": "Assignments & Evaluation",
    "section": "Course glossary",
    "text": "Course glossary\nAs noted above, as a class we will compile a running glossary of “jargon” and terms and concepts in causal inference to provide notes as reference.",
    "crumbs": [
      "Assignments & Evaluation"
    ]
  },
  {
    "objectID": "assignments.html#ideas-for-literature-review-topics-final-project-option-2",
    "href": "assignments.html#ideas-for-literature-review-topics-final-project-option-2",
    "title": "Assignments & Evaluation",
    "section": "Ideas for literature review topics (Final project option 2)",
    "text": "Ideas for literature review topics (Final project option 2)\nYou are not limited to these topics, but here are some ideas:\n\nChallenges and solutions for experimental designs and interpretation (choose one):\n\nNon-compliance and attrition (this is a rich area!)\n\nInterference between experimental units and spillovers (G&G Ch8)\n\nPartial identification (excludability violations, including those common to ecological field experiments)\n\nMechanisms and mediation analysis (G&G Ch 10)\n\nLimits to what experiments tell us about heterogenous treatment effects\n\nIntegration of experimental and quasi-experimental findings (e.g., meta-analyses and extrapolation)\n\n\nNew issues for difference-in-difference and two-way fixed effects estimators (e.g., staggered treatments and heterogeneity\n\nSynthetic control methods (i.e. see https://mixtape.scunning.com/10-synthetic_control)\n\nMachine learning and causal inference\n\nSensitivity analyses and/or placebo designs\n\nMechanism (Mediation) Analysis; Sequential G estimation\n\nRemote sensing and causal inference (e.g. what to do with systematic measurement error !)\nCausal identification in time series\n\nInferences in observational designs and networks\n\nMeta-analyses using observational data",
    "crumbs": [
      "Assignments & Evaluation"
    ]
  },
  {
    "objectID": "assignments.html#deadlines",
    "href": "assignments.html#deadlines",
    "title": "Assignments & Evaluation",
    "section": "Deadlines",
    "text": "Deadlines\n\n\n\nAssignment\nDeadline\n\n\n\n\nPre-course survey\nTuesday 1/14\n\n\nIdentify project focus (option 1 or 2). Identify research questions + create and workshop DAGs for it.\nDue 2/7 (by email)\n\n\nRevised draft of Project Question and revised DAG OR literature review proposal (2 page max)\n2/19 (by email)\n\n\nOne on One Consultations ~March 17th with Laura\n\n\n\nShort presentation on research questions + DAG OR literature review topic, its significance, + proposed methods (prelim results)\nWednesday 4/3 (in class)\n\n\nPresentation of final project\nApril 23rd & 28th (in class)\n\n\nFinal project write-up due by email\nSunday 5/4",
    "crumbs": [
      "Assignments & Evaluation"
    ]
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Course Materials",
    "section": "",
    "text": "Core Readings\nSlides\nDatasets"
  },
  {
    "objectID": "policies.html",
    "href": "policies.html",
    "title": "University and Course Policies",
    "section": "",
    "text": "Students and faculty are responsible for maintaining an appropriate learning environment in all instructional settings, whether in person, remote, or online. Failure to adhere to such behavioral standards may be subject to discipline. Professional courtesy and sensitivity are especially important with respect to individuals and topics dealing with race, color, national origin, sex, pregnancy, age, disability, creed, religion, sexual orientation, gender identity, gender expression, veteran status, marital status, political affiliation, or political philosophy.\nFor more information, see the classroom behavior policy, the Student Code of Conduct, and the Office of Institutional Equity and Compliance.",
    "crumbs": [
      "University Policies"
    ]
  },
  {
    "objectID": "policies.html#classroom-behavior",
    "href": "policies.html#classroom-behavior",
    "title": "University and Course Policies",
    "section": "",
    "text": "Students and faculty are responsible for maintaining an appropriate learning environment in all instructional settings, whether in person, remote, or online. Failure to adhere to such behavioral standards may be subject to discipline. Professional courtesy and sensitivity are especially important with respect to individuals and topics dealing with race, color, national origin, sex, pregnancy, age, disability, creed, religion, sexual orientation, gender identity, gender expression, veteran status, marital status, political affiliation, or political philosophy.\nFor more information, see the classroom behavior policy, the Student Code of Conduct, and the Office of Institutional Equity and Compliance.",
    "crumbs": [
      "University Policies"
    ]
  },
  {
    "objectID": "policies.html#requirements-for-covid-19",
    "href": "policies.html#requirements-for-covid-19",
    "title": "University Policies",
    "section": "Requirements for COVID-19",
    "text": "Requirements for COVID-19\nAs a matter of public health and safety, all members of the CU Boulder community and all visitors to campus must follow university, department and building requirements and all public health orders in place to reduce the risk of spreading infectious disease. CU Boulder currently requires COVID-19 vaccination and boosters for all faculty, staff and students. Students, faculty and staff must upload proof of vaccination and boosters or file for an exemption based on medical, ethical or moral grounds through the MyCUHealth portal.\nThe CU Boulder campus is currently mask-optional. However, if public health conditions change and masks are again required in classrooms, students who fail to adhere to masking requirements will be asked to leave class, and students who do not leave class when asked or who refuse to comply with these requirements will be referred to Student Conduct and Conflict Resolution. For more information, see the policy on classroom behavior and the Student Code of Conduct. If you require accommodation because a disability prevents you from fulfilling these safety measures, please follow the steps in the “Accommodation for Disabilities” statement on this syllabus.\nIf you feel ill and think you might have COVID-19, if you have tested positive for COVID-19, or if you are unvaccinated or partially vaccinated and have been in close contact with someone who has COVID-19, you should stay home and follow the further guidance of the Public Health Office (contacttracing@colorado.edu). If you are fully vaccinated and have been in close contact with someone who has COVID-19, you do not need to stay home; rather, you should self-monitor for symptoms and follow the further guidance of the Public Health Office (contacttracing@colorado.edu).",
    "crumbs": [
      "University Policies"
    ]
  },
  {
    "objectID": "policies.html#accommodation-for-disabilities",
    "href": "policies.html#accommodation-for-disabilities",
    "title": "University Policies",
    "section": "Accommodation for Disabilities",
    "text": "Accommodation for Disabilities\nIf you qualify for accommodations because of a disability, please submit your accommodation letter from Disability Services to your faculty member in a timely manner so that your needs can be addressed. Disability Services determines accommodations based on documented disabilities in the academic environment. Information on requesting accommodations is located on the Disability Services website. Contact Disability Services at 303-492-8671 or dsinfo@colorado.edu for further assistance. If you have a temporary medical condition, see Temporary Medical Conditions on the Disability Services website.",
    "crumbs": [
      "University Policies"
    ]
  },
  {
    "objectID": "policies.html#preferred-student-names-and-pronouns",
    "href": "policies.html#preferred-student-names-and-pronouns",
    "title": "University and Course Policies",
    "section": "Preferred Student Names and Pronouns",
    "text": "Preferred Student Names and Pronouns\nCU Boulder recognizes that students’ legal information doesn’t always align with how they identify. Students may update their preferred names and pronouns via the student portal; those preferred names and pronouns are listed on instructors’ class rosters. In the absence of such updates, the name that appears on the class roster is the student’s legal name.",
    "crumbs": [
      "University Policies"
    ]
  },
  {
    "objectID": "policies.html#honor-code",
    "href": "policies.html#honor-code",
    "title": "University and Course Policies",
    "section": "Honor Code",
    "text": "Honor Code\nAll students enrolled in a University of Colorado Boulder course are responsible for knowing and adhering to the Honor Code. Violations of the Honor Code may include but are not limited to: plagiarism (including use of paper writing services or technology [such as essay bots]), cheating, fabrication, lying, bribery, threat, unauthorized access to academic materials, clicker fraud, submitting the same or similar work in more than one course without permission from all course instructors involved, and aiding academic dishonesty. Understanding the course’s syllabus is a vital part in adhering to the Honor Code.\nAll incidents of academic misconduct will be reported to Student Conduct & Conflict Resolution: StudentConduct@colorado.edu. Students found responsible for violating the Honor Code will be assigned resolution outcomes from the Student Conduct & Conflict Resolution as well as be subject to academic sanctions from the faculty member. Visit Honor Code for more information on the academic integrity policy.",
    "crumbs": [
      "University Policies"
    ]
  },
  {
    "objectID": "policies.html#sexual-misconduct-discrimination-harassment-andor-related-retaliation",
    "href": "policies.html#sexual-misconduct-discrimination-harassment-andor-related-retaliation",
    "title": "University and Course Policies",
    "section": "Sexual Misconduct, Discrimination, Harassment and/or Related Retaliation",
    "text": "Sexual Misconduct, Discrimination, Harassment and/or Related Retaliation\nCU Boulder is committed to fostering an inclusive and welcoming learning, working, and living environment. University policy prohibits protected-class discrimination and harassment, sexual misconduct (harassment, exploitation, and assault), intimate partner abuse (dating or domestic violence), stalking, and related retaliation by or against members of our community on- and off-campus. The Office of Institutional Equity and Compliance (OIEC) addresses these concerns, and individuals who have been subjected to misconduct can contact OIEC at 303-492-2127 or email CUreport@colorado.edu. Information about university policies, reporting options, and OIEC support resources including confidential services can be found on the OIEC website.\nPlease know that faculty and graduate instructors are required to inform OIEC when they are made aware of incidents related to these concerns regardless of when or where something occurred. This is to ensure that individuals impacted receive outreach from OIEC about their options and support resources. To learn more about reporting and support for a variety of concerns, visit the Don’t Ignore It page.",
    "crumbs": [
      "University Policies"
    ]
  },
  {
    "objectID": "policies.html#religious-holidays",
    "href": "policies.html#religious-holidays",
    "title": "University and Course Policies",
    "section": "Religious Holidays",
    "text": "Religious Holidays\nCampus policy regarding religious observances requires that faculty make every effort to deal reasonably and fairly with all students who, because of religious obligations, have conflicts with scheduled exams, assignments or required attendance. See the campus policy regarding religious observances for full details.",
    "crumbs": [
      "University Policies"
    ]
  },
  {
    "objectID": "DAGs.html",
    "href": "DAGs.html",
    "title": "DAGs",
    "section": "",
    "text": "Discussion of how to identify associations, close backdoors, and isolate pathways in a DAG by A. Heiss: https://www.youtube.com/watch?v=_qs_1B4ySWY\nWatch Hernan video: “3. Elements of DAGs”\n\n\n\n\n\nHow to build and begin to analyze them it in ggdag by A. Heiss: https://www.youtube.com/watch?v=uoAjyyToUTE"
  },
  {
    "objectID": "DAGs.html#useful-videos",
    "href": "DAGs.html#useful-videos",
    "title": "DAGs",
    "section": "Useful Videos",
    "text": "Useful Videos\n\nConceptual\n\nDiscussion of how to identify associations, close backdoors, and isolate pathways in a DAG by A. Heiss: https://www.youtube.com/watch?v=_qs_1B4ySWY\nWatch Hernan video: “3. Elements of DAGs”\n\n\n\nImplementation\n\nHow to build and begin to analyze them it in ggdag by A. Heiss: https://www.youtube.com/watch?v=uoAjyyToUTE\nDrawing DAGs with Daggit.net by A. Heiss: https://www.youtube.com/watch?v=3euqrnD9w7c\nIntro to DAGitty for identifying confounding variables: https://www.youtube.com/watch?v=sI_w9uVLdCg",
    "crumbs": [
      "DAG resources"
    ]
  },
  {
    "objectID": "DAGs.html#software-for-dag-construction-and-analysis",
    "href": "DAGs.html#software-for-dag-construction-and-analysis",
    "title": "DAGs",
    "section": "Software for DAG construction and analysis",
    "text": "Software for DAG construction and analysis\na) Daggitity\nb) ggdag is a nice R package based on dagitty but in R tidyverse-compatible and with much better plotting functionality\nc) shinydag is another GUI aimed at visualizing DAGs and exporting them in different publication-ready formats\nd) TETRAD\ne) DAG program\nf) dagR\n\nKey Messages - well said in Cunningham (2021) Ch. 3:\nWell summarized fom Cunningham (2021) Ch. 3: “Causal effects can happen in two ways. They can either be direct (e.g., D→Y), or they can be mediated by a third variable (e.g., D→X→Y). When they are mediated by a third variable, we are capturing a sequence of events originating with D, which may or may not be important to you depending on the question you’re asking.\nA DAG is meant to describe all causal relationships relevant to the effect of D on Y. What makes the DAG distinctive is both the explicit commitment to a causal effect pathway and the complete commitment to the lack of a causal pathway represented by missing arrows. In other words, a DAG will contain both arrows connecting variables and choices to exclude arrows. And the lack of an arrow necessarily means that you think there is no such relationship in the data—this is one of the strongest beliefs you can hold. A complete DAG will have all direct causal effects among the variables in the graph as well as all common causes of any pair of variables in the graph.\nAt this point, you may be wondering where the DAG comes from. It’s an excellent question. It may be the question. A DAG is supposed to be a theoretical representation of the state-of-the-art knowledge about the phenomena you’re studying. It’s what an expert would say is the thing itself, and that expertise comes from a variety of sources. Examples include economic theory, other scientific models, conversations with experts, your own observations and experiences, literature reviews, as well as your own intuition and hypotheses….\n…. I have found DAGs to be useful for understanding the critical role that prior knowledge plays in identifying causal effects. But there are other reasons too. One, I have found that DAGs are very helpful for communicating research designs and estimators if for no other reason than pictures speak a thousand words…..\n….. Two, through concepts such as the backdoor criterion and collider bias, a well-designed DAG can help you develop a credible research design for identifying the causal effects of some intervention. As a bonus, I also think a DAG provides a bridge between various empirical schools, such as the structural and reduced form groups. And finally, DAGs drive home the point that assumptions are necessary for any and all identification of causal effects, which economists have been hammering at for years (Wolpin 2013).”\n\nAdditional Background Readings\nFor additional background on DAGs:\n\nMorgan & Winship (2007) pgs. 29-34 and Ch. 3 (see GDrive)\nCunningham (2021) Ch. 3\nHeiss, A. Do-calculus adventures! Exploring the three rules of do-calculus in plain language and deriving the backdoor adjustment formula by hand",
    "crumbs": [
      "DAG resources"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-113",
    "href": "readings.html#due-mon.-113",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 1/13",
    "text": "Due Mon. 1/13\nFirst class; Complete pre-course survey in class; Review Syllabus and website",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-115-introduction-continued",
    "href": "readings.html#due-wed.-115-introduction-continued",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 1/15 Introduction Continued",
    "text": "Due Wed. 1/15 Introduction Continued\n\nHernan et al. 2019\nAngrist & Pischke 2008, Chapter 1 (from Mostly Harmless Econometrics)\nGerber & Green, Chapter 1 (you can just skim this one)",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-120--no-class-mlk-day",
    "href": "readings.html#due-mon.-120--no-class-mlk-day",
    "title": "Readings & Additional Materials",
    "section": "Due Mon. 1/20- No Class MLK day",
    "text": "Due Mon. 1/20- No Class MLK day\nPotential Outcomes 1) Angrist & Pischke 2008, Chapter 2",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-122-structual-causal-model",
    "href": "readings.html#due-mon.-122-structual-causal-model",
    "title": "Readings & Additional Materials",
    "section": "Due Mon. 1/22 Structual Causal Model",
    "text": "Due Mon. 1/22 Structual Causal Model\nGuest speaker: Dr. Zach Laubach\n\nRohrer 2018\nLaubach et al. 2021",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-120---no-class-mlk-day",
    "href": "readings.html#due-mon.-120---no-class-mlk-day",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 1/20 - No Class MLK day",
    "text": "Due Mon. 1/20 - No Class MLK day",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-122-potential-outcomes",
    "href": "readings.html#due-wed.-122-potential-outcomes",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 1/22 Potential Outcomes",
    "text": "Due Wed. 1/22 Potential Outcomes\n\nAngrist & Pischke 2008, Chapter 2 (from Mostly Harmless Econometrics)\nOR ## Due Mon. 1/22 Structural Causal Model and Directed Acyclic Graphs (DAG) Guest speaker: Dr. Zach Laubach\nRohrer 2018\nLaubach et al. 2021\n\nOptional; additional background reading on DAGs:\n4) Morgan & Winship (2007) pgs. 29-34 and Ch. 3 5) Cunningham (2021) Ch. 3",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-127-scm-or-po",
    "href": "readings.html#due-mon.-127-scm-or-po",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 1/27 SCM or PO",
    "text": "Due Mon. 1/27 SCM or PO",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-129-experiments-and-randomization",
    "href": "readings.html#due-wed.-129-experiments-and-randomization",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 1/29 Experiments and Randomization",
    "text": "Due Wed. 1/29 Experiments and Randomization\n\nGerber & Green, Chapter 2\nKimmel et al. 2021\n\nOptional: Arif & Massey (2023) Reducing bias in experimental ecology through directed acyclic graphs",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-23-introduction-to-challenges-of-observational-data",
    "href": "readings.html#due-mon.-23-introduction-to-challenges-of-observational-data",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 2/3 Introduction to Challenges of Observational Data",
    "text": "Due Mon. 2/3 Introduction to Challenges of Observational Data\n\nWatch Imbens video: 2022 Nobel Prize lecture\nLarsen et al. 2019 - up to section 8, and the Discussion. Can skim in between for preview of what’s to come.\nSiegel & Dee (2025) Ecology Letters - up until section 5.1.\n\nOptional and more advanced: Angrist & Pischke 2008, Chapter 3 (from Mostly Harmless Econometrics)\nOptional: - Ferraro 2009\n- Ferraro and Hanauer 2014 Advances in Measuring the Environmental and Social Impacts of Environmental Programs",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-25---laura-away-class-is-asynchronous",
    "href": "readings.html#due-wed.-25---laura-away-class-is-asynchronous",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 2/5 - Laura Away; Class is asynchronous",
    "text": "Due Wed. 2/5 - Laura Away; Class is asynchronous\nDuring the class period, start your projects and make a first DAG. You will answer a set of questions in the template on the Google Drive based on your question and DAG.\nReview the DAG resources page tab on the website, and send Laura your DAG for feedback and dicussion with the class. Be prepared to present and discuss your DAG.\n\nArif & MacNeil 2022\n\nWatch Hernan video: “3. Elements of DAGs” on Canvas\nCheck out DAG software and website tab on DAGs",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-210-matching",
    "href": "readings.html#due-mon.-210-matching",
    "title": "Readings & Additional Materials",
    "section": "Due Mon. 2/10 Matching",
    "text": "Due Mon. 2/10 Matching\n\nRamsey et al. 2018\nStuart 2010",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-212-discussion-leads-____",
    "href": "readings.html#due-wed.-212-discussion-leads-____",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 2/12 [Discussion lead(s): ____ ]",
    "text": "Due Wed. 2/12 [Discussion lead(s): ____ ]\n\nSiegel et al. 2022\n\nXu et al. 2022\n\ndemo RMarkdowns from Siegel & Dee (2025) for weighting and matching on course GitHub",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-217",
    "href": "readings.html#due-mon.-217",
    "title": "Readings & Additional Materials",
    "section": "Due Mon. 2/17",
    "text": "Due Mon. 2/17",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-219",
    "href": "readings.html#due-wed.-219",
    "title": "Readings & Additional Materials",
    "section": "Due Wed. 2/19",
    "text": "Due Wed. 2/19",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-227-difference-in-difference",
    "href": "readings.html#due-mon.-227-difference-in-difference",
    "title": "Readings & Additional Materials",
    "section": "Due Mon. 2/27 Difference-in-Difference",
    "text": "Due Mon. 2/27 Difference-in-Difference",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-36-two-way-fixed-effects",
    "href": "readings.html#due-mon.-36-two-way-fixed-effects",
    "title": "Readings & Additional Materials",
    "section": "Due Mon. 3/6 Two-way Fixed Effects",
    "text": "Due Mon. 3/6 Two-way Fixed Effects\n\nAngrist & Pischke 2008, Chapter 5",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-313-instrumental-variables",
    "href": "readings.html#due-mon.-313-instrumental-variables",
    "title": "Readings & Additional Materials",
    "section": "Due Mon. 3/13 Instrumental Variables",
    "text": "Due Mon. 3/13 Instrumental Variables\n\nAngrist & Pischke 2015, Chapter 3 (note: this is in Mastering ’Metrics)\n\nKendall book chapter (on Canvas)",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-36-two-way-fixed-effects-or-synthetic-control-asia-kaiser",
    "href": "readings.html#due-mon.-36-two-way-fixed-effects-or-synthetic-control-asia-kaiser",
    "title": "Readings & Additional Materials",
    "section": "Due Mon. 3/6 Two-way Fixed Effects or Synthetic Control (Asia Kaiser)",
    "text": "Due Mon. 3/6 Two-way Fixed Effects or Synthetic Control (Asia Kaiser)",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-320-rdd",
    "href": "readings.html#due-mon.-320-rdd",
    "title": "Readings & Additional Materials",
    "section": "Due Mon. 3/20 RDD",
    "text": "Due Mon. 3/20 RDD\n\nAngrist & Pischke 2015, Chapter 4 (note: this is in Mastering ’Metrics)",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-217-difference-in-difference",
    "href": "readings.html#due-mon.-217-difference-in-difference",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 2/17 Difference-in-Difference",
    "text": "Due Mon. 2/17 Difference-in-Difference\n\nAngrist & Pischke 2015, Chapter 5 (note: this is in the book Mastering ’Metrics)\n\nThe Effect Ch. 16 - Fixed Effects up until 16.2.2 Multiple Sets of Fixed Effects.\nThe Effect Ch. 18 up until 18.2.5 Rollout Designs and Multiple Treatment Periods.",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-219-discussion-leads-____",
    "href": "readings.html#due-wed.-219-discussion-leads-____",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 2/19 [Discussion lead(s): ____ ]",
    "text": "Due Wed. 2/19 [Discussion lead(s): ____ ]\nDUE Draft of Revised DAG OR literature review proposal (1 page max.)\n1) Simler-Williamson & Germino 2022\ndemo RMarkdown using Boulder Open Space fire and vegetation case on course GitHub (data in GoogleDrive)",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.",
    "href": "readings.html#due-mon.",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon.",
    "text": "Due Mon.",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.",
    "href": "readings.html#due-wed.",
    "title": "Readings & Additional Materials",
    "section": "Due Wed.",
    "text": "Due Wed.",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-210-matching-and-weighting",
    "href": "readings.html#due-mon.-210-matching-and-weighting",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 2/10 Matching (and weighting)",
    "text": "Due Mon. 2/10 Matching (and weighting)\n\nRamsey et al. 2018\nStuart 2010\n\nOptional: -Chesnaye et al. 2022\n-McCaffery et al. 2013 A Tutorial on Propensity Score Estimation for Multiple Treatments Using Generalized Boosted Models",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-224-two-way-fixed-effects",
    "href": "readings.html#due-mon.-224-two-way-fixed-effects",
    "title": "Readings & Additional Materials",
    "section": "Due Mon. 2/24 Two-way fixed effects",
    "text": "Due Mon. 2/24 Two-way fixed effects\n\nAngrist & Pischke 2008, Chapter 5",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-226---synthetic-control-asia-kaiser-laura-away",
    "href": "readings.html#due-wed.-226---synthetic-control-asia-kaiser-laura-away",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 2/26 - Synthetic Control (Asia Kaiser); Laura away",
    "text": "Due Wed. 2/26 - Synthetic Control (Asia Kaiser); Laura away",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-224-two-way-fixed-effects-part-1-or-recent-did-extensions",
    "href": "readings.html#due-mon.-224-two-way-fixed-effects-part-1-or-recent-did-extensions",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 2/24 Two-way fixed effects, Part 1 (or recent DiD extensions)",
    "text": "Due Mon. 2/24 Two-way fixed effects, Part 1 (or recent DiD extensions)\n\nAngrist & Pischke 2008, Chapter 5\nThe Effect Ch. 16 - Fixed Effects starting at 16.2.2 Multiple Sets of Fixed Effects\nBrynes and Dee (2025 Ecology Letters)",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-33-two-way-fixed-effects",
    "href": "readings.html#due-mon.-33-two-way-fixed-effects",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 3/3 Two-way fixed effects",
    "text": "Due Mon. 3/3 Two-way fixed effects",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-35-draft-of-dag-or-literature-review-proposal-1-page-max.",
    "href": "readings.html#due-wed.-35-draft-of-dag-or-literature-review-proposal-1-page-max.",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 3/5 Draft of DAG OR literature review proposal (1 page max.)",
    "text": "Due Wed. 3/5 Draft of DAG OR literature review proposal (1 page max.)",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-310-instrumental-variables",
    "href": "readings.html#due-mon.-310-instrumental-variables",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 3/10 Instrumental Variables",
    "text": "Due Mon. 3/10 Instrumental Variables\n\nAngrist & Pischke 2015, Chapter 3 (note: this is in Mastering ’Metrics)\n\nKendall book chapter (in GDrive)",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-312-instrumental-variables-applications-discussion-leads-____",
    "href": "readings.html#due-wed.-312-instrumental-variables-applications-discussion-leads-____",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 3/12 Instrumental Variables Applications [Discussion lead(s): ____ ]",
    "text": "Due Wed. 3/12 Instrumental Variables Applications [Discussion lead(s): ____ ]\n\nSims 2010\n\nMacDonald & Mordecai 2020\ndemo",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-317-regression-discontinuity-designs",
    "href": "readings.html#due-mon.-317-regression-discontinuity-designs",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 3/17 Regression Discontinuity Designs",
    "text": "Due Mon. 3/17 Regression Discontinuity Designs\n\nThe Effect Ch. 20 Optional: Angrist & Pischke 2015, Chapter 4 (note: this is in Mastering ’Metrics in the GDrive)",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-319-rdd-applications-discussion-leads-____",
    "href": "readings.html#due-wed.-319-rdd-applications-discussion-leads-____",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 3/19 RDD Applications [Discussion lead(s): ____ ]",
    "text": "Due Wed. 3/19 RDD Applications [Discussion lead(s): ____ ]\n\nEnglander 2019\n\nNoack et al. 2022\nOptional paper: Burgess et al. 2019",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#week-of-march-24th---spring-break-no-classes-this-week",
    "href": "readings.html#week-of-march-24th---spring-break-no-classes-this-week",
    "title": "Readings by Class & Additional Materials",
    "section": "Week of March 24th - Spring Break, No Classes this week",
    "text": "Week of March 24th - Spring Break, No Classes this week",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-monday-331-comparison-of-designs",
    "href": "readings.html#due-monday-331-comparison-of-designs",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Monday 3/31 Comparison of Designs",
    "text": "Due Monday 3/31 Comparison of Designs\n\nSiegel & Dee (2025) Ecology Letters",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-42-discussion-leads-____",
    "href": "readings.html#due-wed.-42-discussion-leads-____",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 4/2 [Discussion lead(s): ____ ]",
    "text": "Due Wed. 4/2 [Discussion lead(s): ____ ]\nRevisit these papers in light of the assumptions of different designs, their different estimands, generalizability, and their assumptions:  - Simler-Williamson & Germino 2022\n- Dee et al., 2023 - TBD on sensitivity and placebo tests",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-project",
    "href": "readings.html#due-mon.-project",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. Project",
    "text": "Due Mon. Project\nBrief presentation (3-5 minutes) of your project: the question you are addressing, its applications, your DAG (if applicable) and your proposed method for feedback and Q&A",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-project",
    "href": "readings.html#due-wed.-project",
    "title": "Readings & Additional Materials",
    "section": "Due Wed. Project",
    "text": "Due Wed. Project",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-generalizability",
    "href": "readings.html#due-mon.-generalizability",
    "title": "Readings & Additional Materials",
    "section": "Due Mon. Generalizability",
    "text": "Due Mon. Generalizability",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-inference",
    "href": "readings.html#due-wed.-inference",
    "title": "Readings & Additional Materials",
    "section": "Due Wed. Inference",
    "text": "Due Wed. Inference",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-sensitivity-analyses-project-consultations",
    "href": "readings.html#due-wed.-sensitivity-analyses-project-consultations",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. Sensitivity Analyses – Project Consultations",
    "text": "Due Wed. Sensitivity Analyses – Project Consultations",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-generalizability-in-experimental-and-observational-studies",
    "href": "readings.html#due-mon.-generalizability-in-experimental-and-observational-studies",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. Generalizability in experimental and observational studies",
    "text": "Due Mon. Generalizability in experimental and observational studies",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-inference-and-standard-error-estimation",
    "href": "readings.html#due-wed.-inference-and-standard-error-estimation",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. Inference and standard error estimation",
    "text": "Due Wed. Inference and standard error estimation\nRevised DAG + proposed methods OR outline of literature review due",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-33-two-way-fixed-effects-did-extensions-and-issues",
    "href": "readings.html#due-mon.-33-two-way-fixed-effects-did-extensions-and-issues",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 3/3 Two-way fixed effects / DiD extensions and issues",
    "text": "Due Mon. 3/3 Two-way fixed effects / DiD extensions and issues",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-power-standard-error-estimation-and-inference",
    "href": "readings.html#due-wed.-power-standard-error-estimation-and-inference",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. Power, standard error estimation, and inference",
    "text": "Due Wed. Power, standard error estimation, and inference\nRevised DAG + proposed methods OR outline of literature review due",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-monday-331-comparison-of-designs-sensitivity-tests",
    "href": "readings.html#due-monday-331-comparison-of-designs-sensitivity-tests",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Monday 3/31 Comparison of Designs & Sensitivity Tests",
    "text": "Due Monday 3/31 Comparison of Designs & Sensitivity Tests\n\nSiegel & Dee (2025) Ecology Letters\nButsic et al. 2017\nArif & MacNiel 2021 Ecosphere\nTBD on sensitivity tests",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-project-consultations",
    "href": "readings.html#due-wed.-project-consultations",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. Project Consultations",
    "text": "Due Wed. Project Consultations\n1:1 consultations with Laura in lieu of class. Schedule to sign up here",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-35",
    "href": "readings.html#due-wed.-35",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 3/5",
    "text": "Due Wed. 3/5",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-heterogeniety-generalizability-in-experimental-and-observational-studies",
    "href": "readings.html#due-mon.-heterogeniety-generalizability-in-experimental-and-observational-studies",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. Heterogeniety & Generalizability in experimental and observational studies",
    "text": "Due Mon. Heterogeniety & Generalizability in experimental and observational studies",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-first-project-presentations",
    "href": "readings.html#due-mon.-first-project-presentations",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. First Project Presentations",
    "text": "Due Mon. First Project Presentations\nBrief presentation (3-5 minutes) of your project: the question you are addressing, its applications, your DAG (if applicable) and your proposed method for feedback and Q&A",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-226---synthetic-control-asia-kaiser-laura-away-or-twfe-papers-discussion",
    "href": "readings.html#due-wed.-226---synthetic-control-asia-kaiser-laura-away-or-twfe-papers-discussion",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 2/26 - Synthetic Control (Asia Kaiser); Laura away or TWFE papers discussion",
    "text": "Due Wed. 2/26 - Synthetic Control (Asia Kaiser); Laura away or TWFE papers discussion",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-33-two-way-fixed-effects-discussion-or-synth-control-did-extensions-and-issues",
    "href": "readings.html#due-mon.-33-two-way-fixed-effects-discussion-or-synth-control-did-extensions-and-issues",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 3/3 Two-way fixed effects discussion or Synth control / DiD extensions and issues",
    "text": "Due Mon. 3/3 Two-way fixed effects discussion or Synth control / DiD extensions and issues\n\nDudney et al. 2021",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-35-two-way-fixed-effects-did-extensions-and-issues",
    "href": "readings.html#due-wed.-35-two-way-fixed-effects-did-extensions-and-issues",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 3/5 Two-way fixed effects / DiD extensions and issues",
    "text": "Due Wed. 3/5 Two-way fixed effects / DiD extensions and issues",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-226--laura-away-synthetic-control-asia-kaiser-or-twfe-papers-discussion",
    "href": "readings.html#due-wed.-226--laura-away-synthetic-control-asia-kaiser-or-twfe-papers-discussion",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 2/26 -Laura away: Synthetic Control (Asia Kaiser); or TWFE papers discussion",
    "text": "Due Wed. 2/26 -Laura away: Synthetic Control (Asia Kaiser); or TWFE papers discussion",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-33-synthetic-control-asia-kaiser-or-twfe-papers-discussion",
    "href": "readings.html#due-mon.-33-synthetic-control-asia-kaiser-or-twfe-papers-discussion",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 3/3 Synthetic Control (Asia Kaiser); or TWFE papers discussion",
    "text": "Due Mon. 3/3 Synthetic Control (Asia Kaiser); or TWFE papers discussion\n\nDudney et al. 2021\n\nDee et al., 2023",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-226--laura-away-fixed-effects-discussion",
    "href": "readings.html#due-wed.-226--laura-away-fixed-effects-discussion",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 2/26 -Laura away: Fixed Effects Discussion",
    "text": "Due Wed. 2/26 -Laura away: Fixed Effects Discussion\n\nBrief Review – Brynes and Dee (2025 Ecology Letters)\nDudney et al. 2021\n\nDee et al., 2023\nMeehan et al. 2011 vs. Larsen 2013\n\ndemo Dee et al. Rmarkdown on course GitHub",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-35-did-extensions-and-issues",
    "href": "readings.html#due-wed.-35-did-extensions-and-issues",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 3/5 DiD extensions and issues",
    "text": "Due Wed. 3/5 DiD extensions and issues\n\nBrynes and Dee (2025) Ecology Letters\nThe Effect Ch. 18 starting at 18.2.5 Rollout Designs and Multiple Treatment Periods.\n\ndemo Brynes & Dee Rmarkdown on [GitHub here] (https://github.com/jebyrnes/ovb_yeah_you_know_me/tree/v1.0.2)\nOptional: 1) Roth et al. (2023). What’s trending in difference in difference? 2) Pedro Sant’Anna’s DiD resources and Comprehensive Course on DiD 3)Bell et al. 2019. Fixed and random effects models making an informed choice",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-47-first-project-presentations",
    "href": "readings.html#due-mon.-47-first-project-presentations",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 4/7 First Project Presentations",
    "text": "Due Mon. 4/7 First Project Presentations\nBrief presentation (3-5 minutes) of your project: the question you are addressing, its applications, your DAG (if applicable) and your proposed method for feedback and Q&A",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-49-project-consultations",
    "href": "readings.html#due-wed.-49-project-consultations",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 4/9 Project Consultations",
    "text": "Due Wed. 4/9 Project Consultations\n1:1 consultations with Laura in lieu of class. Schedule to sign up here",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-414-heterogeniety-generalizability-in-experimental-and-observational-studies",
    "href": "readings.html#due-mon.-414-heterogeniety-generalizability-in-experimental-and-observational-studies",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 4/14 Heterogeniety & Generalizability in experimental and observational studies",
    "text": "Due Mon. 4/14 Heterogeniety & Generalizability in experimental and observational studies",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-4-16-power-standard-error-estimation-and-inference",
    "href": "readings.html#due-wed.-4-16-power-standard-error-estimation-and-inference",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 4/ 16 Power, standard error estimation, and inference",
    "text": "Due Wed. 4/ 16 Power, standard error estimation, and inference\nRevised DAG + proposed methods OR outline of literature review due",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-421",
    "href": "readings.html#due-mon.-421",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 4/21",
    "text": "Due Mon. 4/21",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed-423",
    "href": "readings.html#due-wed-423",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed 4/23",
    "text": "Due Wed 4/23",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-428",
    "href": "readings.html#due-mon.-428",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 4/28",
    "text": "Due Mon. 4/28",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed-430",
    "href": "readings.html#due-wed-430",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed 4/30",
    "text": "Due Wed 4/30",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-55",
    "href": "readings.html#due-mon.-55",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 5/5",
    "text": "Due Mon. 5/5",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-57",
    "href": "readings.html#due-wed.-57",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 5/7",
    "text": "Due Wed. 5/7",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-55-project-presentations",
    "href": "readings.html#due-mon.-55-project-presentations",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 5/5 Project Presentations",
    "text": "Due Mon. 5/5 Project Presentations\nProject Presentations\nGuidelines for all presentations:\n\n8 minute presentation + 2 minutes for questions\n\nIntroduce the question you address and the motivation for your project\n\nSpecific guidelines for data analysis project presentations:\n\nDescribe the causal inference approach you took and why you chose that approach\n\nPresent your DAG\n\nBriefly describe the data you used\n\nShare your results AND contextualize them with the important limitations and assumptions of the method(s) you used\n\nOutline possible next steps you could take to make your analysis more robust in the future\n\nSpecific guidelines for literature review presentations:\n\nDescribe the challenge for causal inference\n\nDescribe and critique the current approaches people are taking\n\nPresent your ideas for how the field could better incorporate causal inference methods",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-57-project-presentations",
    "href": "readings.html#due-wed.-57-project-presentations",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 5/7 Project Presentations",
    "text": "Due Wed. 5/7 Project Presentations\nProject Presentations Continued",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-421-special-topics",
    "href": "readings.html#due-mon.-421-special-topics",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 4/21 Special topics",
    "text": "Due Mon. 4/21 Special topics",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed-423-special-topics-discussion",
    "href": "readings.html#due-wed-423-special-topics-discussion",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed 4/23 Special topics discussion",
    "text": "Due Wed 4/23 Special topics discussion",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-428-special-topics-or-project-work",
    "href": "readings.html#due-mon.-428-special-topics-or-project-work",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 4/28 Special topics or Project Work",
    "text": "Due Mon. 4/28 Special topics or Project Work",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed-430-special-topics-or-project-work",
    "href": "readings.html#due-wed-430-special-topics-or-project-work",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed 4/30 Special topics or Project Work",
    "text": "Due Wed 4/30 Special topics or Project Work\nProject Presentations Continued",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-421-special-topics-or-project-work",
    "href": "readings.html#due-mon.-421-special-topics-or-project-work",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 4/21 Special topics or Project Work",
    "text": "Due Mon. 4/21 Special topics or Project Work",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed-423-special-topics-discussion-or-project-work",
    "href": "readings.html#due-wed-423-special-topics-discussion-or-project-work",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed 4/23 Special topics discussion or Project Work",
    "text": "Due Wed 4/23 Special topics discussion or Project Work",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-428-project-presentations",
    "href": "readings.html#due-mon.-428-project-presentations",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 4/28 Project Presentations",
    "text": "Due Mon. 4/28 Project Presentations\nProject Presentations\nGuidelines for all presentations:\n\n8 minute presentation + 2 minutes for questions\n\nIntroduce the question you address and the motivation for your project\n\nSpecific guidelines for data analysis project presentations:\n\nDescribe the causal inference approach you took and why you chose that approach\n\nPresent your DAG\n\nBriefly describe the data you used\n\nShare your results AND contextualize them with the important limitations and assumptions of the method(s) you used\n\nOutline possible next steps you could take to make your analysis more robust in the future\nSpecific guidelines for literature review presentations:\nDescribe the challenge for causal inference\n\nDescribe and critique the current approaches people are taking\n\nPresent your ideas for how the field could better incorporate causal inference methods",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#may-1-is-last-day-of-class---projects-due-friday-may-2",
    "href": "readings.html#may-1-is-last-day-of-class---projects-due-friday-may-2",
    "title": "Readings by Class & Additional Materials",
    "section": "May 1 is last day of class - Projects Due Friday May 2",
    "text": "May 1 is last day of class - Projects Due Friday May 2\nSee Assignments & Evaluation tab for more information",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#potential-additional-units-to-be-voted-on-by-the-class",
    "href": "readings.html#potential-additional-units-to-be-voted-on-by-the-class",
    "title": "Readings by Class & Additional Materials",
    "section": "Potential additional units to be voted on by the class:",
    "text": "Potential additional units to be voted on by the class:\n\nGeneralizability in experimental and observational studies\n\nKorell et al. 2019\n\nSpake et al. 2022\n\nSpake et al. 2021\n\n\n\nHeterogeneous treatment effects\n\n\nReplication/Reproducibility/Pre-registration\n\nKimmel et al., in press\n\n\n\nDeeper dive into mechanisms and mediation analysis",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-25---laura-away-class-is-asynchronousflipped",
    "href": "readings.html#due-wed.-25---laura-away-class-is-asynchronousflipped",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 2/5 - Laura Away; Class is asynchronous/flipped",
    "text": "Due Wed. 2/5 - Laura Away; Class is asynchronous/flipped\n\nArif & MacNeil 2022\n\nWatch Hernan video: “3. Elements of DAGs” on Canvas\nCheck out DAG software and website tab on DAGs\n\nDuring the class period, start your projects and make a first DAG. You will answer a set of questions in the template on the Google Drive based on your question and DAG.\nReview the DAG resources page tab on the website, and send Laura your DAG for feedback and dicussion with the class. Be prepared to present and discuss your DAG.",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "policies.html#accommodation-for-disabilities-temporary-medical-conditions-and-medical-isolation",
    "href": "policies.html#accommodation-for-disabilities-temporary-medical-conditions-and-medical-isolation",
    "title": "University and Course Policies",
    "section": "Accommodation for Disabilities, Temporary Medical Conditions, and Medical Isolation",
    "text": "Accommodation for Disabilities, Temporary Medical Conditions, and Medical Isolation\nIf you qualify for accommodations because of a disability, please submit your accommodation letter from Disability Services to your faculty member in a timely manner so that your needs can be addressed. Disability Services determines accommodations based on documented disabilities in the academic environment. Information on requesting accommodations is located on the Disability Services website. Contact Disability Services at 303-492-8671 or DSinfo@colorado.edu for further assistance. If you have a temporary medical condition, see Temporary Medical Conditions on the Disability Services website.\nIf you have a temporary illness, injury or required medical isolation for which you require adjustment, contact the instructor to make a plan. Because of FERPA student privacy laws, you do not need to state the nature of their illness when alerting me.",
    "crumbs": [
      "University Policies"
    ]
  },
  {
    "objectID": "policies.html#mental-health-and-wellness",
    "href": "policies.html#mental-health-and-wellness",
    "title": "University and Course Policies",
    "section": "Mental Health and Wellness",
    "text": "Mental Health and Wellness\nThe University of Colorado Boulder is committed to the well-being of all students. If you are struggling with personal stressors, mental health or substance use concerns that are impacting academic or daily life, please contact Counseling and Psychiatric Services (CAPS) located in C4C or call (303) 492-2277, 24/7.\nFree and unlimited telehealth is also available through Academic Live Care. The Academic Live Care site also provides information about additional wellness services on campus that are available to students.",
    "crumbs": [
      "University Policies"
    ]
  },
  {
    "objectID": "policies.html#acceptable-use-of-ai-in-this-class",
    "href": "policies.html#acceptable-use-of-ai-in-this-class",
    "title": "University and Course Policies",
    "section": "Acceptable Use of AI in this Class",
    "text": "Acceptable Use of AI in this Class\nThe acceptable use of AI in this class is to help you write and trouble shoot code for implementing designs and models in this course, or from translating across programming languages (e.g. R to Python).\nUnacceptable uses include 1) using AI instead of reading papers, 2) using AI instead of writing or preparing your own slides or discussion materials. These uses will be regarded as academic dishonesty. They are also cheating yourself from learning.",
    "crumbs": [
      "University Policies"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-224-panel-methods-continued-two-way-fixed-effects",
    "href": "readings.html#due-mon.-224-panel-methods-continued-two-way-fixed-effects",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 2/24 Panel methods continued; Two-way fixed effects",
    "text": "Due Mon. 2/24 Panel methods continued; Two-way fixed effects\n\nAngrist & Pischke 2008, Chapter 5\nThe Effect Ch. 16 - Fixed Effects starting at 16.2.2 Multiple Sets of Fixed Effects\n\nOptional: Bell et al. 2019. Fixed and random effects models making an informed choice\nBrynes and Dee (2025) Ecology Letters",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-25---laura-away-class-is-online",
    "href": "readings.html#due-wed.-25---laura-away-class-is-online",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 2/5 - Laura Away; Class is online",
    "text": "Due Wed. 2/5 - Laura Away; Class is online\n\nArif & MacNeil 2022\n\nVanderwheele 2019 Principles of confounder selection\nReview DAG resources and DAG software on the website’s DAGs resources tab\n\nPrior to and during the class period, you will start your projects and make a first DAG. Review the DAG resources page tab on the website, and send Laura your DAG for feedback and dicussion with the class. Be prepared to present and discuss your DAG. You will answer a set of questions in the template on the Google Drive based on your question and DAG.",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-226---laura-away-fixed-effects-discussion",
    "href": "readings.html#due-wed.-226---laura-away-fixed-effects-discussion",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 2/26 - Laura away: Fixed Effects Discussion",
    "text": "Due Wed. 2/26 - Laura away: Fixed Effects Discussion\n\nDudney et al. 2021\n\nDee et al., 2023\n\nOptional (but good for the discussants!) Meehan et al. 2011 vs. Larsen 2013\ndemo Dee et al. Rmarkdown on course GitHub",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-33-synthetic-control-asia-kaiser",
    "href": "readings.html#due-mon.-33-synthetic-control-asia-kaiser",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 3/3 Synthetic Control (Asia Kaiser)",
    "text": "Due Mon. 3/3 Synthetic Control (Asia Kaiser)\n\nFick et al (2020)\n\nOptional: The Effect section 21.2.1 Synthetic Control\nFor a more complete treatment of synthetic control, see: Mixtape Ch. 10\nApplications: - Wu et al. 2023\n- Lawson and Smith 2023",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "EBIO 5460: Causal Inference in Ecology",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWebsite for EBIO 5460: Causal Inference in Ecology for Spring 2025 taught by Laura Dee. The course website url is: https://lauradee.github.io/EcologyCausalInference/\nWe made this course page using the template developed by Francisco Rodriguez-Sanchez (https://github.com/Pakillo/quarto-course-website-template), with additional inspiration from Andrew Heiss (https://www.andrewheiss.com/), Paul Ferraro’s syllabus, and based on the course and page of EBIO 5460 course, developed and taught by Laura Dee and Katherine Siegel at the University of Colorado-Boulder in Spring 2023. Link to that site: https://katherinesiegel.github.io/EBIOcausalinference",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "exercises/index.html#wednesday-25",
    "href": "exercises/index.html#wednesday-25",
    "title": "Exercises in R",
    "section": "Wednesday 2/5",
    "text": "Wednesday 2/5\nCreating and Analyzing DAGs in ggdag (or Daggity).",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/index.html#measurement-error",
    "href": "exercises/index.html#measurement-error",
    "title": "Exercises in R",
    "section": "Measurement Error",
    "text": "Measurement Error",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/index.html#sims",
    "href": "exercises/index.html#sims",
    "title": "Exercises in R",
    "section": "Sims",
    "text": "Sims",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/index.html#day-1-data-dont-speak",
    "href": "exercises/index.html#day-1-data-dont-speak",
    "title": "Exercises in R",
    "section": "Day 1: Data Don’t Speak",
    "text": "Day 1: Data Don’t Speak\nAn example of from the online book, Causal inference in R Ch 5, that demonstrates that summary statistics and data visualization are insufficient to untangle causal effects. Instead, prior knowledge, theory, and assumptions are necessary.",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/index.html#intro-data-dont-speak",
    "href": "exercises/index.html#intro-data-dont-speak",
    "title": "Exercises in R",
    "section": "Intro: Data Don’t Speak",
    "text": "Intro: Data Don’t Speak\nCode An example of from the online book, Causal inference in R Ch 5, that demonstrates that summary statistics and data visualization are insufficient to untangle causal effects. Instead, prior knowledge, theory, and assumptions are necessary.",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-122-structural-causal-model-and-directed-acyclic-graphs-dag",
    "href": "readings.html#due-wed.-122-structural-causal-model-and-directed-acyclic-graphs-dag",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 1/22 Structural Causal Model and Directed Acyclic Graphs (DAG)",
    "text": "Due Wed. 1/22 Structural Causal Model and Directed Acyclic Graphs (DAG)\nGuest speaker: Dr. Suchinta Arif\n1) Arif & MacNeil 2022\n2) Laubach et al. 2021\nOptional; additional background reading on DAGs:\n3) The Effect Ch. 6\n4) Morgan & Winship (2007) pgs. 29-34 and Ch. 3\n5) Cunningham (2021) Ch. 3\n## Due Mon. 1/27 Potential Outcomes 1) Angrist & Pischke 2008, Chapter 2 (from Mostly Harmless Econometrics)\nOptional: Morgan & Winship Book, Ch. 2",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-127-potential-outcomes",
    "href": "readings.html#due-mon.-127-potential-outcomes",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 1/27 Potential Outcomes",
    "text": "Due Mon. 1/27 Potential Outcomes\n\nAngrist & Pischke 2008, Chapter 2 (from Mostly Harmless Econometrics)\nOptional: Morgan & Winship Book, Ch. 2",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-25---laura-away-class-is-online-or-dr.-zach-laubach",
    "href": "readings.html#due-wed.-25---laura-away-class-is-online-or-dr.-zach-laubach",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 2/5 - Laura Away; Class is online or Dr. Zach Laubach",
    "text": "Due Wed. 2/5 - Laura Away; Class is online or Dr. Zach Laubach\n\n\nRohrer 2018\n\nReview Arif & MacNeil 2022\n\nVanderwheele 2019 Principles of confounder selection\nReview DAG resources and DAG software on the website’s DAGs resources tab\n\nPrior to and during the class period, you will start your projects and make a first DAG. Review the DAG resources page tab on the website, and send Laura your DAG for feedback and dicussion with the class. Be prepared to present and discuss your DAG. You will answer a set of questions in the template on the Google Drive based on your question and DAG.",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "ggdag_demo.html",
    "href": "ggdag_demo.html",
    "title": "ggdag_demo",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nlibrary(ggdag)\n\n\nAttaching package: 'ggdag'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nrequire(knitr)\n\nLoading required package: knitr\n\nlibrary(dagitty)\nrequire(tidyr)\n\nLoading required package: tidyr\n\nrequire(ddplyr)\n\nLoading required package: ddplyr\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'ddplyr'\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ purrr     1.0.2\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.1     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks ggdag::filter(), stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "ggdag_demo.html#r-markdown",
    "href": "ggdag_demo.html#r-markdown",
    "title": "ggdag_demo",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nlibrary(ggdag)\n\n\nAttaching package: 'ggdag'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nrequire(knitr)\n\nLoading required package: knitr\n\nlibrary(dagitty)\nrequire(tidyr)\n\nLoading required package: tidyr\n\nrequire(ddplyr)\n\nLoading required package: ddplyr\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'ddplyr'\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ purrr     1.0.2\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.1     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks ggdag::filter(), stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "ggdag_demo.html#including-plots",
    "href": "ggdag_demo.html#including-plots",
    "title": "ggdag_demo",
    "section": "Including Plots",
    "text": "Including Plots\nYou can also embed plots, for example:\n\n\n\n\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "ggdag_demo.html#so-now-we-just-have-past-richness-affecting-current-exotic-not-contemporaneous-because-otherwise-isnt-acyclic-graph",
    "href": "ggdag_demo.html#so-now-we-just-have-past-richness-affecting-current-exotic-not-contemporaneous-because-otherwise-isnt-acyclic-graph",
    "title": "ggdag_demo",
    "section": "So now we just have past richness affecting current exotic (not contemporaneous because otherwise isnt acyclic graph)!",
    "text": "So now we just have past richness affecting current exotic (not contemporaneous because otherwise isnt acyclic graph)!\n\nrichness_dag &lt;- dagify(Native_psy ~ Exotic_psy + NPKTreatment + Exotics_yr0 + Native_yr0 + Ndep_sy + Climate_sy + SoilFert_yr0 +  livestock_sy + Dist_to_Road_s + Oldfield_past_s + pastlivestock_yr0, \n                #is there anything else I missed in the native diversity not in the exotic? or vice versa?\n Exotic_psy ~  NPKTreatment + Exotics_yr0 + Native_yr0 + Ndep_sy + Climate_sy + SoilFert_yr0 +\n livestock_sy + Dist_to_Road_s + Oldfield_past_s + pastlivestock_yr0, \n Ndep_sy ~  Climate_sy, \nExotics_yr0 ~ Climate_sy + Ndep_sy + pastlivestock_yr0 + livestock_sy + Dist_to_Road_s + SoilFert_yr0,\n                # Did you have Native div in year 0 related to N deposition too?\n                # wouldn't native and invasive diversity in year 0 be related too?\n Native_yr0 ~ Climate_sy + pastlivestock_yr0 + livestock_sy + SoilFert_yr0, \nSoilFert_yr0 ~ Ndep_sy + Oldfield_past_s +  livestock_sy +  pastlivestock_yr0, \n  Oldfield_past_s  ~ Dist_to_Road_s, \nlivestock_sy  ~ pastlivestock_yr0 + Dist_to_Road_s,\npastlivestock_yr0 ~ Dist_to_Road_s, \nexposure = \"Exotic_psy\", #wont run if current richess also effects current exotic \n outcome = \"Native_psy\",\nlabels = c(outcome = \"richness\",\n           exposure = \"Exotic\"))\n\nset.seed(124)\nggdag(richness_dag, \n       use_labels = \"label\")\n\n\n\n\n\n\n\nggdag_status(richness_dag,\n             use_labels = \"label\",\n             text = TRUE,\n             label_alpha = 0.5) +\n   #guides(fill = FALSE, color = FALSE) +\n  theme_dag()\n\n\n\n\n\n\n\n  #identify the paths that need to be adjusted for \npaths(richness_dag)\n\n$paths\n  [1] \"Exotic_psy -&gt; Native_psy\"                                                                                                                                                \n  [2] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 -&gt; Native_psy\"                                                                                                                   \n  [3] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Native_psy\"                                                                                                 \n  [4] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; Native_psy\"                                                                              \n  [5] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 -&gt; Native_psy\"                                                              \n  [6] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                                                \n  [7] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; Native_psy\"                                \n  [8] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_psy\"           \n  [9] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"                           \n [10] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_psy\"           \n [11] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                                                   \n [12] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- livestock_sy -&gt; Native_psy\"                                              \n [13] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- livestock_sy -&gt; Native_yr0 -&gt; Native_psy\"                                \n [14] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- livestock_sy -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"           \n [15] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_psy\"                         \n [16] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_yr0 -&gt; Native_psy\"           \n [17] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"                                         \n [18] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- pastlivestock_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                           \n [19] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- pastlivestock_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; Native_psy\"           \n [20] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_psy\"                         \n [21] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_yr0 -&gt; Native_psy\"           \n [22] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_psy\"                                                                                 \n [23] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 -&gt; Native_psy\"                                                                   \n [24] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- SoilFert_yr0 -&gt; Native_psy\"                                                   \n [25] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                                        \n [26] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"                                \n [27] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"                              \n [28] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"                                              \n [29] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; SoilFert_yr0 -&gt; Native_psy\"                              \n [30] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                   \n [31] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"           \n [32] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; SoilFert_yr0 -&gt; Native_psy\"                                                                 \n [33] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                                                   \n [34] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"                              \n [35] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                                                      \n [36] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"                                              \n [37] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; SoilFert_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"                                            \n [38] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; SoilFert_yr0 &lt;- pastlivestock_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                              \n [39] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_psy\"                                                            \n [40] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                                              \n [41] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_yr0 &lt;- SoilFert_yr0 -&gt; Native_psy\"                              \n [42] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                   \n [43] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"           \n [44] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; SoilFert_yr0 -&gt; Native_psy\"                                            \n [45] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; SoilFert_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                              \n [46] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                                 \n [47] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"                         \n [48] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_psy\"                                                                            \n [49] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                                                              \n [50] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- SoilFert_yr0 -&gt; Native_psy\"                                              \n [51] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                                   \n [52] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"                           \n [53] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- livestock_sy -&gt; Native_psy\"                              \n [54] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; Native_psy\"                                              \n [55] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; SoilFert_yr0 -&gt; Native_psy\"                              \n [56] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                   \n [57] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"           \n [58] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; SoilFert_yr0 -&gt; Native_psy\"                                                            \n [59] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; SoilFert_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                                              \n [60] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; Native_psy\"                              \n [61] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                                                 \n [62] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"                                         \n [63] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; SoilFert_yr0 &lt;- livestock_sy -&gt; Native_psy\"                                            \n [64] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; SoilFert_yr0 &lt;- livestock_sy -&gt; Native_yr0 -&gt; Native_psy\"                              \n [65] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_psy\"                                                            \n [66] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_yr0 -&gt; Native_psy\"                                              \n [67] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_yr0 &lt;- SoilFert_yr0 -&gt; Native_psy\"                              \n [68] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                   \n [69] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"           \n [70] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; SoilFert_yr0 -&gt; Native_psy\"                                            \n [71] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                              \n [72] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                                 \n [73] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"                         \n [74] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                                                                                                        \n [75] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_psy\"                                                                                        \n [76] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                                                                          \n [77] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; Native_psy\"                                                          \n [78] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy &lt;- Dist_to_Road_s -&gt; Native_psy\"                                        \n [79] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; Native_psy\"                     \n [80] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_psy\"                   \n [81] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_psy\"                                     \n [82] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy &lt;- pastlivestock_yr0 &lt;- Dist_to_Road_s -&gt; Native_psy\"                   \n [83] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy &lt;- pastlivestock_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; Native_psy\"\n [84] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"                                                     \n [85] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_psy\"                                     \n [86] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; livestock_sy &lt;- Dist_to_Road_s -&gt; Native_psy\"                   \n [87] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; livestock_sy &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; Native_psy\"\n [88] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 &lt;- Dist_to_Road_s -&gt; Native_psy\"                                   \n [89] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; Native_psy\"                \n [90] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_psy\"                   \n [91] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"                                                                     \n [92] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; Native_psy\"                                                   \n [93] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_psy\"                                   \n [94] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 -&gt; Native_psy\"                     \n [95] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"\n [96] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_psy\"              \n [97] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_yr0 -&gt; Native_psy\"\n [98] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_psy\"                              \n [99] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                \n[100] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; Native_psy\"\n\n$open\n  [1]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE\n\nadjustmentSets(richness_dag)\n\n{ Climate_sy, Dist_to_Road_s, Exotics_yr0, NPKTreatment, Native_yr0,\n  Ndep_sy, Oldfield_past_s, SoilFert_yr0, livestock_sy,\n  pastlivestock_yr0 }\n\nggdag_paths(richness_dag)\n\n\n\n\n\n\n\nggdag_paths(richness_dag, shadow = TRUE)\n\n\n\n\n\n\n\nggdag_adjustment_set(richness_dag, shadow = TRUE)\n\n\n\n\n\n\n\nggdag_descendants(richness_dag, \"Climate_sy\")\n\n\n\n\n\n\n\nggdag_descendants(richness_dag, \"Ndep_sy\")\n\n\n\n\n\n\n\nimpliedConditionalIndependencies(richness_dag)\n\nClm_ _||_ D__R\nClm_ _||_ NPKT\nClm_ _||_ Ol__\nClm_ _||_ SF_0 | Ndp_\nClm_ _||_ lvs_\nClm_ _||_ ps_0\nD__R _||_ NPKT\nD__R _||_ Nt_0 | Clm_, SF_0, lvs_, ps_0\nD__R _||_ Nt_0 | Ndp_, SF_0, lvs_, ps_0\nD__R _||_ Nt_0 | Ol__, lvs_, ps_0\nD__R _||_ Ndp_\nD__R _||_ SF_0 | Ol__, lvs_, ps_0\nEx_0 _||_ NPKT\nEx_0 _||_ Nt_0 | Clm_, SF_0, lvs_, ps_0\nEx_0 _||_ Ol__ | D__R, Ndp_, SF_0, lvs_, ps_0\nNPKT _||_ Nt_0\nNPKT _||_ Ndp_\nNPKT _||_ Ol__\nNPKT _||_ SF_0\nNPKT _||_ lvs_\nNPKT _||_ ps_0\nNt_0 _||_ Ndp_ | Clm_, SF_0, lvs_, ps_0\nNt_0 _||_ Ol__ | Ndp_, SF_0, lvs_, ps_0\nNt_0 _||_ Ol__ | Clm_, SF_0, lvs_, ps_0\nNdp_ _||_ Ol__\nNdp_ _||_ lvs_\nNdp_ _||_ ps_0\nOl__ _||_ lvs_ | D__R\nOl__ _||_ ps_0 | D__R\n\n# We can also find all the paths between x and y using the paths() function from the dagitty package. We can see that there are three open paths between x and y:\n\npaths(richness_dag)\n\n$paths\n  [1] \"Exotic_psy -&gt; Native_psy\"                                                                                                                                                \n  [2] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 -&gt; Native_psy\"                                                                                                                   \n  [3] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Native_psy\"                                                                                                 \n  [4] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; Native_psy\"                                                                              \n  [5] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 -&gt; Native_psy\"                                                              \n  [6] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                                                \n  [7] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; Native_psy\"                                \n  [8] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_psy\"           \n  [9] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"                           \n [10] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_psy\"           \n [11] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                                                   \n [12] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- livestock_sy -&gt; Native_psy\"                                              \n [13] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- livestock_sy -&gt; Native_yr0 -&gt; Native_psy\"                                \n [14] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- livestock_sy -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"           \n [15] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_psy\"                         \n [16] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_yr0 -&gt; Native_psy\"           \n [17] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"                                         \n [18] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- pastlivestock_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                           \n [19] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- pastlivestock_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; Native_psy\"           \n [20] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_psy\"                         \n [21] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_yr0 -&gt; Native_psy\"           \n [22] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_psy\"                                                                                 \n [23] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 -&gt; Native_psy\"                                                                   \n [24] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- SoilFert_yr0 -&gt; Native_psy\"                                                   \n [25] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                                        \n [26] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"                                \n [27] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"                              \n [28] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"                                              \n [29] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; SoilFert_yr0 -&gt; Native_psy\"                              \n [30] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                   \n [31] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"           \n [32] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; SoilFert_yr0 -&gt; Native_psy\"                                                                 \n [33] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                                                   \n [34] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"                              \n [35] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                                                      \n [36] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"                                              \n [37] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; SoilFert_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"                                            \n [38] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; SoilFert_yr0 &lt;- pastlivestock_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                              \n [39] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_psy\"                                                            \n [40] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                                              \n [41] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_yr0 &lt;- SoilFert_yr0 -&gt; Native_psy\"                              \n [42] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                   \n [43] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"           \n [44] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; SoilFert_yr0 -&gt; Native_psy\"                                            \n [45] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; SoilFert_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                              \n [46] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                                 \n [47] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"                         \n [48] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_psy\"                                                                            \n [49] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                                                              \n [50] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- SoilFert_yr0 -&gt; Native_psy\"                                              \n [51] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                                   \n [52] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"                           \n [53] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- livestock_sy -&gt; Native_psy\"                              \n [54] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; Native_psy\"                                              \n [55] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; SoilFert_yr0 -&gt; Native_psy\"                              \n [56] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                   \n [57] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"           \n [58] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; SoilFert_yr0 -&gt; Native_psy\"                                                            \n [59] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; SoilFert_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                                              \n [60] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; Native_psy\"                              \n [61] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                                                 \n [62] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"                                         \n [63] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; SoilFert_yr0 &lt;- livestock_sy -&gt; Native_psy\"                                            \n [64] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; SoilFert_yr0 &lt;- livestock_sy -&gt; Native_yr0 -&gt; Native_psy\"                              \n [65] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_psy\"                                                            \n [66] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_yr0 -&gt; Native_psy\"                                              \n [67] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_yr0 &lt;- SoilFert_yr0 -&gt; Native_psy\"                              \n [68] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                   \n [69] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"           \n [70] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; SoilFert_yr0 -&gt; Native_psy\"                                            \n [71] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                              \n [72] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                                 \n [73] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"                         \n [74] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                                                                                                        \n [75] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_psy\"                                                                                        \n [76] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                                                                          \n [77] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; Native_psy\"                                                          \n [78] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy &lt;- Dist_to_Road_s -&gt; Native_psy\"                                        \n [79] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; Native_psy\"                     \n [80] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_psy\"                   \n [81] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_psy\"                                     \n [82] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy &lt;- pastlivestock_yr0 &lt;- Dist_to_Road_s -&gt; Native_psy\"                   \n [83] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy &lt;- pastlivestock_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; Native_psy\"\n [84] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"                                                     \n [85] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_psy\"                                     \n [86] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; livestock_sy &lt;- Dist_to_Road_s -&gt; Native_psy\"                   \n [87] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; livestock_sy &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; Native_psy\"\n [88] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 &lt;- Dist_to_Road_s -&gt; Native_psy\"                                   \n [89] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; Native_psy\"                \n [90] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_psy\"                   \n [91] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"                                                                     \n [92] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; Native_psy\"                                                   \n [93] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_psy\"                                   \n [94] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 -&gt; Native_psy\"                     \n [95] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"\n [96] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_psy\"              \n [97] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_yr0 -&gt; Native_psy\"\n [98] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_psy\"                              \n [99] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                \n[100] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; Native_psy\"\n\n$open\n  [1]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE"
  },
  {
    "objectID": "exercises/index.html#measurement-error-creates-bias",
    "href": "exercises/index.html#measurement-error-creates-bias",
    "title": "Exercises in R",
    "section": "Measurement Error Creates Bias",
    "text": "Measurement Error Creates Bias\ncode demo here",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/index.html#fun-with-spurious-correlations-ai-generated-explanations",
    "href": "exercises/index.html#fun-with-spurious-correlations-ai-generated-explanations",
    "title": "Exercises in R",
    "section": "Fun with spurious correlations & AI-generated explanations",
    "text": "Fun with spurious correlations & AI-generated explanations\nSpurious correlations",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "readings.html#do-you-feel-like-you-could-use-more-background-or-a-refresher-on-the-basics",
    "href": "readings.html#do-you-feel-like-you-could-use-more-background-or-a-refresher-on-the-basics",
    "title": "Readings by Class & Additional Materials",
    "section": "Do you feel like you could use more background or a refresher on the basics?",
    "text": "Do you feel like you could use more background or a refresher on the basics?\nhttps://theeffectbook.net/ch-DescribingVariables.html",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#variables-distributions-types-of-data",
    "href": "readings.html#variables-distributions-types-of-data",
    "title": "Readings by Class & Additional Materials",
    "section": "Variables, distributions, types of data",
    "text": "Variables, distributions, types of data\nThe Effect Ch. 3: Describing Variables\nHypothesis testing, type I and type II errors",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#variables-distributions-conditional-distributins-and-means-types-of-data",
    "href": "readings.html#variables-distributions-conditional-distributins-and-means-types-of-data",
    "title": "Readings by Class & Additional Materials",
    "section": "Variables, distributions, conditional distributins and means, types of data",
    "text": "Variables, distributions, conditional distributins and means, types of data\n\nThe Effect Ch. 3: Describing Variables\n-The Effect Ch. 4: Describing Relationships\n\nHypothesis testing, type I and type II errors",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-25---visitor-dr.-zach-laubach",
    "href": "readings.html#due-wed.-25---visitor-dr.-zach-laubach",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 2/5 - Visitor: Dr. Zach Laubach",
    "text": "Due Wed. 2/5 - Visitor: Dr. Zach Laubach\n\nRohrer 2018\nReview Laubach et al. 2021\nVanderwheele 2019 Principles of confounder selection\nReview DAG resources and DAG software on the website’s DAGs resources tab\n\nPrior to and during the class period, you will start your projects and make a first DAG. Review the DAG resources page tab on the website, and send Laura your DAG for feedback and dicussion with the class. Be prepared to present and discuss your DAG. You will answer a set of questions in the template on the Google Drive based on your question and DAG.",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-25---start-your-dags-with-guest-speaker-and-visitor-dr.-zach-laubach",
    "href": "readings.html#due-wed.-25---start-your-dags-with-guest-speaker-and-visitor-dr.-zach-laubach",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 2/5 - Start your DAGs with Guest Speaker and visitor: Dr. Zach Laubach",
    "text": "Due Wed. 2/5 - Start your DAGs with Guest Speaker and visitor: Dr. Zach Laubach\n\nRohrer 2018\nReview Laubach et al. 2021\nVanderwheele 2019 Principles of confounder selection\nReview DAG resources and DAG software on the website’s DAGs resources tab\n\nPrior to and during the class period, you will start your projects and make a first DAG. Review the DAG resources page tab on the website, and send Laura your DAG for feedback and dicussion with the class. Be prepared to present and discuss your DAG.",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "exercises/index.html#the-potential-outcomes-framework-helps-clarify-the-estamind-what-effect-we-are-capturing.",
    "href": "exercises/index.html#the-potential-outcomes-framework-helps-clarify-the-estamind-what-effect-we-are-capturing.",
    "title": "Exercises in R",
    "section": "The Potential Outcomes Framework helps clarify the estamind: what effect we are capturing.",
    "text": "The Potential Outcomes Framework helps clarify the estamind: what effect we are capturing.\n\nComparing different estimands - ATE, ATT versus ATU\nAndrew Heiss has a great breakdown here",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/inclass_demos/IPTW_exercise.html",
    "href": "exercises/inclass_demos/IPTW_exercise.html",
    "title": "IPTW",
    "section": "",
    "text": "This document is to help students understand how the Inverse Probability of Treatment Weighting (IPTW) method works and how to apply it when adjusting for treatment assignment in observational studies.\nBelow a table with the formulas for Weight (IPTW) included for clarity. The formulas are added to show how the weights are calculated based on the Probability of High Pollution (P(T=1)) for each island.\n\nDistance to Shoreline: Represents the proximity of the island to the shoreline.\nTreatment (High vs Low Pollution): 1 for high pollution exposure, 0 for low pollution exposure.\nProbability of High Pollution (P(T=1)): The estimated likelihood of high pollution exposure for each island.\n\n\nUpdated Table with Formulas for IPTW:\n\n\n\n\n\n\n\n\n\n\nIsland ID\nDistance to Shoreline (km)\nTreatment (T)\nProbability of High Pollution (P(T=1))\nWeight (IPTW)\n\n\n\n\n1\n1.2\n1\n0.85\n( = 1.18 )\n\n\n2\n2.5\n0\n0.45\n( = 1.82 )\n\n\n3\n0.8\n1\n0.70\n( = 1.43 )\n\n\n4\n3.0\n0\n0.30\n( = 1.43 )\n\n\n5\n1.5\n1\n0.75\n( = 1.33 )\n\n\n6\n4.0\n0\n0.20\n( = 1.25 )\n\n\n7\n2.0\n1\n0.90\n( = 1.11 )\n\n\n8\n5.0\n0\n0.10\n( = 1.11 )\n\n\n\n\n\nFormulas for Calculating Weight (IPTW):\n\nFor treated islands (T=1): [ = ] Where ( P(T=1) ) is the probability of being exposed to high pollution (probability of treatment).\nFor untreated islands (T=0): [ = ] Where ( 1 - P(T=1) ) represents the probability of being exposed to low pollution (complement of treatment probability).\n\n\n\nExample of Calculation:\n\nFor Island 1 (treated, high pollution):\n\nProbability of high pollution = 0.85\nWeight: [ = 1.18 ]\n\nFor Island 2 (untreated, low pollution):\n\nProbability of high pollution = 0.45\nWeight: [ = 1.82 ]"
  },
  {
    "objectID": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Matching_Weighting.html",
    "href": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Matching_Weighting.html",
    "title": "Tutorial on propensity score matching and inverse probability of treatment weighting",
    "section": "",
    "text": "This tutorial demonstrates how to use propensity score matching and inverse probability of treatment weighting using a real dataset from Siegel et al. 2022. The dataset for the entire western US is very large and unwieldy, so you’ll work with a subset of data for a single year in Colorado."
  },
  {
    "objectID": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Matching_Weighting.html#description",
    "href": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Matching_Weighting.html#description",
    "title": "Tutorial on propensity score matching and inverse probability of treatment weighting",
    "section": "",
    "text": "This tutorial demonstrates how to use propensity score matching and inverse probability of treatment weighting using a real dataset from Siegel et al. 2022. The dataset for the entire western US is very large and unwieldy, so you’ll work with a subset of data for a single year in Colorado."
  },
  {
    "objectID": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Matching_Weighting.html#set-up",
    "href": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Matching_Weighting.html#set-up",
    "title": "Tutorial on propensity score matching and inverse probability of treatment weighting",
    "section": "Set up",
    "text": "Set up\nLoad the packages used for data manipulation (tidyverse, sf), making a directed acyclic graph (ggdag), matching (MatchIt), weighting (ipw), and regression models (lme4)."
  },
  {
    "objectID": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Matching_Weighting.html#the-context",
    "href": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Matching_Weighting.html#the-context",
    "title": "Tutorial on propensity score matching and inverse probability of treatment weighting",
    "section": "The context",
    "text": "The context\nThe Siegel et al. 2022 study examines the effect of forest management (through the proxy of land ownership) on annual burn probability in forests of the western US. Specifically, it looks at the effect of federal (treatment) vs. private (control) ownership on wildfire occurrence in sample units.\n\nDirected acyclic graph\nHere’s a DAG for the research question:"
  },
  {
    "objectID": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Matching_Weighting.html#the-data",
    "href": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Matching_Weighting.html#the-data",
    "title": "Tutorial on propensity score matching and inverse probability of treatment weighting",
    "section": "The data",
    "text": "The data\nThe data are in the file matching_ipw_data.csv.\n\nVariable names\n\nstate: the state the sample unit is from (Colorado)\n\nUID: a unique identifier for each sample unit\n\nyear: the year that the fire and climate data is from (2002)\n\nburned: whether or not the unit burned in 2002 (0 = unburned, 1 = burned)\n\nprot_cat_recl: the ownership class. 0 = private, 1 = federal\n\ndist_rds_km: distance to the nearest road, in kiometers\n\nslope: slope, in degrees\n\naspect_srai: solar radiation aspect index\n\nelev_km: elevation, in 1000 m\n\nlon: longitude\n\nlat: latitude\n\npdsi_avg_season: seasonal average Palmer Drought Severity Index value (fall, spring, summer, winter)\n\nsoil_avg_season: seasonal average soil moisture (fall, spring, summer, winter)\n\ntmmn_avg_season: seasonal average minimum temperature (fall, spring, summer, winter)\ntmmx_avg_season: seasonal average maximum temperature (fall, spring, summer, winter)\n\nvs_max_season: seasonal average maximum wind speed (fall, spring, summer, winter)\n\ntotal_precip_season: total seasonal precipitation (fall, spring, summer, winter)\n\nprev_yr_precip: total precipitation in the previous year"
  },
  {
    "objectID": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Matching_Weighting.html#data-exploration",
    "href": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Matching_Weighting.html#data-exploration",
    "title": "Tutorial on propensity score matching and inverse probability of treatment weighting",
    "section": "Data exploration",
    "text": "Data exploration\n\nWhat’s the breakdown of private (value = 0) vs. federal (value = 1) units?\n\n\n\nUnits on federal (=1) and private (=0) land\n\n\nVar1\nFreq\n\n\n\n\n0\n22877\n\n\n1\n60654\n\n\n\n\n\n\n\nWhat’s the breakdown of units that burned (value = 1) in 2002 vs. units that did not burn (value = 0)?\n\n\n\nUnits that burned (=1) or did not burn (=0) in 2002\n\n\nVar1\nFreq\n\n\n\n\n0\n82189\n\n\n1\n1342\n\n\n\n\n\n\n\nHow do the private (in red) vs federal (in blue) units differ in terms of potential confounders?"
  },
  {
    "objectID": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Matching_Weighting.html#run-naive-regression",
    "href": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Matching_Weighting.html#run-naive-regression",
    "title": "Tutorial on propensity score matching and inverse probability of treatment weighting",
    "section": "Run naive regression",
    "text": "Run naive regression\nWe could just run a naive regression, ignoring the potential impact of confounders. There are some highly correlated covariates in the model, but let’s ignore them for now. Let’s see what that would yield:\n\n\n\nCoefficient estimates for naive model\n\n\nVariable\nEstimate\nStd. Error\np value\n\n\n\n\n(Intercept)\n-14.088\n3.606\n0.000\n\n\nprot_cat_recl1\n1.229\n0.095\n0.000\n\n\ndist_rds_km\n-0.079\n0.029\n0.007\n\n\nslope\n0.019\n0.003\n0.000\n\n\naspect_srai\n0.149\n0.084\n0.077\n\n\nelev_km\n1.627\n0.215\n0.000\n\n\npdsi_avg_winter\n-0.014\n0.002\n0.000\n\n\npdsi_avg_spring\n-0.021\n0.003\n0.000\n\n\npdsi_avg_summer\n0.036\n0.004\n0.000\n\n\npdsi_avg_fall\n-0.004\n0.003\n0.143\n\n\nsoil_avg_winter\n0.000\n0.001\n0.597\n\n\nsoil_avg_spring\n0.006\n0.001\n0.000\n\n\nsoil_avg_summer\n-0.002\n0.001\n0.200\n\n\nsoil_avg_fall\n-0.002\n0.001\n0.003\n\n\ntmmn_avg_winter\n-0.112\n0.011\n0.000\n\n\ntmmn_avg_spring\n0.054\n0.026\n0.035\n\n\ntmmn_avg_summer\n-0.095\n0.022\n0.000\n\n\ntmmn_avg_fall\n0.131\n0.026\n0.000\n\n\ntmmx_avg_winter\n0.133\n0.012\n0.000\n\n\ntmmx_avg_spring\n-0.017\n0.021\n0.431\n\n\ntmmx_avg_summer\n0.265\n0.021\n0.000\n\n\ntmmx_avg_fall\n-0.395\n0.029\n0.000\n\n\nvs_max_winter\n0.044\n0.004\n0.000\n\n\nvs_max_spring\n-0.038\n0.005\n0.000\n\n\nvs_max_summer\n-0.007\n0.005\n0.157\n\n\nvs_max_fall\n-0.045\n0.006\n0.000\n\n\ntotal_precip_winter\n-0.152\n0.008\n0.000\n\n\ntotal_precip_spring\n-0.034\n0.007\n0.000\n\n\ntotal_precip_summer\n0.130\n0.007\n0.000\n\n\ntotal_precip_fall\n0.044\n0.005\n0.000\n\n\nprev_yr_precip\n-0.003\n0.003\n0.196"
  },
  {
    "objectID": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Matching_Weighting.html#use-matching-to-overcome-issues-with-observed-confounding-variables",
    "href": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Matching_Weighting.html#use-matching-to-overcome-issues-with-observed-confounding-variables",
    "title": "Tutorial on propensity score matching and inverse probability of treatment weighting",
    "section": "Use matching to overcome issues with observed confounding variables",
    "text": "Use matching to overcome issues with observed confounding variables\n\nMatch the data\nMatch the data on the observable covariates, using the MatchIt package. You can play around with the settings to see how it affects the matched data you end up with.\n\nAssess match quality\nTake a look at the quality of the matches: how many units were matched? Control = private units, Treated = federal units\n\n\n\nBreakdown of matched and unmatched units\n\n\n\nControl\nTreated\n\n\n\n\nAll (ESS)\n22877\n60654\n\n\nAll\n22877\n60654\n\n\nMatched (ESS)\n14945\n14945\n\n\nMatched\n14945\n14945\n\n\nUnmatched\n7932\n45709\n\n\nDiscarded\n0\n0\n\n\n\n\n\nWhat are the covariate means in the matched dataset for the treated (federal) and control (private) units? What was the covariate balance after matching?\n\n\n\nCovariate balance of matched dataset\n\n\n\n\n\n\n\n\n\nMeans Treated\nMeans Control\nStandardized Mean Difference\n\n\n\n\ndistance\n0.6175546\n0.6158610\n0.0094201\n\n\nelev_km\n2.5041427\n2.4860995\n0.0386318\n\n\nslope\n13.3461585\n13.1416783\n0.0218229\n\n\naspect_srai\n0.4408603\n0.4421638\n-0.0037552\n\n\nlon\n850258.8129272\n845066.5747204\n0.0544996\n\n\nlat\n4319761.5608274\n4321966.2446682\n-0.0183548\n\n\ndist_rds_km\n0.5773463\n0.5285751\n0.0359075\n\n\nprev_yr_precip\n415.2926731\n413.7834727\n0.0100340\n\n\nvs_max_fall\n333.4980930\n330.1436601\n0.0480587\n\n\nvs_max_winter\n337.5478086\n332.5623285\n0.0474268\n\n\nvs_max_spring\n433.4669789\n430.1073938\n0.0569886\n\n\nvs_max_summer\n370.0096353\n367.1274005\n0.0643136\n\n\ntotal_precip_fall\n135.4748745\n136.7334225\n-0.0266053\n\n\ntotal_precip_winter\n49.2408832\n48.9946470\n0.0102433\n\n\ntotal_precip_spring\n43.5216460\n44.2109067\n-0.0351669\n\n\ntotal_precip_summer\n74.1271328\n73.3822014\n0.0370406\n\n\ntmmx_avg_fall\n129.5169399\n130.7038920\n-0.0368784\n\n\ntmmx_avg_winter\n30.7551466\n31.4414408\n-0.0293219\n\n\ntmmx_avg_spring\n117.5830043\n118.7652058\n-0.0290855\n\n\ntmmx_avg_summer\n262.3050742\n263.7936657\n-0.0361478\n\n\ntmmn_avg_fall\n-29.5255715\n-28.6079402\n-0.0378691\n\n\ntmmn_avg_winter\n-136.7402699\n-135.5962752\n-0.0395408\n\n\ntmmn_avg_spring\n-56.5992863\n-55.5367012\n-0.0311808\n\n\ntmmn_avg_summer\n64.7037359\n65.6066020\n-0.0292879\n\n\npdsi_avg_fall\n-307.7629977\n-306.1462250\n-0.0233885\n\n\npdsi_avg_winter\n-221.2940560\n-223.2854243\n0.0277561\n\n\npdsi_avg_spring\n-351.1321289\n-352.4421100\n0.0165642\n\n\npdsi_avg_summer\n-504.4508308\n-504.6720419\n0.0030818\n\n\nsoil_avg_fall\n123.7579123\n125.0271440\n-0.0069754\n\n\nsoil_avg_winter\n163.2296866\n165.6294190\n-0.0225604\n\n\nsoil_avg_spring\n210.8781755\n215.3971451\n-0.0262021\n\n\nsoil_avg_summer\n113.1436601\n114.4098138\n-0.0126119\n\n\n\n\n\nComparison of standardized mean differences in the covariate values in the full vs. matched dataset\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyze the matched dataset\n\nExtract the matched data\nFirst, you’ll need to extract the matched data and use the UIDs from the matched data to subset the full dataset for analysis.\n\n\nModel the effect of ownership/management on wildfire probability\nAgain, there are correlated covariates, but let’s just ignore them\n\n\n\nCoefficient estimates for model with matching\n\n\nVariable\nEstimate\nStd. Error\np value\n\n\n\n\n(Intercept)\n-41.135\n7.993\n0.000\n\n\nprot_cat_recl1\n0.868\n0.116\n0.000\n\n\ndist_rds_km\n-0.057\n0.103\n0.578\n\n\nslope\n0.021\n0.006\n0.000\n\n\naspect_srai\n0.044\n0.142\n0.760\n\n\nelev_km\n1.996\n0.413\n0.000\n\n\npdsi_avg_winter\n-0.018\n0.004\n0.000\n\n\npdsi_avg_spring\n-0.025\n0.006\n0.000\n\n\npdsi_avg_summer\n0.033\n0.009\n0.000\n\n\npdsi_avg_fall\n-0.003\n0.007\n0.629\n\n\nsoil_avg_winter\n0.001\n0.002\n0.637\n\n\nsoil_avg_spring\n0.009\n0.002\n0.000\n\n\nsoil_avg_summer\n-0.004\n0.003\n0.211\n\n\nsoil_avg_fall\n-0.001\n0.001\n0.622\n\n\ntmmn_avg_winter\n-0.091\n0.021\n0.000\n\n\ntmmn_avg_spring\n-0.081\n0.046\n0.078\n\n\ntmmn_avg_summer\n-0.185\n0.044\n0.000\n\n\ntmmn_avg_fall\n0.283\n0.053\n0.000\n\n\ntmmx_avg_winter\n0.178\n0.024\n0.000\n\n\ntmmx_avg_spring\n-0.036\n0.041\n0.376\n\n\ntmmx_avg_summer\n0.542\n0.044\n0.000\n\n\ntmmx_avg_fall\n-0.650\n0.060\n0.000\n\n\nvs_max_winter\n0.051\n0.008\n0.000\n\n\nvs_max_spring\n-0.083\n0.013\n0.000\n\n\nvs_max_summer\n0.011\n0.010\n0.278\n\n\nvs_max_fall\n-0.044\n0.014\n0.001\n\n\ntotal_precip_winter\n-0.289\n0.019\n0.000\n\n\ntotal_precip_spring\n-0.136\n0.015\n0.000\n\n\ntotal_precip_summer\n0.014\n0.016\n0.399\n\n\ntotal_precip_fall\n-0.003\n0.010\n0.730\n\n\nprev_yr_precip\n0.056\n0.007\n0.000"
  },
  {
    "objectID": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Matching_Weighting.html#use-weighting-to-overcome-issues-with-observed-confounding-variables",
    "href": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Matching_Weighting.html#use-weighting-to-overcome-issues-with-observed-confounding-variables",
    "title": "Tutorial on propensity score matching and inverse probability of treatment weighting",
    "section": "Use weighting to overcome issues with observed confounding variables",
    "text": "Use weighting to overcome issues with observed confounding variables\n\nWeight the data\nUse the package ipw\n\nWhat’s the range of weights?\n\n\n[1]    1.00 6069.64\n\n\n\n\n\n\n\n\n\n\n\n\nModel the effect of ownership\n\n\n\nCoefficient estimates for model with weighting\n\n\nVariable\nEstimate\nStd. Error\np value\n\n\n\n\n(Intercept)\n-11.151\n2.821\n0.000\n\n\nprot_cat_recl1\n0.921\n0.054\n0.000\n\n\ndist_rds_km\n-0.014\n0.026\n0.581\n\n\nslope\n0.011\n0.003\n0.000\n\n\naspect_srai\n0.077\n0.066\n0.245\n\n\nelev_km\n1.890\n0.174\n0.000\n\n\npdsi_avg_winter\n-0.018\n0.002\n0.000\n\n\npdsi_avg_spring\n-0.020\n0.002\n0.000\n\n\npdsi_avg_summer\n0.036\n0.003\n0.000\n\n\npdsi_avg_fall\n-0.003\n0.002\n0.130\n\n\nsoil_avg_winter\n-0.001\n0.001\n0.331\n\n\nsoil_avg_spring\n0.008\n0.001\n0.000\n\n\nsoil_avg_summer\n-0.003\n0.001\n0.005\n\n\nsoil_avg_fall\n-0.002\n0.001\n0.000\n\n\ntmmn_avg_winter\n-0.118\n0.008\n0.000\n\n\ntmmn_avg_spring\n0.059\n0.019\n0.002\n\n\ntmmn_avg_summer\n-0.127\n0.017\n0.000\n\n\ntmmn_avg_fall\n0.158\n0.021\n0.000\n\n\ntmmx_avg_winter\n0.152\n0.009\n0.000\n\n\ntmmx_avg_spring\n-0.026\n0.016\n0.108\n\n\ntmmx_avg_summer\n0.299\n0.016\n0.000\n\n\ntmmx_avg_fall\n-0.431\n0.023\n0.000\n\n\nvs_max_winter\n0.054\n0.003\n0.000\n\n\nvs_max_spring\n-0.060\n0.004\n0.000\n\n\nvs_max_summer\n0.001\n0.004\n0.863\n\n\nvs_max_fall\n-0.042\n0.005\n0.000\n\n\ntotal_precip_winter\n-0.165\n0.006\n0.000\n\n\ntotal_precip_spring\n-0.061\n0.005\n0.000\n\n\ntotal_precip_summer\n0.089\n0.006\n0.000\n\n\ntotal_precip_fall\n0.039\n0.004\n0.000\n\n\nprev_yr_precip\n0.004\n0.002\n0.060"
  },
  {
    "objectID": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Matching_Weighting.html#compare-outputs-from-the-naive-matched-and-weighted-regressions",
    "href": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Matching_Weighting.html#compare-outputs-from-the-naive-matched-and-weighted-regressions",
    "title": "Tutorial on propensity score matching and inverse probability of treatment weighting",
    "section": "Compare outputs from the naive, matched, and weighted regressions",
    "text": "Compare outputs from the naive, matched, and weighted regressions\n\n\n\n\n\n\n\n\n\nList of 136\n $ line                            :List of 6\n  ..$ colour       : chr \"black\"\n  ..$ linewidth    : num 0.5\n  ..$ linetype     : num 1\n  ..$ lineend      : chr \"butt\"\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ rect                            :List of 5\n  ..$ fill         : chr \"white\"\n  ..$ colour       : chr \"black\"\n  ..$ linewidth    : num 0.5\n  ..$ linetype     : num 1\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ text                            :List of 11\n  ..$ family       : chr \"\"\n  ..$ face         : chr \"plain\"\n  ..$ colour       : chr \"black\"\n  ..$ size         : num 11\n  ..$ hjust        : num 0.5\n  ..$ vjust        : num 0.5\n  ..$ angle        : num 0\n  ..$ lineheight   : num 0.9\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ title                           : NULL\n $ aspect.ratio                    : NULL\n $ axis.title                      : NULL\n $ axis.title.x                    :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 2.75points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.x.top                :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 2.75points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.x.bottom             : NULL\n $ axis.title.y                    :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : num 90\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.75points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.y.left               : NULL\n $ axis.title.y.right              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : num -90\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 2.75points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text                       :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : chr \"grey30\"\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x                     :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 2.2points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x.top                 :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 2.2points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x.bottom              : NULL\n $ axis.text.y                     :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 1\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.2points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.y.left                : NULL\n $ axis.text.y.right               :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 2.2points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.theta                 : NULL\n $ axis.text.r                     :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0.5\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.2points 0points 2.2points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.ticks                      :List of 6\n  ..$ colour       : chr \"grey20\"\n  ..$ linewidth    : NULL\n  ..$ linetype     : NULL\n  ..$ lineend      : NULL\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ axis.ticks.x                    : NULL\n $ axis.ticks.x.top                : NULL\n $ axis.ticks.x.bottom             : NULL\n $ axis.ticks.y                    : NULL\n $ axis.ticks.y.left               : NULL\n $ axis.ticks.y.right              : NULL\n $ axis.ticks.theta                : NULL\n $ axis.ticks.r                    : NULL\n $ axis.minor.ticks.x.top          : NULL\n $ axis.minor.ticks.x.bottom       : NULL\n $ axis.minor.ticks.y.left         : NULL\n $ axis.minor.ticks.y.right        : NULL\n $ axis.minor.ticks.theta          : NULL\n $ axis.minor.ticks.r              : NULL\n $ axis.ticks.length               : 'simpleUnit' num 2.75points\n  ..- attr(*, \"unit\")= int 8\n $ axis.ticks.length.x             : NULL\n $ axis.ticks.length.x.top         : NULL\n $ axis.ticks.length.x.bottom      : NULL\n $ axis.ticks.length.y             : NULL\n $ axis.ticks.length.y.left        : NULL\n $ axis.ticks.length.y.right       : NULL\n $ axis.ticks.length.theta         : NULL\n $ axis.ticks.length.r             : NULL\n $ axis.minor.ticks.length         : 'rel' num 0.75\n $ axis.minor.ticks.length.x       : NULL\n $ axis.minor.ticks.length.x.top   : NULL\n $ axis.minor.ticks.length.x.bottom: NULL\n $ axis.minor.ticks.length.y       : NULL\n $ axis.minor.ticks.length.y.left  : NULL\n $ axis.minor.ticks.length.y.right : NULL\n $ axis.minor.ticks.length.theta   : NULL\n $ axis.minor.ticks.length.r       : NULL\n $ axis.line                       : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ axis.line.x                     : NULL\n $ axis.line.x.top                 : NULL\n $ axis.line.x.bottom              : NULL\n $ axis.line.y                     : NULL\n $ axis.line.y.left                : NULL\n $ axis.line.y.right               : NULL\n $ axis.line.theta                 : NULL\n $ axis.line.r                     : NULL\n $ legend.background               :List of 5\n  ..$ fill         : NULL\n  ..$ colour       : logi NA\n  ..$ linewidth    : NULL\n  ..$ linetype     : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ legend.margin                   : 'margin' num [1:4] 5.5points 5.5points 5.5points 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ legend.spacing                  : 'simpleUnit' num 11points\n  ..- attr(*, \"unit\")= int 8\n $ legend.spacing.x                : NULL\n $ legend.spacing.y                : NULL\n $ legend.key                      : NULL\n $ legend.key.size                 : 'simpleUnit' num 1.2lines\n  ..- attr(*, \"unit\")= int 3\n $ legend.key.height               : NULL\n $ legend.key.width                : NULL\n $ legend.key.spacing              : 'simpleUnit' num 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ legend.key.spacing.x            : NULL\n $ legend.key.spacing.y            : NULL\n $ legend.frame                    : NULL\n $ legend.ticks                    : NULL\n $ legend.ticks.length             : 'rel' num 0.2\n $ legend.axis.line                : NULL\n $ legend.text                     :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.text.position            : NULL\n $ legend.title                    :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.title.position           : NULL\n $ legend.position                 : chr \"right\"\n $ legend.position.inside          : NULL\n $ legend.direction                : NULL\n $ legend.byrow                    : NULL\n $ legend.justification            : chr \"center\"\n $ legend.justification.top        : NULL\n $ legend.justification.bottom     : NULL\n $ legend.justification.left       : NULL\n $ legend.justification.right      : NULL\n $ legend.justification.inside     : NULL\n $ legend.location                 : NULL\n $ legend.box                      : NULL\n $ legend.box.just                 : NULL\n $ legend.box.margin               : 'margin' num [1:4] 0cm 0cm 0cm 0cm\n  ..- attr(*, \"unit\")= int 1\n $ legend.box.background           : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.box.spacing              : 'simpleUnit' num 11points\n  ..- attr(*, \"unit\")= int 8\n  [list output truncated]\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi TRUE\n - attr(*, \"validate\")= logi TRUE"
  },
  {
    "objectID": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Fixed_Effects.html",
    "href": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Fixed_Effects.html",
    "title": "Tutorial on panel data and fixed effects designs",
    "section": "",
    "text": "This tutorial demonstrates how to use panel data and fixed effects designs using a real dataset from Dee et al. 2023 (https://doi.org/10.1038/s41467-023-37194-5). It is modified from the online tutorial for the main analyses run in Dee et al. (2023) Nature Communications, originally written by Laura Dee and Chris Severen with data from the Nutrient Network."
  },
  {
    "objectID": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Fixed_Effects.html#description",
    "href": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Fixed_Effects.html#description",
    "title": "Tutorial on panel data and fixed effects designs",
    "section": "",
    "text": "This tutorial demonstrates how to use panel data and fixed effects designs using a real dataset from Dee et al. 2023 (https://doi.org/10.1038/s41467-023-37194-5). It is modified from the online tutorial for the main analyses run in Dee et al. (2023) Nature Communications, originally written by Laura Dee and Chris Severen with data from the Nutrient Network."
  },
  {
    "objectID": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Fixed_Effects.html#set-up",
    "href": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Fixed_Effects.html#set-up",
    "title": "Tutorial on panel data and fixed effects designs",
    "section": "Set up",
    "text": "Set up\nLoad the packages used for data manipulation (tidyverse, data.table), making a directed acyclic graph (ggdag), and analysis (fixest, lme4)."
  },
  {
    "objectID": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Fixed_Effects.html#the-context",
    "href": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Fixed_Effects.html#the-context",
    "title": "Tutorial on panel data and fixed effects designs",
    "section": "The context",
    "text": "The context\nThe Dee et al. 2023 study examines the effect of grassland species richness on productivity, engaging with ongoing debates about the relationship between biodiversity and ecosystem functioning. Specifically, it looks at how plant species richness (a continuous treatment) affects live aboveground biomass (outcome) in sample plots across different research sites over time.\n\nDirected acyclic graph\nHere’s a simplified DAG for the research question:"
  },
  {
    "objectID": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Fixed_Effects.html#the-data",
    "href": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Fixed_Effects.html#the-data",
    "title": "Tutorial on panel data and fixed effects designs",
    "section": "The data",
    "text": "The data\nThe data are in the file fixed_effects_data.csv.\nThe original data come from the Nutrient Network (https://nutnet.org/) or “NutNet.” Dee et al. (2023) cleaned and processed the data for their analyses: these data are available in the project release on Zenodo (https://zenodo.org/records/7675340). The data consist of control plots from 43 NutNet sites, with at least 5 years of data.\n\nVariable names\nThe dataset contains a wide number of variables; we’ll mainly be using the variables live_mass (live aboveground biomass) and rich (plant species richness). Also important are the site_code, plot, and year variables, which we use to create fixed effects. site.by.yeardummy is a dummy variable for the interaction between the site and the year."
  },
  {
    "objectID": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Fixed_Effects.html#naive-correlations-in-single-years",
    "href": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Fixed_Effects.html#naive-correlations-in-single-years",
    "title": "Tutorial on panel data and fixed effects designs",
    "section": "Naive correlations in single years",
    "text": "Naive correlations in single years\nLet’s begin by looking at simple correlations that do not account for any confounding variables. We will examine both the overall correlation and correlations within a single year (i.e., using cross-sectional variation). We proceed with a linear regression framework, using the log of productivity (measured as live aboveground biomass) as the outcome and the log species richness as the explanatory variable.\n\n\n\n\n\nlog(Live Mass) as a function of log(Richness).\n\n\n\n\nWe initially estimate and report \\(\\beta\\) in: \\[\\begin{equation}\n\\ln(\\text{Live Mass}_{pst}) = \\alpha + \\beta \\ln(\\text{Richness}_{pst}) + e_{pst}\n(\\#eq:eq1)\n\\end{equation}\\] where \\(p\\) indexes plots, \\(s\\) indexes sites, and \\(t\\) indexes years. The unobserved error term is \\(e_{pst}\\), and there is a constant \\(\\alpha\\).\nWe can estimate results below using all years of data first, and then estimate the results using two individual years: 2012 and 2013. We will cluster standard errors by plot to reflect serial correlation in errors terms within a plot across years (we do not assume that errors are independent and identically distributed). Note that when we use only a single year of data, this is equivalent to using heteroskedasticity-robust errors.\n\n\n\n\n\n\n\n\n\n\n\n\nSimpleCorrAll\nSimpleCorr2012\nSimpleCorr2013\n\n\n\n\n\nData All Years\nData in 2012\nData in 2013\n\n\nDependent Var.:\nlog(live_mass)\nlog(live_mass)\nlog(live_mass)\n\n\n\n\n\n\n\n\nConstant\n5.288*** (0.2398)\n4.915*** (0.4111)\n6.421*** (0.2294)\n\n\nlog(rich)\n0.0699 (0.0952)\n0.1849 (0.1706)\n-0.3766*** (0.1006)\n\n\n_______________\n_________________\n_________________\n___________________\n\n\nS.E.: Clustered\nby: newplotid\nby: newplotid\nby: newplotid\n\n\nObservations\n1,231\n145\n121\n\n\nR2\n0.00152\n0.00928\n0.08045\n\n\nAdj. R2\n0.00071\n0.00235\n0.07272\n\n\n\n\n\nUsing all years of data gives a non-significant positive relationship between productivity and richness. In just the 2012 data, the coefficient is larger in magnitude, but still not significant. Finally, just using 2013 data, the coefficient switches signs and becomes significant.\nSo…which one should we believe? Well, probably none of these, because they likely do not identify the true causal effect of richness on productivity without statistical bias. Their variability highlights that these estimates, which rely on non-experimental cross-sectional data, are likely contaminated by omitted variable bias.\nWhen does \\(\\hat{\\beta}\\) capture a causal relationship? When there are no unobservables that are correlated with richness that also influence productivity: \\(\\mathbb{E}[e_{pst} \\times \\ln(\\text{Richness}_{pst})]=0\\) (i.e., \\(e_{pst}\\) and \\(\\ln(\\text{Richness}_{pst})\\) aren’t correlated). In the above results, there’s probably stuff in \\(e\\) that is correlated with richness, like precipitation, disturbance, land-use history, soil characteristics, and other characteristics of sites and plots."
  },
  {
    "objectID": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Fixed_Effects.html#multivariate-regression-that-adjusts-for-confounding-variables",
    "href": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Fixed_Effects.html#multivariate-regression-that-adjusts-for-confounding-variables",
    "title": "Tutorial on panel data and fixed effects designs",
    "section": "Multivariate Regression that adjusts for confounding variables",
    "text": "Multivariate Regression that adjusts for confounding variables\nOf course, in the above correlations, we include plots in sites from across the world, implicitly comparing grasslands in warmer climates with those in cooler ones, or wetter with dryer, or Europe with the Americas. There are a lot of differences between these places! A common response to this problem is to try to measure these differences and include them in the model.\nMore generally, a common statistical design in ecology is to measure and control for confounding variables in multivariate regression. In the causal inference literature, this is known as conditioning on observables or Pearl’s back-door criteria. Conditioning on observables is convenient but makes strong assumptions for causal inference, namely the “Selection on Observables” Assumption. Informally, this assumption implies that confounding variables that could introduce bias into a design are known and observable to the researcher. The bias they introduce into an estimator can be eliminated (controlled, blocked) by conditioning strategies, such as regression, matching, or stratification methods.\nTo explore the consequences of adding in covariates, let’s run models that add different subsets of the potential confounders. We’ll create a table with model results for 5 different models. The first model is the simple model we generated in the previous code chunk. The second column adds in soil chemistry covariates, the third column instead adds weather covariates, and the fourth instead adds management variables plus habitat. The last columns adds in everything. For the purposes of this tutorial, we only show coefficient estimates for richness in the following table, even though the other terms are included in the model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimpleCorrAll\nSoilCovars\nWeatherCovars\nMgmtCovars\nAllCovars\n\n\n\n\n\nData All Years\n+ Soil\n+ Weather\n+ Management\n+ All\n\n\nDependent Var.:\nlog(live_mass)\nlog(live_mass)\nlog(live_mass)\nlog(live_mass)\nlog(live_mass)\n\n\n\n\n\n\n\n\n\n\nlog(rich)\n0.0699 (0.0952)\n0.2251* (0.1127)\n0.0203 (0.0907)\n0.0275 (0.0822)\n0.4190* (0.1649)\n\n\n_______________\n_______________\n________________\n_______________\n_______________\n________________\n\n\nS.E.: Clustered\nby: newplotid\nby: newplotid\nby: newplotid\nby: newplotid\nby: newplotid\n\n\nObservations\n1,231\n675\n1,231\n1,231\n675\n\n\nR2\n0.00152\n0.33781\n0.20626\n0.37496\n0.51377\n\n\nAdj. R2\n0.00071\n0.32068\n0.20041\n0.36568\n0.48309\n\n\n\n\n\nEstimates jump around depending on which covariates are used! This is likely a sign of some sort of omitted variables bias. Even though we consecutively explain more and more of the variation in the data, we are not necessarily any closer to a causal relationship."
  },
  {
    "objectID": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Fixed_Effects.html#fixed-effects-changing-the-source-of-variation",
    "href": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Fixed_Effects.html#fixed-effects-changing-the-source-of-variation",
    "title": "Tutorial on panel data and fixed effects designs",
    "section": "Fixed effects: changing the source of variation",
    "text": "Fixed effects: changing the source of variation\nWe now move on to the focus of this tutorial: let’s switch up where the identification comes from.\n\nPlot fixed effects\nLet’s ignore sites for a minute, and just think about the plots that lie in a single site. We’re going to estimate the following model: \\[\\begin{equation}\n\\ln(\\text{Live Mass}_{pt}) = \\beta \\ln(\\text{Richness}_{pt}) + \\delta_p + \\mu_t + e_{pt}\n\\end{equation}\\] where we’ve added the term \\(\\delta_p\\). This represents a vector of plot-specific fixed effect – a dummy variable for each plot. We also add time fixed effects (\\(\\mu_t\\), a dummy for each year) to control for the common differences to all plots in a year (in a site). We’ll touch on that more later, but really, the plot fixed effects are of greatest consequence.\nWhat does adding this vector of plot dummy variables do? Two big things. First, it controls for any and all time-invariant features of the plot, whether or not we observe them! To see this, imagine putting in a variable \\(x_p\\) into the above equation linearly with the coefficient \\(\\gamma\\). We wouldn’t actually be able to estimate \\(\\gamma x_p\\); it’s already a component of \\(\\delta_p\\). Don’t know what functional for you should use for \\(x_p\\) or whether it should be interacted with another variable? That’s fine, that’s already included in \\(\\delta_p\\)! We get a whole lot for the inclusion of this variable. In DAG form, we have now removed observable and unobservable plot-level confounding effects.\nSecond, and most importantly conceptually, is that we are no longer directly comparing different plots; we aren’t using cross-sectional variation any more. Instead, we are using variation in richness and productivity within the same plot over time. So, we’re implicitly comparing a plot in year \\(t\\) with this same plot in year \\(t+k\\) for some \\(k\\). Another way to see this is that we could write a very similar equation in differences (ignore the \\(\\mu_t\\) for a moment): \\[\\begin{equation}\n\\left(\\ln(\\text{Live Mass}_{pt})-\\ln(\\text{Live Mass}_{pt-1}) \\right)= \\beta \\left( \\ln(\\text{Richness}_{pt}) - \\ln(\\text{Richness}_{pt-1}) \\right) + \\left( e_{pt} - e_{pt-1}\\right)\n\\end{equation}\\] Where did \\(\\lambda_p\\) go? Well, \\(\\lambda_p-\\lambda_p=0\\), so we don’t need it. (NB: We could also subtract the mean of each variable over time within each plot and arrive at a similar estimator. There are subtle differences between the two approaches that depend on the nature of the error terms \\(e\\), but they draw on the same source of variation).\nWhat do we have to assume for a causal interpretation? There are a couple of different assumptions we could choose; I think it’s easiest to frame it like this: \\(\\mathbb{E}[ (e_{pt} - e_{pt-1}) \\times (\\ln(\\text{Richness}_{pt}) - \\ln(\\text{Richness}_{pt-1}))]=0\\). That is, changes in richness are uncorrelated with changes in unobserved determinants of richness. Because time-invariant unobservable variables do not change, they are no longer a concern! Instead, we’re concerned if movements in some unobserved factor could both be driving our outcome variable and be correlated with richness.\nThe figures below illustrate graphically what the plot fixed effects do to the outcome variable (productivity). First, we plot he raw data, and showing log(live mass) in four plots split between two sites (at the Sedgwick Reserve and at the Sevilleta Long Term Ecological Research sites). Sedgwick has higher productivity on average. The productivity at these sites also appears to be following different trajectories through time (e.g., note the dip in productivity at Sevilleta in 2009).\n\nData prep\n\n\n\n\n\nRaw variation in live mass at four plots across two sites.\n\n\n\n\nNext, we inclue fixed effects. The resulting plot shows that we have now removed the average productivity in each site. The fixed effects do not remove site-and-year specific sources of confounding variation (e.g., if a more extreme drought happened at Sevilleta than at Sedgwick in 2009 affecting both productivity and richness); we turn to eliminating site and year specific confounding variables next.\n\n\n\n\n\nVariation demaned by plot in live mass at four plots across two sites.\n\n\n\n\nTo the statistical model: We’re first going to estimate the following equation site-by-site on the five sites with the largest number of observations (in terms of the number plot-years we observe; see Table S1). \\[\\begin{equation}\n\\ln(\\text{Live Mass}_{pt}) = \\beta \\ln(\\text{Richness}_{pt}) + \\delta_p + \\mu_t + e_{pt}\n\\end{equation}\\] The year fixed effects \\(\\mu_t\\) control for time-varying factors (observed or unobserved) that affect all plots at the site under consideration. For example, suppose 2007 was a particularly damp and rainy year at the site; \\(\\mu_t\\) controls for the average impact of that across all plots. Because what happens at one site in a year is probably very different from what happens at a different site in the same year, we estimate these separately for each site. This will make the point estimates for each site less precise (especially because we’re clustering by plot), but this is just for illustration’s sake.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimpleCorrAll\nPlotFE_1\nPlotFE_2\nPlotFE_3\nPlotFE_4\nPlotFE_5\n\n\n\n\n\nData All Years\nUS - CDCR\nUS - CDPT\nCA - Koffler\nUS - SEDG\nUS - SIER\n\n\nDependent Var.:\nlog(live_mass)\nlog(live_mass)\nlog(live_mass)\nlog(live_mass)\nlog(live_mass)\nlog(live_mass)\n\n\n\n\n\n\n\n\n\n\n\nlog(rich)\n0.0699 (0.0952)\n-0.0972 (0.2189)\n-0.6247. (0.2475)\n0.0257 (0.2501)\n-0.0569 (0.1133)\n-0.2651 (0.2472)\n\n\nFixed-Effects:\n—————\n—————-\n—————–\n—————\n—————-\n—————-\n\n\nnewplotid\nNo\nYes\nYes\nYes\nYes\nYes\n\n\nyear\nNo\nYes\nYes\nYes\nYes\nYes\n\n\n_______________\n_______________\n________________\n_________________\n_______________\n________________\n________________\n\n\nS.E.: Clustered\nby: newplotid\nby: newplotid\nby: newplotid\nby: newplotid\nby: newplotid\nby: newplotid\n\n\nObservations\n1,231\n55\n60\n72\n66\n53\n\n\nR2\n0.00152\n0.74115\n0.72916\n0.26473\n0.73402\n0.62055\n\n\nWithin R2\n–\n0.00325\n0.05917\n0.00011\n0.00444\n0.04260\n\n\n\n\n\nWe again show the bivariate correlation on all sites first (SimpleCorrAll), and then the estimate for each site. Now we’re getting some negative coefficients (though mostly insignificant due to smaller effective sample sizes). We’re controlling for lots and lots of things that we couldn’t control for before, either because we didn’t think to include them or we couldn’t collect data on them. The R-squared values confirm that this is the case; we’re generally explaining much more of the data than before (but note: R-squared values are NOT important for causal interpretations generally). Note that, with the plot fixed effects, we do not have much statistical power estimating sites individually.\nTakeaway: Using unit fixed effects in panel data shifts the identifying variation from across units to within units over time."
  },
  {
    "objectID": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Fixed_Effects.html#bringing-it-all-together-with-site-by-year-fixed-effects",
    "href": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Fixed_Effects.html#bringing-it-all-together-with-site-by-year-fixed-effects",
    "title": "Tutorial on panel data and fixed effects designs",
    "section": "Bringing it all together with site-by-year fixed effects",
    "text": "Bringing it all together with site-by-year fixed effects\nWe now combine all sites together to give us more statistical power to detect effects. We do want to account for the fact that different sites experience different conditions in different years. To do so in a flexible way, we include site-by-year fixed effects, \\(\\mu_{st}\\). \\[\\begin{equation}\n\\ln(\\text{Live Mass}_{pst}) = \\beta \\ln(\\text{Richness}_{pst}) + \\delta_p + \\delta_{st} +  e_{pst}\n\\end{equation}\\] These additional fixed effects control for all time-varying effects that impact the site as whole (i.e., that apply to all the plots equally). Thus, they capture the first order effects of weather, among other factors that could shift outcomes for the site as whole. This gives us sufficient power to conduct conservative inference on our estimated average treatment effect.\nTo get a sense for what these site-by-year effects do, first recall Figure 2. Plots that are in the same site seem to have similar movements in productivity over time, even after controlling for plot fixed effects. The site-by-year fixed effects remove the average of everything that happens across the site in the data in a year (e.g., a drought at a site). Figure 4 removes this variation; see how the big drop in Sevilleta live mass in 2009 is much less in Figure 4.\n\n\n\n\n\nVariation demaned by plot and site-by-year effects in live mass at four plots across two sites.\n\n\n\n\nTo provide confidence that the results are robust, we will also include a couple of time-varying controls, evenness and lagged richness. NB: To make sure we don’t drop locations with values of zero evenness, we use the inverse hyperbolic sine instead of the natural log. Note that we don’t need to worry about that for productivity or richness because they never take a zero value.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMainMod_Rich\nMainMod_RichEven\nMainMod_RichLag\nMainMod_RichEve..\n\n\n\n\nDependent Var.:\nlog(live_mass)\nlog(live_mass)\nlog(live_mass)\nlog(live_mass)\n\n\n\n\n\n\n\n\n\nlog(rich)\n-0.2418** (0.0854)\n-0.2237** (0.0851)\n-0.2185* (0.0939)\n-0.2057* (0.0948)\n\n\nihs(even)\n\n-0.1864 (0.2122)\n\n-0.1450 (0.2387)\n\n\nlog(laggedrich)\n\n\n-0.0146 (0.0905)\n-0.0096 (0.0903)\n\n\nFixed-Effects:\n——————\n——————\n—————–\n—————–\n\n\nnewplotid\nYes\nYes\nYes\nYes\n\n\nsite.by.yeardummy\nYes\nYes\nYes\nYes\n\n\n_________________\n__________________\n__________________\n_________________\n_________________\n\n\nS.E.: Clustered\nby: newplotid\nby: newplotid\nby: newplotid\nby: newplotid\n\n\nObservations\n1,231\n1,231\n1,093\n1,093\n\n\nR2\n0.86669\n0.86689\n0.87092\n0.87103\n\n\nWithin R2\n0.01281\n0.01425\n0.01064\n0.01148\n\n\n\n\n\nAs you can see, estimate on log richness are relatively stable across different specifications. Of special note: the coefficient on lagged richness (richness from the year before) is small and insignificant, given us confidence that our results reflect contemporaneous movement in richness, and not some factor that is also correlated with last year’s richness."
  },
  {
    "objectID": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Fixed_Effects.html#graphical-depictions-of-fixed-effects",
    "href": "exercises/Siegel_and_Dee_2025_Exercises_intro_causal_inf/Fixed_Effects.html#graphical-depictions-of-fixed-effects",
    "title": "Tutorial on panel data and fixed effects designs",
    "section": "Graphical depictions of fixed effects",
    "text": "Graphical depictions of fixed effects\nUsing simple plots of the relationships between richness and productivity (as log of richness and log of live mass), we can see the relationships shift from positive to negative as we add in fixed effects.\nFigure 5 just shows the raw bivariate correlation, and is somewhat positive. Notice the range of variation across productivity and richness.\n\n\n\n\n\nThe bivariate relationship with no controls.\n\n\n\n\nFigure 6 shows the relationship conditional on plot fixed effects. Note that this collapses the range of variation substantially. Recall that the plot fixed effects remove factors that are time-invariant and plot-specific. However, this graph still reflects the variation in sites across years.\n\n\n\n\n\nControlling for only plot-level attributes that do not change through time\n\n\n\n\nFigure 7 continues to control for plot fixed effects, but also removes site-by-year variation via site-by-year fixed effects. This shifts the variation negative as factors that are common to sites within years no longer confound the relationship.\n\n\n\n\n\nControlling for plot attributes and attributes of sites that change through time"
  },
  {
    "objectID": "exercises/ggdag_demo.html",
    "href": "exercises/ggdag_demo.html",
    "title": "Constructing and Analyzing DAGs in dagitty and ggdag_demo",
    "section": "",
    "text": "Understand what a DAG is and what it is useful for.\nUnderstand how to construct a DAG in R to represent a set of assumptions about causal relationships.\nUnderstand how to use a DAG to determine the consequences of including variables in a regression model.\nUnderstand how to use a DAG to select appropriate statistical analysis methods (with focus on regression models) for estimating causal relationships in non-randomized studies, including:\nhow to identify variables that should be included in a regression model\nhow to identify variables that should NOT be included in a regression model (colliders, bad controls)\n\nFor more recap and a live guide through ggdag and dagitty, I highly recommend this live demonstration and tutorial by Andrew Heiss here"
  },
  {
    "objectID": "exercises/ggdag_demo.html#heres-the-dag",
    "href": "exercises/ggdag_demo.html#heres-the-dag",
    "title": "Constructing and Analyzing DAGs in dagitty and ggdag_demo",
    "section": "Here’s the DAG:",
    "text": "Here’s the DAG:\n\n\nCode\nrichness_dag &lt;- dagify(Native_psy ~ Exotic_psy + NPKTreatment + Exotics_yr0 + Native_yr0 + Ndep_sy + Climate_sy + SoilFert_yr0 +  livestock_sy + Dist_to_Road_s + Oldfield_past_s + pastlivestock_yr0, \n Exotic_psy ~  NPKTreatment + Exotics_yr0 + Native_yr0 + Ndep_sy + Climate_sy + SoilFert_yr0 +\n livestock_sy + Dist_to_Road_s + Oldfield_past_s + pastlivestock_yr0, \n Ndep_sy ~  Climate_sy, \n Exotics_yr0 ~ Climate_sy + Ndep_sy + pastlivestock_yr0 + livestock_sy + Dist_to_Road_s + SoilFert_yr0,\n Native_yr0 ~ Climate_sy + pastlivestock_yr0 + livestock_sy + SoilFert_yr0, \n   SoilFert_yr0 ~ Ndep_sy + Oldfield_past_s +  livestock_sy +  pastlivestock_yr0, \n  Oldfield_past_s  ~ Dist_to_Road_s, \n  livestock_sy  ~ pastlivestock_yr0 + Dist_to_Road_s,\n   pastlivestock_yr0 ~ Dist_to_Road_s, \n  exposure = \"Exotic_psy\", #wont run if current richess also effects current exotic \n  outcome = \"Native_psy\",\n labels = c(outcome = \"richness\",\n           exposure = \"Exotic\"))\n\nset.seed(124)\nggdag(richness_dag, \n       use_labels = \"label\")\n\n\n\n\n\n\n\n\n\nWe can now plot and analyze the DAG, identifying the adjustment set (covariates to control for!) to satisfy the backdoor criterion.\n\n\nCode\n# plot the DAG with ggdag \nggdag_status(richness_dag,\n             use_labels = \"label\",\n             text = TRUE,\n             label_alpha = 0.5) + theme_dag()\n\n\n\n\n\n\n\n\n\nCode\n# We can find and print all the paths between x and y using the paths() function from the dagitty package. We can see that there are three open paths between x and y:\npaths(richness_dag)\n\n\n$paths\n  [1] \"Exotic_psy -&gt; Native_psy\"                                                                                                                                                \n  [2] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 -&gt; Native_psy\"                                                                                                                   \n  [3] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Native_psy\"                                                                                                 \n  [4] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; Native_psy\"                                                                              \n  [5] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 -&gt; Native_psy\"                                                              \n  [6] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                                                \n  [7] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; Native_psy\"                                \n  [8] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_psy\"           \n  [9] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"                           \n [10] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_psy\"           \n [11] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                                                   \n [12] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- livestock_sy -&gt; Native_psy\"                                              \n [13] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- livestock_sy -&gt; Native_yr0 -&gt; Native_psy\"                                \n [14] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- livestock_sy -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"           \n [15] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_psy\"                         \n [16] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_yr0 -&gt; Native_psy\"           \n [17] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"                                         \n [18] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- pastlivestock_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                           \n [19] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- pastlivestock_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; Native_psy\"           \n [20] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_psy\"                         \n [21] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; SoilFert_yr0 &lt;- pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_yr0 -&gt; Native_psy\"           \n [22] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_psy\"                                                                                 \n [23] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 -&gt; Native_psy\"                                                                   \n [24] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- SoilFert_yr0 -&gt; Native_psy\"                                                   \n [25] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                                        \n [26] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"                                \n [27] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"                              \n [28] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"                                              \n [29] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; SoilFert_yr0 -&gt; Native_psy\"                              \n [30] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                   \n [31] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"           \n [32] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; SoilFert_yr0 -&gt; Native_psy\"                                                                 \n [33] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                                                   \n [34] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"                              \n [35] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                                                      \n [36] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"                                              \n [37] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; SoilFert_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"                                            \n [38] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; SoilFert_yr0 &lt;- pastlivestock_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                              \n [39] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_psy\"                                                            \n [40] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                                              \n [41] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_yr0 &lt;- SoilFert_yr0 -&gt; Native_psy\"                              \n [42] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                   \n [43] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"           \n [44] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; SoilFert_yr0 -&gt; Native_psy\"                                            \n [45] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; SoilFert_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                              \n [46] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                                 \n [47] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"                         \n [48] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_psy\"                                                                            \n [49] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                                                              \n [50] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- SoilFert_yr0 -&gt; Native_psy\"                                              \n [51] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                                   \n [52] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"                           \n [53] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- livestock_sy -&gt; Native_psy\"                              \n [54] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; Native_psy\"                                              \n [55] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; SoilFert_yr0 -&gt; Native_psy\"                              \n [56] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                   \n [57] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"           \n [58] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; SoilFert_yr0 -&gt; Native_psy\"                                                            \n [59] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; SoilFert_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                                              \n [60] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; Native_psy\"                              \n [61] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                                                 \n [62] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"                                         \n [63] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; SoilFert_yr0 &lt;- livestock_sy -&gt; Native_psy\"                                            \n [64] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; SoilFert_yr0 &lt;- livestock_sy -&gt; Native_yr0 -&gt; Native_psy\"                              \n [65] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_psy\"                                                            \n [66] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_yr0 -&gt; Native_psy\"                                              \n [67] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_yr0 &lt;- SoilFert_yr0 -&gt; Native_psy\"                              \n [68] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                   \n [69] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_yr0 &lt;- SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"           \n [70] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; SoilFert_yr0 -&gt; Native_psy\"                                            \n [71] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                              \n [72] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; SoilFert_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                                 \n [73] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; livestock_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"                         \n [74] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; Native_psy\"                                                                                                        \n [75] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_psy\"                                                                                        \n [76] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                                                                          \n [77] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; Native_psy\"                                                          \n [78] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy &lt;- Dist_to_Road_s -&gt; Native_psy\"                                        \n [79] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; Native_psy\"                     \n [80] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_psy\"                   \n [81] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_psy\"                                     \n [82] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy &lt;- pastlivestock_yr0 &lt;- Dist_to_Road_s -&gt; Native_psy\"                   \n [83] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- livestock_sy &lt;- pastlivestock_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; Native_psy\"\n [84] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"                                                     \n [85] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; livestock_sy -&gt; Native_psy\"                                     \n [86] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; livestock_sy &lt;- Dist_to_Road_s -&gt; Native_psy\"                   \n [87] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; livestock_sy &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; Native_psy\"\n [88] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 &lt;- Dist_to_Road_s -&gt; Native_psy\"                                   \n [89] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 &lt;- Dist_to_Road_s -&gt; Oldfield_past_s -&gt; Native_psy\"                \n [90] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 -&gt; Native_yr0 &lt;- pastlivestock_yr0 &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_psy\"                   \n [91] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s -&gt; Native_psy\"                                                                     \n [92] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; Native_psy\"                                                   \n [93] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_psy\"                                   \n [94] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 -&gt; Native_psy\"                     \n [95] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; livestock_sy -&gt; Native_yr0 &lt;- pastlivestock_yr0 -&gt; Native_psy\"\n [96] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_psy\"              \n [97] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; livestock_sy &lt;- pastlivestock_yr0 -&gt; Native_yr0 -&gt; Native_psy\"\n [98] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_psy\"                              \n [99] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 -&gt; Native_psy\"                \n[100] \"Exotic_psy &lt;- Climate_sy -&gt; Exotics_yr0 &lt;- Ndep_sy -&gt; SoilFert_yr0 &lt;- Oldfield_past_s &lt;- Dist_to_Road_s -&gt; pastlivestock_yr0 -&gt; Native_yr0 &lt;- livestock_sy -&gt; Native_psy\"\n\n$open\n  [1]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE\n\n\nCode\n# print conditional independences\nimpliedConditionalIndependencies(richness_dag)\n\n\nClm_ _||_ D__R\nClm_ _||_ NPKT\nClm_ _||_ Ol__\nClm_ _||_ SF_0 | Ndp_\nClm_ _||_ lvs_\nClm_ _||_ ps_0\nD__R _||_ NPKT\nD__R _||_ Nt_0 | Clm_, SF_0, lvs_, ps_0\nD__R _||_ Nt_0 | Ndp_, SF_0, lvs_, ps_0\nD__R _||_ Nt_0 | Ol__, lvs_, ps_0\nD__R _||_ Ndp_\nD__R _||_ SF_0 | Ol__, lvs_, ps_0\nEx_0 _||_ NPKT\nEx_0 _||_ Nt_0 | Clm_, SF_0, lvs_, ps_0\nEx_0 _||_ Ol__ | D__R, Ndp_, SF_0, lvs_, ps_0\nNPKT _||_ Nt_0\nNPKT _||_ Ndp_\nNPKT _||_ Ol__\nNPKT _||_ SF_0\nNPKT _||_ lvs_\nNPKT _||_ ps_0\nNt_0 _||_ Ndp_ | Clm_, SF_0, lvs_, ps_0\nNt_0 _||_ Ol__ | Ndp_, SF_0, lvs_, ps_0\nNt_0 _||_ Ol__ | Clm_, SF_0, lvs_, ps_0\nNdp_ _||_ Ol__\nNdp_ _||_ lvs_\nNdp_ _||_ ps_0\nOl__ _||_ lvs_ | D__R\nOl__ _||_ ps_0 | D__R\n\n\nCode\n# Now, lets plot the open paths\nggdag_paths(richness_dag, shadow = TRUE)\n\n\n\n\n\n\n\n\n\nCode\n#to identify descendants \nggdag_descendants(richness_dag, \"Climate_sy\")\n\n\n\n\n\n\n\n\n\nCode\nggdag_descendants(richness_dag, \"Ndep_sy\")\n\n\n\n\n\n\n\n\n\nCode\nggdag_descendants(richness_dag, \"SoilFert_yr0\")\n\n\n\n\n\n\n\n\n\nCode\n#identify the paths that need to be adjusted for - because we have specified the exposure to be invasive abundance and outcome to be richness, we are looking at the direct effect (not the mediated effect), so the direct and total will be thed same here: \nadjustmentSets(richness_dag, effect = \"direct\")\n\n\n{ Climate_sy, Dist_to_Road_s, Exotics_yr0, NPKTreatment, Native_yr0,\n  Ndep_sy, Oldfield_past_s, SoilFert_yr0, livestock_sy,\n  pastlivestock_yr0 }\n\n\nCode\nadjustmentSets(richness_dag, effect = \"total\")\n\n\n{ Climate_sy, Dist_to_Road_s, Exotics_yr0, NPKTreatment, Native_yr0,\n  Ndep_sy, Oldfield_past_s, SoilFert_yr0, livestock_sy,\n  pastlivestock_yr0 }\n\n\nCode\n#Let's plot the identified the adjustment set to meet the backdoor criterion \nggdag_adjustment_set(richness_dag, shadow = TRUE)\n\n\n\n\n\n\n\n\n\nLet’s compare the above with analyzing the treatment effect, which was randomized. You can see we don’t need to adjust to estimate the treatment effect on richness.\n\n\nCode\nNPK_dag &lt;- dagify(Native_psy ~ Exotic_psy + NPKTreatment + Exotics_yr0 + Native_yr0 + Ndep_sy + Climate_sy + SoilFert_yr0 +  livestock_sy + Dist_to_Road_s + Oldfield_past_s + pastlivestock_yr0, \n Exotic_psy ~  NPKTreatment + Exotics_yr0 + Native_yr0 + Ndep_sy + Climate_sy + SoilFert_yr0 +\n livestock_sy + Dist_to_Road_s + Oldfield_past_s + pastlivestock_yr0, \n Ndep_sy ~  Climate_sy, \n Exotics_yr0 ~ Climate_sy + Ndep_sy + pastlivestock_yr0 + livestock_sy + Dist_to_Road_s + SoilFert_yr0,\n Native_yr0 ~ Climate_sy + pastlivestock_yr0 + livestock_sy + SoilFert_yr0, \n   SoilFert_yr0 ~ Ndep_sy + Oldfield_past_s +  livestock_sy +  pastlivestock_yr0, \n  Oldfield_past_s  ~ Dist_to_Road_s, \n  livestock_sy  ~ pastlivestock_yr0 + Dist_to_Road_s,\n   pastlivestock_yr0 ~ Dist_to_Road_s, \n  exposure = \"NPKTreatment\", \n  outcome = \"Native_psy\",\n labels = c(outcome = \"richness\",\n           exposure = \"NPKTreatment\"))\n\nggdag_status(NPK_dag,\n             use_labels = \"label\",\n             text = TRUE,\n             label_alpha = 0.5) + theme_dag()\n\n\n\n\n\n\n\n\n\nCode\nadjustmentSets(NPK_dag, effect = \"total\")\n\n\n {}"
  },
  {
    "objectID": "exercises/ggdag_demo.html#lets-plot-the-arif-et-al.-2022-coral-reef-example-for-more-practice",
    "href": "exercises/ggdag_demo.html#lets-plot-the-arif-et-al.-2022-coral-reef-example-for-more-practice",
    "title": "Constructing and Analyzing DAGs in dagitty and ggdag_demo",
    "section": "Let’s plot the Arif et al. (2022) Coral Reef Example for more practice!",
    "text": "Let’s plot the Arif et al. (2022) Coral Reef Example for more practice!\nYou’ll note, to analyze the DAG, you’ll need to specify the exposure and the outcome. To look at a bunch of different relationships, this needs to be done one by one to avoid a “causal salad” in the amazing terms of Arif et al. \nYou can practice writing code to do this for a given exposure and outcome, modifying the code above for this example below!\n\n\nCode\nseychelles &lt;- dagify(\n  regime_shift ~ initial_algae + wave_exposure + herbivore_biomass + depth + nutrients + branching_coral + structural_complexity,\n  initial_algae ~ wave_exposure + herbivore_biomass + nutrients,\n  herbivore_biomass ~ mpa + structural_complexity,\n  nutrients ~ depth,\n  branching_coral ~ mpa + depth + wave_exposure,\n  structural_complexity ~ branching_coral\n)\n\nplot(seychelles)\n\n\nPlot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.\n\n\n\n\n\n\n\n\n\nYour turn – pick an exposure and outcome and go!\nTo consider other sources of bias (collider, omitted variable bias, etc) see https://lfoswald.github.io/2021-spring-stats2/materials/session-4/04-online-tutorial/"
  },
  {
    "objectID": "readings.html#due-wed.-212-discussion-leads-miles-teresea",
    "href": "readings.html#due-wed.-212-discussion-leads-miles-teresea",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 2/12 [Discussion lead(s): Miles & Teresea]",
    "text": "Due Wed. 2/12 [Discussion lead(s): Miles & Teresea]\n\nSiegel et al. 2022\n\nXu et al. 2022\n\ndemo RMarkdowns from Siegel & Dee (2025) Ecology Letters for weighting and matching on course GitHub here",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-226---fixed-effects-discussion",
    "href": "readings.html#due-wed.-226---fixed-effects-discussion",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 2/26 - Fixed Effects Discussion",
    "text": "Due Wed. 2/26 - Fixed Effects Discussion\n\nDudney et al. 2021\n\nDee et al., 2023\n\nOptional (but good for the discussants!) Meehan et al. 2011 vs. Larsen 2013\ndemo Dee et al. Rmarkdown on course GitHub",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-42-discussion-leads-rachel-____",
    "href": "readings.html#due-wed.-42-discussion-leads-rachel-____",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 4/2 [Discussion lead(s): Rachel & ____ ]",
    "text": "Due Wed. 4/2 [Discussion lead(s): Rachel & ____ ]\nRevisit these papers in light of the assumptions of different designs, their different estimands, generalizability, and their assumptions:  - Simler-Williamson & Germino 2022\n- Dee et al., 2023 - TBD on sensitivity and placebo tests",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#logistic-regression",
    "href": "readings.html#logistic-regression",
    "title": "Readings by Class & Additional Materials",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic Regression Refresher",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-219-discussion-leadsadvyth-kathryn",
    "href": "readings.html#due-wed.-219-discussion-leadsadvyth-kathryn",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 2/19 [Discussion lead(s):Advyth + Kathryn ]",
    "text": "Due Wed. 2/19 [Discussion lead(s):Advyth + Kathryn ]\nDUE Draft of Revised DAG OR literature review proposal (1 page max.)\n1) Simler-Williamson & Germino 2022\ndemo RMarkdown using Boulder Open Space fire and vegetation case on course GitHub (data in GoogleDrive)",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-226---fixed-effects-discussion-hunter-treson",
    "href": "readings.html#due-wed.-226---fixed-effects-discussion-hunter-treson",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 2/26 - Fixed Effects Discussion [Hunter + Treson]",
    "text": "Due Wed. 2/26 - Fixed Effects Discussion [Hunter + Treson]\n\nDudney et al. 2021\n\nDee et al., 2023\n\nOptional (but good for the discussants!) Meehan et al. 2011 vs. Larsen 2013\ndemo Dee et al. Rmarkdown on course GitHub",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-312-instrumental-variables-applications-discussion-leads-hope-katie",
    "href": "readings.html#due-wed.-312-instrumental-variables-applications-discussion-leads-hope-katie",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 3/12 Instrumental Variables Applications [Discussion lead(s): Hope & Katie]",
    "text": "Due Wed. 3/12 Instrumental Variables Applications [Discussion lead(s): Hope & Katie]\n\nSims 2010\n\nMacDonald & Mordecai 2020\ndemo",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-319-rdd-applications-discussion-leads-anna-jiacheng",
    "href": "readings.html#due-wed.-319-rdd-applications-discussion-leads-anna-jiacheng",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 3/19 RDD Applications [Discussion lead(s): Anna & Jiacheng]",
    "text": "Due Wed. 3/19 RDD Applications [Discussion lead(s): Anna & Jiacheng]\n\nEnglander 2019\n\nNoack et al. 2022\nOptional paper: Burgess et al. 2019",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-42-discussion-leads-rachel-luis",
    "href": "readings.html#due-wed.-42-discussion-leads-rachel-luis",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 4/2 [Discussion lead(s): Rachel & Luis ]",
    "text": "Due Wed. 4/2 [Discussion lead(s): Rachel & Luis ]\nRevisit these papers in light of the assumptions of different designs, their different estimands, generalizability, and their assumptions:  - Simler-Williamson & Germino 2022\n- Dee et al., 2023 - TBD on sensitivity and placebo tests",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed-423-project-presentations",
    "href": "readings.html#due-wed-423-project-presentations",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed 4/23 Project Presentations",
    "text": "Due Wed 4/23 Project Presentations",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed-430-project-presentations-or-no-class",
    "href": "readings.html#due-wed-430-project-presentations-or-no-class",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed 4/30 Project Presentations (or no class)",
    "text": "Due Wed 4/30 Project Presentations (or no class)\nProject Presentations Continued",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#projects-due-friday-may-2",
    "href": "readings.html#projects-due-friday-may-2",
    "title": "Readings by Class & Additional Materials",
    "section": "Projects Due Friday May 2",
    "text": "Projects Due Friday May 2\nSee Assignments & Evaluation tab for more information",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "demos/IPTW.html",
    "href": "demos/IPTW.html",
    "title": "IPTW",
    "section": "",
    "text": "This document is to help students understand how the Inverse Probability of Treatment Weighting (IPTW) method works and how to apply it when adjusting for treatment assignment in observational studies.\nBelow a table with the formulas for Weight (IPTW) included for clarity. The formulas are added to show how the weights are calculated based on the Probability of High Pollution (P(T=1)) for each island.\n\nDistance to Shoreline: Represents the proximity of the island to the shoreline.\nTreatment (High vs Low Pollution): 1 for high pollution exposure, 0 for low pollution exposure.\nProbability of High Pollution (P(T=1)): The estimated likelihood of high pollution exposure for each island.\n\n\nUpdated Table with Formulas for IPTW:\n\n\n\n\n\n\n\n\n\n\nIsland ID\nDistance to Shoreline (km)\nTreatment (T)\nProbability of High Pollution (P(T=1))\nWeight (IPTW)\n\n\n\n\n1\n1.2\n1\n0.85\n( = 1.18 )\n\n\n2\n2.5\n0\n0.45\n( = 1.82 )\n\n\n3\n0.8\n1\n0.70\n( = 1.43 )\n\n\n4\n3.0\n0\n0.30\n( = 1.43 )\n\n\n5\n1.5\n1\n0.75\n( = 1.33 )\n\n\n6\n4.0\n0\n0.20\n( = 1.25 )\n\n\n7\n2.0\n1\n0.90\n( = 1.11 )\n\n\n8\n5.0\n0\n0.10\n( = 1.11 )\n\n\n\n\n\nFormulas for Calculating Weight (IPTW):\n\nFor treated islands (T=1): [ = ] Where ( P(T=1) ) is the probability of being exposed to high pollution (probability of treatment).\nFor untreated islands (T=0): [ = ] Where ( 1 - P(T=1) ) represents the probability of being exposed to low pollution (complement of treatment probability).\n\n\n\nExample of Calculation:\n\nFor Island 1 (treated, high pollution):\n\nProbability of high pollution = 0.85\nWeight: [ = 1.18 ]\n\nFor Island 2 (untreated, low pollution):\n\nProbability of high pollution = 0.45\nWeight: [ = 1.82 ]"
  },
  {
    "objectID": "exercises/index.html#bootstrapping-standard-errors-for-propensity-score-matching",
    "href": "exercises/index.html#bootstrapping-standard-errors-for-propensity-score-matching",
    "title": "Exercises in R",
    "section": "Bootstrapping standard errors for propensity score matching",
    "text": "Bootstrapping standard errors for propensity score matching\nAndrew Heiss has code and an example here",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-317-project-consultations",
    "href": "readings.html#due-mon.-317-project-consultations",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 3/17 Project Consultations",
    "text": "Due Mon. 3/17 Project Consultations\n1:1 consultations with Laura in lieu of class. Please read the rest of the following papers and come to the call with a proposal for the statistical design you’ll use and reasoning why: Siegel & Dee (2025) Ecology Letters - up until section 5.1. Larsen et al. 2019 - up to section 8, and the Discussion. Can skim in between for preview of what’s to come. 3)\nSchedule to sign up here",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-319-first-project-presentations",
    "href": "readings.html#due-wed.-319-first-project-presentations",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 3/19 First Project Presentations",
    "text": "Due Wed. 3/19 First Project Presentations\nBrief presentation (3-5 minutes) of your project: the question you are addressing, its applications, your DAG (if applicable) and your proposed method for feedback and Q&A.",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-monday-331-regression-discontinuity-designs",
    "href": "readings.html#due-monday-331-regression-discontinuity-designs",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Monday 3/31 Regression Discontinuity Designs",
    "text": "Due Monday 3/31 Regression Discontinuity Designs\n\nThe Effect Ch. 20 Optional: Angrist & Pischke 2015, Chapter 4 (note: this is in Mastering ’Metrics in the GDrive)",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-42-rdd-applications-discussion-leads-anna-jiacheng",
    "href": "readings.html#due-wed.-42-rdd-applications-discussion-leads-anna-jiacheng",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 4/2 RDD Applications [Discussion lead(s): Anna & Jiacheng]",
    "text": "Due Wed. 4/2 RDD Applications [Discussion lead(s): Anna & Jiacheng]\n\nEnglander 2019\n\nNoack et al. 2022\nOptional paper: Burgess et al. 2019",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-47-project-check-ins-or-comparison-of-designs-sensitivity-tests",
    "href": "readings.html#due-mon.-47-project-check-ins-or-comparison-of-designs-sensitivity-tests",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 4/7 Project Check ins or Comparison of Designs & Sensitivity Tests",
    "text": "Due Mon. 4/7 Project Check ins or Comparison of Designs & Sensitivity Tests\n\nSiegel & Dee (2025) Ecology Letters\nButsic et al. 2017\nArif & MacNiel 2021 Ecosphere\nTBD on sensitivity tests",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-49-comparison-of-designs-discussion-leads-rachel-luis",
    "href": "readings.html#due-wed.-49-comparison-of-designs-discussion-leads-rachel-luis",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 4/9 Comparison of Designs [Discussion lead(s): Rachel & Luis ]",
    "text": "Due Wed. 4/9 Comparison of Designs [Discussion lead(s): Rachel & Luis ]\nRevisit these papers in light of the assumptions of different designs, their different estimands, generalizability, and their assumptions:  - Simler-Williamson & Germino 2022\n- Dee et al., 2023 - TBD on sensitivity and placebo tests",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-416-power-standard-error-estimation-and-inference",
    "href": "readings.html#due-wed.-416-power-standard-error-estimation-and-inference",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 4/16 Power, standard error estimation, and inference",
    "text": "Due Wed. 4/16 Power, standard error estimation, and inference",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-319-regression-discontinuity-designs",
    "href": "readings.html#due-wed.-319-regression-discontinuity-designs",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 3/19 Regression Discontinuity Designs",
    "text": "Due Wed. 3/19 Regression Discontinuity Designs\n\nThe Effect Ch. 20 Optional: Angrist & Pischke 2015, Chapter 4 (note: this is in Mastering ’Metrics in the GDrive)",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-monday-331-rdd-applications-discussion-leads-anna-jiacheng",
    "href": "readings.html#due-monday-331-rdd-applications-discussion-leads-anna-jiacheng",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Monday 3/31 RDD Applications [Discussion lead(s): Anna & Jiacheng]",
    "text": "Due Monday 3/31 RDD Applications [Discussion lead(s): Anna & Jiacheng]\n\nEnglander 2019\n\nNoack et al. 2022\nOptional paper: Burgess et al. 2019",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-42-first-project-presentations",
    "href": "readings.html#due-wed.-42-first-project-presentations",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 4/2 First Project Presentations",
    "text": "Due Wed. 4/2 First Project Presentations\nBrief presentation (5 minutes) of your project: the “so what” big picture, the question you are addressing, its applications, your DAG and/or data context, and your proposed method for feedback and Q&A.",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-47-presentations-continued-intro-to-robustness-checks",
    "href": "readings.html#due-mon.-47-presentations-continued-intro-to-robustness-checks",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 4/7 Presentations Continued; intro to robustness checks",
    "text": "Due Mon. 4/7 Presentations Continued; intro to robustness checks",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed.-49-comparison-of-designs-discussion-leads-rachel-luis-sensitivity-tests-laura",
    "href": "readings.html#due-wed.-49-comparison-of-designs-discussion-leads-rachel-luis-sensitivity-tests-laura",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 4/9 Comparison of Designs [Discussion lead(s): Rachel & Luis ] & Sensitivity Tests (Laura)",
    "text": "Due Wed. 4/9 Comparison of Designs [Discussion lead(s): Rachel & Luis ] & Sensitivity Tests (Laura)\nRevisit these papers in light of the assumptions of different designs, their different estimands, generalizability, and their assumptions:  - Simler-Williamson & Germino 2022\n- Dee et al., 2023 - TBD on sensitivity and placebo tests\nOptional to revisit for methods comparison 1) Siegel & Dee (2025) Ecology Letters 2) Butsic et al. 2017 3) Arif & MacNiel 2021 Ecosphere",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-mon.-421-dr.-rebecca-spakes-generalizability",
    "href": "readings.html#due-mon.-421-dr.-rebecca-spakes-generalizability",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 4/21 Dr. Rebecca Spakes: Generalizability",
    "text": "Due Mon. 4/21 Dr. Rebecca Spakes: Generalizability\nRead Spake et al. 2022",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "demos/RosenbaumBounds.html",
    "href": "demos/RosenbaumBounds.html",
    "title": "Rosenbaum Bounds Sensitivity Analysis in Ecology",
    "section": "",
    "text": "This document demonstrates how to perform Rosenbaum Bounds Sensitivity Analysis using R, applied to an ecological study assessing the impact of deforestation on bird species richness."
  },
  {
    "objectID": "demos/RosenbaumBounds.html#introduction",
    "href": "demos/RosenbaumBounds.html#introduction",
    "title": "Rosenbaum Bounds Sensitivity Analysis in Ecology",
    "section": "",
    "text": "This document demonstrates how to perform Rosenbaum Bounds Sensitivity Analysis using R, applied to an ecological study assessing the impact of deforestation on bird species richness."
  },
  {
    "objectID": "demos/RosenbaumBounds.html#load-required-libraries",
    "href": "demos/RosenbaumBounds.html#load-required-libraries",
    "title": "Rosenbaum Bounds Sensitivity Analysis in Ecology",
    "section": "Load Required Libraries",
    "text": "Load Required Libraries\n\n# Install packages if not already installed\nif (!require(MatchIt)) install.packages(\"MatchIt\", dependencies = TRUE)\n\nLoading required package: MatchIt\n\nif (!require(rbounds)) install.packages(\"rbounds\", dependencies = TRUE)\n\nLoading required package: rbounds\n\n# Load the libraries\nlibrary(MatchIt)\nlibrary(rbounds)"
  },
  {
    "objectID": "demos/RosenbaumBounds.html#simulate-ecological-data",
    "href": "demos/RosenbaumBounds.html#simulate-ecological-data",
    "title": "Rosenbaum Bounds Sensitivity Analysis in Ecology",
    "section": "Simulate Ecological Data",
    "text": "Simulate Ecological Data\nWe create a dataset where bird species richness is compared between deforested and non-deforested sites while controlling for elevation and precipitation.\n\nset.seed(123)\ndata &lt;- data.frame(\n  deforestation = rep(c(1, 0), each = 50),\n  species_richness = c(rnorm(50, mean = 10, sd = 3), rnorm(50, mean = 15, sd = 3)),\n  elevation = rnorm(100, mean = 500, sd = 100),\n  precipitation = rnorm(100, mean = 1000, sd = 200)\n)"
  },
  {
    "objectID": "demos/RosenbaumBounds.html#perform-propensity-score-matching",
    "href": "demos/RosenbaumBounds.html#perform-propensity-score-matching",
    "title": "Rosenbaum Bounds Sensitivity Analysis in Ecology",
    "section": "Perform Propensity Score Matching",
    "text": "Perform Propensity Score Matching\nWe match deforested and non-deforested sites based on elevation and precipitation.\n\nmatch_model &lt;- matchit(deforestation ~ elevation + precipitation, data = data, method = \"nearest\")\nmatched_data &lt;- match.data(match_model)"
  },
  {
    "objectID": "demos/RosenbaumBounds.html#conduct-sensitivity-analysis-with-rosenbaum-bounds",
    "href": "demos/RosenbaumBounds.html#conduct-sensitivity-analysis-with-rosenbaum-bounds",
    "title": "Rosenbaum Bounds Sensitivity Analysis in Ecology",
    "section": "Conduct Sensitivity Analysis with Rosenbaum Bounds",
    "text": "Conduct Sensitivity Analysis with Rosenbaum Bounds\nWe check how sensitive our results are to hidden bias.\n\n# Extract matched treatment and control outcomes\ntreated_outcomes &lt;- matched_data$species_richness[matched_data$deforestation == 1]\ncontrol_outcomes &lt;- matched_data$species_richness[matched_data$deforestation == 0]\n\n# Perform Rosenbaum sensitivity analysis\npsens(x = treated_outcomes, y = control_outcomes, Gamma = 1.5)  # Adjust Gamma to test sensitivity\n\n\n Rosenbaum Sensitivity Test for Wilcoxon Signed Rank P-Value \n \nUnconfounded estimate ....  1 \n\n Gamma Lower bound Upper bound\n     1           1           1\n\n Note: Gamma is Odds of Differential Assignment To\n Treatment Due to Unobserved Factors"
  },
  {
    "objectID": "demos/RosenbaumBounds.html#interpretation-of-results",
    "href": "demos/RosenbaumBounds.html#interpretation-of-results",
    "title": "Rosenbaum Bounds Sensitivity Analysis in Ecology",
    "section": "Interpretation of Results",
    "text": "Interpretation of Results\n\nIf the p-value remains small at Γ = 1.5, our conclusions are robust to moderate hidden bias.\nIf the p-value becomes insignificant at lower values of Γ, our study is highly sensitive to hidden confounders."
  },
  {
    "objectID": "demos/RosenbaumBounds.html#conclusion",
    "href": "demos/RosenbaumBounds.html#conclusion",
    "title": "Rosenbaum Bounds Sensitivity Analysis in Ecology",
    "section": "Conclusion",
    "text": "Conclusion\nRosenbaum bounds allow us to assess the robustness of our ecological study results against unmeasured confounding. This is particularly useful when controlled experiments are infeasible.\n\nTake Aways\nRosenbaum bounds provide a way to assess how much an unmeasured confounder could influence the causal relationship between deforestation and bird species richness. In ecological studies, where controlled experiments are often impractical, this method helps gauge the reliability of observational findings."
  },
  {
    "objectID": "demos/RDD_2025_fire_insects.html",
    "href": "demos/RDD_2025_fire_insects.html",
    "title": "Simulation Study of Sharp and Fuzzy RDD",
    "section": "",
    "text": "This simulation study investigates the impact of a forest fire (discontinuity) on insect diversity using distance from the fire edge as the running variable."
  },
  {
    "objectID": "demos/RDD_2025_fire_insects.html#introduction",
    "href": "demos/RDD_2025_fire_insects.html#introduction",
    "title": "Simulation Study of Sharp and Fuzzy RDD",
    "section": "",
    "text": "This simulation study investigates the impact of a forest fire (discontinuity) on insect diversity using distance from the fire edge as the running variable."
  },
  {
    "objectID": "demos/RDD_2025_fire_insects.html#simulating-data",
    "href": "demos/RDD_2025_fire_insects.html#simulating-data",
    "title": "Simulation Study of Sharp and Fuzzy RDD",
    "section": "Simulating Data",
    "text": "Simulating Data\n\nset.seed(123)\nn &lt;- 500\ndistance &lt;- runif(n, -50, 50)  # Running variable: distance from fire edge\n\ndiversity_control &lt;- 50 + 0.5 * distance + rnorm(n, sd = 5)  # Control group trend\ndiversity_treated &lt;- 30 + 0.3 * distance + rnorm(n, sd = 5)  # Treated group trend\n\ntreatment &lt;- ifelse(distance &lt; 0, 1, 0)  # Treatment assignment at fire edge\ndiversity &lt;- ifelse(treatment == 1, diversity_treated, diversity_control)\n\n# Adding confounders\nvegetation_density &lt;- 80 - 0.6 * distance + rnorm(n, sd = 5)  # Higher closer to fire\nsoil_moisture &lt;- 40 + 0.4 * distance + rnorm(n, sd = 3)  # Lower closer to fire\n\ndata &lt;- data.frame(distance, treatment, diversity, vegetation_density, soil_moisture)"
  },
  {
    "objectID": "demos/RDD_2025_fire_insects.html#visualizing-the-discontinuity",
    "href": "demos/RDD_2025_fire_insects.html#visualizing-the-discontinuity",
    "title": "Simulation Study of Sharp and Fuzzy RDD",
    "section": "Visualizing the Discontinuity",
    "text": "Visualizing the Discontinuity\n\nggplot(data, aes(x = distance, y = diversity, color = factor(treatment))) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal() +\n  labs(title = \"Effect of Forest Fire on Insect Diversity\",\n       x = \"Distance from Fire Edge (meters)\",\n       y = \"Insect Diversity Index\",\n       color = \"Treatment\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "demos/RDD_2025_fire_insects.html#assessing-covariates-as-a-function-of-distance",
    "href": "demos/RDD_2025_fire_insects.html#assessing-covariates-as-a-function-of-distance",
    "title": "Simulation Study of Sharp and Fuzzy RDD",
    "section": "Assessing Covariates as a Function of Distance",
    "text": "Assessing Covariates as a Function of Distance\n\nggplot(data, aes(x = distance, y = vegetation_density)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  theme_minimal() +\n  labs(title = \"Vegetation Density as a Function of Distance\",\n       x = \"Distance from Fire Edge\",\n       y = \"Vegetation Density\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(data, aes(x = distance, y = soil_moisture)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"green\") +\n  theme_minimal() +\n  labs(title = \"Soil Moisture as a Function of Distance\",\n       x = \"Distance from Fire Edge\",\n       y = \"Soil Moisture\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "demos/RDD_2025_fire_insects.html#sharp-rdd-estimation",
    "href": "demos/RDD_2025_fire_insects.html#sharp-rdd-estimation",
    "title": "Simulation Study of Sharp and Fuzzy RDD",
    "section": "Sharp RDD Estimation",
    "text": "Sharp RDD Estimation\n\nrdd_object &lt;- rdd_data(y = diversity, x = distance, cutpoint = 0, covar = data[, c(\"vegetation_density\", \"soil_moisture\")])\nrdd_result &lt;- rdd_reg_lm(rdd_object)\nsummary(rdd_result)\n\n\nCall:\nlm(formula = y ~ ., data = dat_step1, weights = weights)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.9399  -3.1587   0.2082   3.4872  12.5939 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 29.87636    0.59444  50.260  &lt; 2e-16 ***\nD           20.89177    0.88546  23.594  &lt; 2e-16 ***\nx            0.29176    0.02164  13.484  &lt; 2e-16 ***\nx_right      0.19520    0.03106   6.284 7.22e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.87 on 496 degrees of freedom\nMultiple R-squared:  0.9489,    Adjusted R-squared:  0.9486 \nF-statistic:  3072 on 3 and 496 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "demos/RDD_2025_fire_insects.html#fuzzy-rdd-simulation-with-imperfect-treatment-assignment",
    "href": "demos/RDD_2025_fire_insects.html#fuzzy-rdd-simulation-with-imperfect-treatment-assignment",
    "title": "Simulation Study of Sharp and Fuzzy RDD",
    "section": "Fuzzy RDD Simulation (with Imperfect Treatment Assignment)",
    "text": "Fuzzy RDD Simulation (with Imperfect Treatment Assignment)\n\ntreatment_fuzzy &lt;- ifelse(distance &lt; 0, rbinom(n, 1, 0.8), rbinom(n, 1, 0.2))  # Imperfect compliance\ndiversity_fuzzy &lt;- ifelse(treatment_fuzzy == 1, diversity_treated, diversity_control)\n\ndata_fuzzy &lt;- data.frame(distance, treatment_fuzzy, diversity_fuzzy, vegetation_density, soil_moisture)\n\n\nggplot(data_fuzzy, aes(x = distance, y = diversity_fuzzy, color = factor(treatment_fuzzy))) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal() +\n  labs(title = \"Fuzzy RDD: Effect of Fire on Insect Diversity\",\n       x = \"Distance from Fire Edge\",\n       y = \"Insect Diversity Index\",\n       color = \"Observed Treatment\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nrdd_fuzzy_object &lt;- rdd_data(y = diversity_fuzzy, x = distance, cutpoint = 0, covar = c(\"vegetation_density\", \"soil_moisture\"), data = data_fuzzy)\nrdd_fuzzy_result &lt;- rdd_reg_lm(rdd_fuzzy_object, covariates = TRUE)\nsummary(rdd_fuzzy_result)\n\n\nCall:\nlm(formula = y ~ ., data = dat_step1, weights = weights)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.661  -4.477   0.739   5.487  28.000 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.15522    1.12319  29.519  &lt; 2e-16 ***\nD           12.33450    1.67307   7.372 7.07e-13 ***\nx            0.31123    0.04088   7.612 1.37e-13 ***\nx_right      0.21549    0.05869   3.672 0.000267 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.202 on 496 degrees of freedom\nMultiple R-squared:  0.7881,    Adjusted R-squared:  0.7868 \nF-statistic: 614.9 on 3 and 496 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "demos/RDD_2025_fire_insects.html#conclusion",
    "href": "demos/RDD_2025_fire_insects.html#conclusion",
    "title": "Simulation Study of Sharp and Fuzzy RDD",
    "section": "Conclusion",
    "text": "Conclusion\nThis simulation demonstrates the use of Sharp and Fuzzy RDD in evaluating the impact of forest fires on insect diversity. The sharp RDD estimates the direct effect, while the fuzzy RDD accounts for imperfect treatment assignment. Additionally, we assess vegetation density and soil moisture as potential confounders and visualize their relationship with distance from the fire edge."
  },
  {
    "objectID": "demos/RDD v2 insects and fire.html",
    "href": "demos/RDD v2 insects and fire.html",
    "title": "Simulation Study of Sharp and Fuzzy RDD v2",
    "section": "",
    "text": "This simulation study investigates the impact of a forest fire (discontinuity) on insect diversity using distance from the fire edge as the running variable."
  },
  {
    "objectID": "demos/RDD v2 insects and fire.html#introduction",
    "href": "demos/RDD v2 insects and fire.html#introduction",
    "title": "Simulation Study of Sharp and Fuzzy RDD v2",
    "section": "",
    "text": "This simulation study investigates the impact of a forest fire (discontinuity) on insect diversity using distance from the fire edge as the running variable."
  },
  {
    "objectID": "demos/RDD v2 insects and fire.html#simulating-data",
    "href": "demos/RDD v2 insects and fire.html#simulating-data",
    "title": "Simulation Study of Sharp and Fuzzy RDD v2",
    "section": "Simulating Data",
    "text": "Simulating Data\n\nset.seed(123)\nn &lt;- 500\ndistance &lt;- runif(n, -50, 50)  # Running variable: distance from fire edge\n\ndiversity_control &lt;- 50 + 0.5 * distance + rnorm(n, sd = 5)  # Control group trend\ndiversity_treated &lt;- 30 + 0.3 * distance + rnorm(n, sd = 5)  # Treated group trend\n\ntreatment &lt;- ifelse(distance &lt; 0, 1, 0)  # Treatment assignment at fire edge\ndiversity &lt;- ifelse(treatment == 1, diversity_treated, diversity_control)\n\n# Adding covariates that are non-confounding within -25 to +25\ndistance_effect &lt;- ifelse(abs(distance) &lt;= 25, 0, 1)\nvegetation_density &lt;- 80 - 0.6 * distance * distance_effect + rnorm(n, sd = 5)\nsoil_moisture &lt;- 40 + 0.4 * distance * distance_effect + rnorm(n, sd = 3)\n\ndata &lt;- data.frame(distance, treatment, diversity, vegetation_density, soil_moisture)"
  },
  {
    "objectID": "demos/RDD v2 insects and fire.html#visualizing-the-discontinuity",
    "href": "demos/RDD v2 insects and fire.html#visualizing-the-discontinuity",
    "title": "Simulation Study of Sharp and Fuzzy RDD v2",
    "section": "Visualizing the Discontinuity",
    "text": "Visualizing the Discontinuity\n\nggplot(data, aes(x = distance, y = diversity, color = factor(treatment))) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal() +\n  labs(title = \"Effect of Forest Fire on Insect Diversity\",\n       x = \"Distance from Fire Edge (meters)\",\n       y = \"Insect Diversity Index\",\n       color = \"Treatment\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "demos/RDD v2 insects and fire.html#assessing-covariates-as-a-function-of-distance",
    "href": "demos/RDD v2 insects and fire.html#assessing-covariates-as-a-function-of-distance",
    "title": "Simulation Study of Sharp and Fuzzy RDD v2",
    "section": "Assessing Covariates as a Function of Distance",
    "text": "Assessing Covariates as a Function of Distance\n\nggplot(data, aes(x = distance, y = vegetation_density)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  theme_minimal() +\n  labs(title = \"Vegetation Density as a Function of Distance\",\n       x = \"Distance from Fire Edge\",\n       y = \"Vegetation Density\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(data, aes(x = distance, y = soil_moisture)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"green\") +\n  theme_minimal() +\n  labs(title = \"Soil Moisture as a Function of Distance\",\n       x = \"Distance from Fire Edge\",\n       y = \"Soil Moisture\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "demos/RDD v2 insects and fire.html#sharp-rdd-estimation",
    "href": "demos/RDD v2 insects and fire.html#sharp-rdd-estimation",
    "title": "Simulation Study of Sharp and Fuzzy RDD v2",
    "section": "Sharp RDD Estimation",
    "text": "Sharp RDD Estimation\n\nrdd_object &lt;- rdd_data(y = diversity, x = distance, cutpoint = 0, covar = data[, c(\"vegetation_density\", \"soil_moisture\")])\nrdd_result &lt;- rdd_reg_lm(rdd_object)\nsummary(rdd_result)\n\n\nCall:\nlm(formula = y ~ ., data = dat_step1, weights = weights)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.9399  -3.1587   0.2082   3.4872  12.5939 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 29.87636    0.59444  50.260  &lt; 2e-16 ***\nD           20.89177    0.88546  23.594  &lt; 2e-16 ***\nx            0.29176    0.02164  13.484  &lt; 2e-16 ***\nx_right      0.19520    0.03106   6.284 7.22e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.87 on 496 degrees of freedom\nMultiple R-squared:  0.9489,    Adjusted R-squared:  0.9486 \nF-statistic:  3072 on 3 and 496 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "demos/RDD v2 insects and fire.html#fuzzy-rdd-simulation-with-imperfect-treatment-assignment",
    "href": "demos/RDD v2 insects and fire.html#fuzzy-rdd-simulation-with-imperfect-treatment-assignment",
    "title": "Simulation Study of Sharp and Fuzzy RDD v2",
    "section": "Fuzzy RDD Simulation (with Imperfect Treatment Assignment)",
    "text": "Fuzzy RDD Simulation (with Imperfect Treatment Assignment)\n\ntreatment_fuzzy &lt;- ifelse(distance &lt; 0, rbinom(n, 1, 0.8), rbinom(n, 1, 0.2))  # Imperfect compliance\ndiversity_fuzzy &lt;- ifelse(treatment_fuzzy == 1, diversity_treated, diversity_control)\n\ndata_fuzzy &lt;- data.frame(distance, treatment_fuzzy, diversity_fuzzy, vegetation_density, soil_moisture)\n\n\nggplot(data_fuzzy, aes(x = distance, y = diversity_fuzzy, color = factor(treatment_fuzzy))) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal() +\n  labs(title = \"Fuzzy RDD: Effect of Fire on Insect Diversity\",\n       x = \"Distance from Fire Edge\",\n       y = \"Insect Diversity Index\",\n       color = \"Observed Treatment\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nrdd_fuzzy_object &lt;- rdd_data(y = diversity_fuzzy, x = distance, cutpoint = 0, covar =  c(\"vegetation_density\", \"soil_moisture\"), data = data_fuzzy)\nrdd_fuzzy_result &lt;- rdd_reg_lm(rdd_fuzzy_object, covariates = TRUE)\nsummary(rdd_fuzzy_result)\n\n\nCall:\nlm(formula = y ~ ., data = dat_step1, weights = weights)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.661  -4.477   0.739   5.487  28.000 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.15522    1.12319  29.519  &lt; 2e-16 ***\nD           12.33450    1.67307   7.372 7.07e-13 ***\nx            0.31123    0.04088   7.612 1.37e-13 ***\nx_right      0.21549    0.05869   3.672 0.000267 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.202 on 496 degrees of freedom\nMultiple R-squared:  0.7881,    Adjusted R-squared:  0.7868 \nF-statistic: 614.9 on 3 and 496 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "demos/RDD v2 insects and fire.html#conclusion",
    "href": "demos/RDD v2 insects and fire.html#conclusion",
    "title": "Simulation Study of Sharp and Fuzzy RDD v2",
    "section": "Conclusion",
    "text": "Conclusion\nThis simulation demonstrates the use of Sharp and Fuzzy RDD in evaluating the impact of forest fires on insect diversity. The sharp RDD estimates the direct effect, while the fuzzy RDD accounts for imperfect treatment assignment. Additionally, we assess vegetation density and soil moisture as covariates that are non-confounding within -25 to +25 from the fire edge but still vary beyond this range."
  },
  {
    "objectID": "demos/Sensivity Test2.html",
    "href": "demos/Sensivity Test2.html",
    "title": "Sensitivity Tests",
    "section": "",
    "text": "✅ Placebo tests ensure the estimated effect is not driven by spurious correlations. ✅ Sensitivity analyses check if conclusions remain valid under different assumptions. ✅ R provides powerful tools (e.g., EValue, rbounds, placebo regressions) for assessing robustness in causal inference.\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nLoading required package: zoo\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nSee details in:\n\nCarlos Cinelli and Chad Hazlett (2020). Making Sense of Sensitivity: Extending Omitted Variable Bias. Journal of the Royal Statistical Society, Series B (Statistical Methodology)."
  },
  {
    "objectID": "demos/Sensivity Test2.html#example-1-placebo-test-with-pre-treatment-outcomes",
    "href": "demos/Sensivity Test2.html#example-1-placebo-test-with-pre-treatment-outcomes",
    "title": "Sensitivity Tests",
    "section": "Example 1: Placebo Test with Pre-Treatment Outcomes",
    "text": "Example 1: Placebo Test with Pre-Treatment Outcomes\nWe check if a policy or treatment has an effect before it was implemented. If significant effects are found before the actual intervention, the causal claim is questionable.\nScenario Examples: A state implements a minimum wage increase in 2018, and we estimate its effect on employment. A placebo test checks if a pre-2018 placebo treatment also shows an effect.\n\n# Simulate data\nset.seed(123)\ndata &lt;- tibble(\n  state = rep(1:50, each = 10),\n  year = rep(2009:2018, times = 50),\n  treated = ifelse(state &lt;= 25, 1, 0), # 25 treated states\n  post_treatment = ifelse(year &gt;= 2018, 1, 0), # Policy starts in 2018\n  employment = rnorm(500, mean = 100, sd = 10) - 2 * treated * post_treatment # Simulated policy effect\n)\n\n# Run regression for actual treatment\nmodel_actual &lt;- lm(employment ~ treated * post_treatment + factor(state) + factor(year), data = data)\nsummary(model_actual)\n\n\nCall:\nlm(formula = employment ~ treated * post_treatment + factor(state) + \n    factor(year), data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-28.7438  -6.0938  -0.2846   6.0033  29.7668 \n\nCoefficients: (2 not defined because of singularities)\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            103.0010     3.3976  30.315   &lt;2e-16 ***\ntreated                  0.4352     4.4290   0.098   0.9218    \npost_treatment          -2.6077     2.4649  -1.058   0.2907    \nfactor(state)2           1.3400     4.4192   0.303   0.7619    \nfactor(state)3          -4.9918     4.4192  -1.130   0.2593    \nfactor(state)4           2.4742     4.4192   0.560   0.5759    \nfactor(state)5          -0.8334     4.4192  -0.189   0.8505    \nfactor(state)6           1.4706     4.4192   0.333   0.7395    \nfactor(state)7           0.4846     4.4192   0.110   0.9127    \nfactor(state)8          -4.3754     4.4192  -0.990   0.3227    \nfactor(state)9           2.3847     4.4192   0.540   0.5897    \nfactor(state)10          3.6247     4.4192   0.820   0.4125    \nfactor(state)11         -4.7047     4.4192  -1.065   0.2876    \nfactor(state)12         -3.9754     4.4192  -0.900   0.3688    \nfactor(state)13         -1.8506     4.4192  -0.419   0.6756    \nfactor(state)14         -0.4089     4.4192  -0.093   0.9263    \nfactor(state)15         -5.4867     4.4192  -1.242   0.2151    \nfactor(state)16          0.5274     4.4192   0.119   0.9050    \nfactor(state)17          2.1581     4.4192   0.488   0.6255    \nfactor(state)18         -0.3129     4.4192  -0.071   0.9436    \nfactor(state)19         -0.7456     4.4192  -0.169   0.8661    \nfactor(state)20         -3.4181     4.4192  -0.773   0.4397    \nfactor(state)21          2.3661     4.4192   0.535   0.5926    \nfactor(state)22         -2.6689     4.4192  -0.604   0.5462    \nfactor(state)23         -0.7415     4.4192  -0.168   0.8668    \nfactor(state)24         -1.5179     4.4192  -0.343   0.7314    \nfactor(state)25         -1.5951     4.4192  -0.361   0.7183    \nfactor(state)26         -0.1501     4.4192  -0.034   0.9729    \nfactor(state)27          1.4536     4.4192   0.329   0.7424    \nfactor(state)28          1.6677     4.4192   0.377   0.7061    \nfactor(state)29          0.7183     4.4192   0.163   0.8710    \nfactor(state)30          6.1122     4.4192   1.383   0.1673    \nfactor(state)31         -2.6271     4.4192  -0.594   0.5525    \nfactor(state)32          0.7785     4.4192   0.176   0.8602    \nfactor(state)33          2.3192     4.4192   0.525   0.6000    \nfactor(state)34         -2.3832     4.4192  -0.539   0.5900    \nfactor(state)35         -0.6556     4.4192  -0.148   0.8821    \nfactor(state)36         -1.8980     4.4192  -0.429   0.6678    \nfactor(state)37          1.6467     4.4192   0.373   0.7096    \nfactor(state)38         -2.1433     4.4192  -0.485   0.6279    \nfactor(state)39         -0.1039     4.4192  -0.024   0.9813    \nfactor(state)40         -3.8974     4.4192  -0.882   0.3783    \nfactor(state)41         -3.3247     4.4192  -0.752   0.4523    \nfactor(state)42         -1.9829     4.4192  -0.449   0.6539    \nfactor(state)43          4.3652     4.4192   0.988   0.3238    \nfactor(state)44         -1.3018     4.4192  -0.295   0.7684    \nfactor(state)45         -3.7068     4.4192  -0.839   0.4020    \nfactor(state)46          1.9103     4.4192   0.432   0.6657    \nfactor(state)47          3.8897     4.4192   0.880   0.3792    \nfactor(state)48          1.0666     4.4192   0.241   0.8094    \nfactor(state)49          4.3279     4.4192   0.979   0.3280    \nfactor(state)50              NA         NA      NA       NA    \nfactor(year)2010        -2.9238     1.9763  -1.479   0.1397    \nfactor(year)2011        -2.4424     1.9763  -1.236   0.2172    \nfactor(year)2012        -2.9507     1.9763  -1.493   0.1361    \nfactor(year)2013        -3.5011     1.9763  -1.772   0.0772 .  \nfactor(year)2014        -2.7767     1.9763  -1.405   0.1607    \nfactor(year)2015        -1.2810     1.9763  -0.648   0.5172    \nfactor(year)2016        -4.3472     1.9763  -2.200   0.0284 *  \nfactor(year)2017        -1.8373     1.9763  -0.930   0.3531    \nfactor(year)2018             NA         NA      NA       NA    \ntreated:post_treatment  -4.2308     2.9461  -1.436   0.1517    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.882 on 440 degrees of freedom\nMultiple R-squared:  0.09686,   Adjusted R-squared:  -0.02424 \nF-statistic: 0.7999 on 59 and 440 DF,  p-value: 0.8544\n\n# Placebo Test: Using 2016 as the fake treatment year\ndata$placebo_post &lt;- ifelse(data$year &gt;= 2016, 1, 0)\nmodel_placebo &lt;- lm(employment ~ treated * placebo_post + factor(state) + factor(year), data = data)\nsummary(model_placebo)\n\n\nCall:\nlm(formula = employment ~ treated * placebo_post + factor(state) + \n    factor(year), data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-28.7205  -6.1867  -0.1883   6.1179  29.7436 \n\nCoefficients: (2 not defined because of singularities)\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          102.9777     3.4122  30.179   &lt;2e-16 ***\ntreated                0.4817     4.4640   0.108   0.9141    \nplacebo_post          -3.9403     2.2026  -1.789   0.0743 .  \nfactor(state)2         1.3400     4.4262   0.303   0.7622    \nfactor(state)3        -4.9918     4.4262  -1.128   0.2600    \nfactor(state)4         2.4742     4.4262   0.559   0.5765    \nfactor(state)5        -0.8334     4.4262  -0.188   0.8507    \nfactor(state)6         1.4706     4.4262   0.332   0.7399    \nfactor(state)7         0.4846     4.4262   0.109   0.9129    \nfactor(state)8        -4.3754     4.4262  -0.989   0.3234    \nfactor(state)9         2.3847     4.4262   0.539   0.5903    \nfactor(state)10        3.6247     4.4262   0.819   0.4133    \nfactor(state)11       -4.7047     4.4262  -1.063   0.2884    \nfactor(state)12       -3.9754     4.4262  -0.898   0.3696    \nfactor(state)13       -1.8506     4.4262  -0.418   0.6761    \nfactor(state)14       -0.4089     4.4262  -0.092   0.9264    \nfactor(state)15       -5.4867     4.4262  -1.240   0.2158    \nfactor(state)16        0.5274     4.4262   0.119   0.9052    \nfactor(state)17        2.1581     4.4262   0.488   0.6261    \nfactor(state)18       -0.3129     4.4262  -0.071   0.9437    \nfactor(state)19       -0.7456     4.4262  -0.168   0.8663    \nfactor(state)20       -3.4181     4.4262  -0.772   0.4404    \nfactor(state)21        2.3661     4.4262   0.535   0.5932    \nfactor(state)22       -2.6689     4.4262  -0.603   0.5468    \nfactor(state)23       -0.7415     4.4262  -0.168   0.8670    \nfactor(state)24       -1.5179     4.4262  -0.343   0.7318    \nfactor(state)25       -1.5951     4.4262  -0.360   0.7187    \nfactor(state)26       -0.1501     4.4262  -0.034   0.9730    \nfactor(state)27        1.4536     4.4262   0.328   0.7428    \nfactor(state)28        1.6677     4.4262   0.377   0.7065    \nfactor(state)29        0.7183     4.4262   0.162   0.8712    \nfactor(state)30        6.1122     4.4262   1.381   0.1680    \nfactor(state)31       -2.6271     4.4262  -0.594   0.5531    \nfactor(state)32        0.7785     4.4262   0.176   0.8605    \nfactor(state)33        2.3192     4.4262   0.524   0.6006    \nfactor(state)34       -2.3832     4.4262  -0.538   0.5906    \nfactor(state)35       -0.6556     4.4262  -0.148   0.8823    \nfactor(state)36       -1.8980     4.4262  -0.429   0.6683    \nfactor(state)37        1.6467     4.4262   0.372   0.7101    \nfactor(state)38       -2.1433     4.4262  -0.484   0.6285    \nfactor(state)39       -0.1039     4.4262  -0.023   0.9813    \nfactor(state)40       -3.8974     4.4262  -0.881   0.3791    \nfactor(state)41       -3.3247     4.4262  -0.751   0.4530    \nfactor(state)42       -1.9829     4.4262  -0.448   0.6544    \nfactor(state)43        4.3652     4.4262   0.986   0.3246    \nfactor(state)44       -1.3018     4.4262  -0.294   0.7688    \nfactor(state)45       -3.7068     4.4262  -0.837   0.4028    \nfactor(state)46        1.9103     4.4262   0.432   0.6662    \nfactor(state)47        3.8897     4.4262   0.879   0.3800    \nfactor(state)48        1.0666     4.4262   0.241   0.8097    \nfactor(state)49        4.3279     4.4262   0.978   0.3287    \nfactor(state)50            NA         NA      NA       NA    \nfactor(year)2010      -2.9238     1.9795  -1.477   0.1404    \nfactor(year)2011      -2.4424     1.9795  -1.234   0.2179    \nfactor(year)2012      -2.9507     1.9795  -1.491   0.1368    \nfactor(year)2013      -3.5011     1.9795  -1.769   0.0776 .  \nfactor(year)2014      -2.7767     1.9795  -1.403   0.1614    \nfactor(year)2015      -1.2810     1.9795  -0.647   0.5179    \nfactor(year)2016       0.3758     1.9795   0.190   0.8495    \nfactor(year)2017       2.8858     1.9795   1.458   0.1456    \nfactor(year)2018           NA         NA      NA       NA    \ntreated:placebo_post  -1.5655     1.9318  -0.810   0.4182    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.897 on 440 degrees of freedom\nMultiple R-squared:  0.09398,   Adjusted R-squared:  -0.0275 \nF-statistic: 0.7736 on 59 and 440 DF,  p-value: 0.8878\n\n# Visualizing results\nggplot(data, aes(x = year, y = employment, color = as.factor(treated))) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~treated) +\n  labs(title = \"Employment Trends Before and After Policy\",\n       x = \"Year\", y = \"Employment Level\",\n       color = \"Treatment Group\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nInterpretation: If the placebo treatment (2016) shows a significant effect, it suggests that trends or confounding factors, not the actual policy, might be driving results. If no effect is found, it supports the credibility of the causal effect estimated from the actual policy change."
  },
  {
    "objectID": "demos/Sensivity Test2.html#example-2-placebo-test-with-fake-treatment-groups",
    "href": "demos/Sensivity Test2.html#example-2-placebo-test-with-fake-treatment-groups",
    "title": "Sensitivity Tests",
    "section": "Example 2: Placebo Test with Fake Treatment Groups",
    "text": "Example 2: Placebo Test with Fake Treatment Groups\nThis placebo test ensures that the observed effect isn’t due to random chance by testing a fake, randomized treatment assignment.\nScenario: We analyze the effect of an education program on students’ test scores, but we randomly assign treatment as a placebo test.\nExpanded Explanation:\nA real education program is tested to see if it improves student test scores. We run a placebo test where treatment is randomly assigned to see if the model falsely detects an effect. If the placebo treatment appears significant, our original causal claim is likely biased.\nWhy Randomly Assigning the Treatment is a Placebo Test True Treatment vs. Fake Treatment: In a real study, a policy or intervention (e.g., an education program) is assigned to students based on specific criteria. In a placebo test, we replace the real assignment with a completely random assignment. Key Idea: If our model finds a significant effect for a randomly assigned treatment, it suggests our original results may be driven by spurious correlations or bias rather than a true causal effect. Expected Outcome: A valid causal model should show no significant effect for a placebo (randomly assigned) treatment. If the placebo treatment is significant, it suggests our main analysis may be biased (e.g., due to confounding, model errors, or random chance).\n\n# Simulate Data\nset.seed(456)\ndata &lt;- tibble(\n  student_id = 1:1000,\n  treated_real = sample(0:1, 1000, replace = TRUE),  # Real treatment\n  treated_placebo = sample(0:1, 1000, replace = TRUE),  # Fake treatment\n  pre_score = rnorm(1000, 50, 10),  # Pre-treatment test scores\n  post_score = pre_score + treated_real * 5 + rnorm(1000, 0, 3)  # Treatment adds 5 points on avg\n)\n\n# Real treatment effect\nmodel_real &lt;- lm(post_score ~ treated_real + pre_score, data = data)\nsummary(model_real)\n\n\nCall:\nlm(formula = post_score ~ treated_real + pre_score, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-10.149  -1.991   0.069   1.907  11.665 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.669861   0.478638    -1.4    0.162    \ntreated_real  5.192764   0.182868    28.4   &lt;2e-16 ***\npre_score     1.009045   0.009113   110.7   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.891 on 997 degrees of freedom\nMultiple R-squared:  0.9292,    Adjusted R-squared:  0.9291 \nF-statistic:  6543 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n# Placebo test: Fake treatment group\nmodel_placebo &lt;- lm(post_score ~ treated_placebo + pre_score, data = data)\nsummary(model_placebo)\n\n\nCall:\nlm(formula = post_score ~ treated_placebo + pre_score, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2109  -2.7614   0.0739   2.8359  12.2893 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      1.97996    0.64436   3.073  0.00218 ** \ntreated_placebo -0.26389    0.24592  -1.073  0.28350    \npre_score        1.00997    0.01225  82.449  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.886 on 997 degrees of freedom\nMultiple R-squared:  0.8721,    Adjusted R-squared:  0.8718 \nF-statistic:  3399 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n\n\n# Set seed for reproducibility\nset.seed(456)\n\n# Generate simulated data\ndata &lt;- tibble(\n  student_id = 1:1000,\n  treated_real = sample(0:1, 1000, replace = TRUE),  # True treatment assignment\n  treated_placebo = sample(0:1, 1000, replace = TRUE),  # Fake (random) treatment\n  pre_score = rnorm(1000, 50, 10),  # Pre-treatment test scores\n  post_score = pre_score + treated_real * 5 + rnorm(1000, 0, 3)  # True treatment adds 5 points\n)\n\ntreated_real: This represents the actual education program assignment. treated_placebo: This is a randomly assigned treatment (placebo). pre_score: Students’ scores before the program. post_score: Students’ scores after the program, where the real treatment increases scores by 5 points on average.\nStep 2: Run the Real Treatment Model\n\n# Analyze the effect of the real treatment\nmodel_real &lt;- lm(post_score ~ treated_real + pre_score, data = data)\nsummary(model_real)\n\n\nCall:\nlm(formula = post_score ~ treated_real + pre_score, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-10.149  -1.991   0.069   1.907  11.665 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.669861   0.478638    -1.4    0.162    \ntreated_real  5.192764   0.182868    28.4   &lt;2e-16 ***\npre_score     1.009045   0.009113   110.7   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.891 on 997 degrees of freedom\nMultiple R-squared:  0.9292,    Adjusted R-squared:  0.9291 \nF-statistic:  6543 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n\nThis regression estimates the effect of the real education program. We expect a positive, significant coefficient for treated_real, since the real program increases test scores.\nStep 3: Run the Placebo Test (Fake Treatment)\n\n# Analyze the effect of the fake (randomly assigned) treatment\nmodel_placebo &lt;- lm(post_score ~ treated_placebo + pre_score, data = data)\nsummary(model_placebo)\n\n\nCall:\nlm(formula = post_score ~ treated_placebo + pre_score, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2109  -2.7614   0.0739   2.8359  12.2893 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      1.97996    0.64436   3.073  0.00218 ** \ntreated_placebo -0.26389    0.24592  -1.073  0.28350    \npre_score        1.00997    0.01225  82.449  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.886 on 997 degrees of freedom\nMultiple R-squared:  0.8721,    Adjusted R-squared:  0.8718 \nF-statistic:  3399 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n\nIf the placebo treatment shows no effect, the original study is likely valid.\nIf the placebo treatment is significant, it suggests that our main analysis may be biased (e.g., by omitted variables, incorrect assumptions, or random noise).\nStep 4: Visualize Results We can plot test scores before and after the placebo treatment to see if any patterns emerge.\n\nggplot(data, aes(x = pre_score, y = post_score, color = as.factor(treated_placebo))) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Placebo Test: Fake Treatment vs. Test Scores\",\n       x = \"Pre-Test Score\", y = \"Post-Test Score\",\n       color = \"Fake Treatment Group\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nIf students who received the fake treatment have significantly different test scores, it suggests an issue in our model.\nIf the lines for placebo and non-placebo groups overlap, it means the fake treatment has no real effect, supporting the validity of our causal estimate.\nInterpretation:\nIf the placebo treatment (randomly assigned) is statistically significant, it suggests that the main analysis may be biased. If the placebo effect is insignificant, it strengthens the validity of the causal effect estimate."
  },
  {
    "objectID": "demos/Sensivity Test2.html#example-3-sensitivity-analysis-for-unmeasured-confounding-e-value-calculation",
    "href": "demos/Sensivity Test2.html#example-3-sensitivity-analysis-for-unmeasured-confounding-e-value-calculation",
    "title": "Sensitivity Tests",
    "section": "Example 3: Sensitivity Analysis for Unmeasured Confounding (E-value Calculation)",
    "text": "Example 3: Sensitivity Analysis for Unmeasured Confounding (E-value Calculation)\nThe E-value estimates how strong an unmeasured confounder would need to be to explain away the observed effect. We can use the E-value package.\nScenario: We estimate the effect of smoking on lung cancer and check how sensitive the result is to unmeasured confounding.\nThe E-value is a statistical tool used in causal inference to assess the robustness of an observed treatment effect to unmeasured confounding. It quantifies how strong an unmeasured confounder would have to be to fully explain away the observed association between treatment and outcome.\n\n1. Why Use E-Values?\nIn observational studies, treatment and control groups may differ due to unmeasured confounding (e.g., socioeconomic status, genetic factors, environment). E-values help answer: How strong must an unmeasured confounder be to reduce our estimated effect to zero (null effect)?\n\n\n2. How Do E-Values Work?\nSuppose we estimate the effect of smoking on lung cancer and find a risk ratio (RR) of 2.5. The E-value tells us how strong an unmeasured confounder must be (in terms of association with both treatment and outcome) to fully explain away the observed RR of 2.5. If the E-value is large (e.g., 5.0), an extremely strong confounder would be needed to invalidate the result. If the E-value is small (e.g., 1.5), even a weak confounder could explain away the effect. 3. Formula for E-Value Calculation\n\n\nFor a risk ratio (RR) or odds ratio (OR) estimate:\n\n\n4. Interpreting E-Values\nE-Value Interpretation High (e.g., &gt;4.0) ✅ Very strong confounding needed to explain away the result → The effect is robust. Moderate (e.g., 2.0 - 4.0) ⚠️ Some confounding could reduce the effect, but strong confounders are still needed. Low (e.g., &lt;2.0) ❌ A relatively weak confounder could fully explain away the observed effect → The effect is not robust.\n\n\nAssume an estimated risk ratio (RR) of 2.5 with a confidence interval (1.8, 3.2)\n\nhelp(package = \"EValue\")\nEValue::evalues.RR(2.5, lo = 1.8, hi = 3.2)\n\n            point lower upper\nRR       2.500000   1.8   3.2\nE-values 4.436492   3.0    NA\n\nEValue::evalues.RR(est = 2.5, lo = 1.8, hi = 3.2)\n\n            point lower upper\nRR       2.500000   1.8   3.2\nE-values 4.436492   3.0    NA\n\n\nOr do it manually\n\nRR &lt;- 2.5  # Estimated risk ratio\nE_value &lt;- RR + sqrt(RR * (RR - 1))\nE_value\n\n[1] 4.436492\n\n\n\n\nInterpretation:\nIf the E-value is high (e.g., 5+), an extremely strong unmeasured confounder would be needed to fully explain away the effect. If the E-value is low (e.g., 1.5), the result is more sensitive to hidden confounding."
  },
  {
    "objectID": "demos/Sensivity Test2.html#example-4-sensitivity-to-model-specification-robustness-checks",
    "href": "demos/Sensivity Test2.html#example-4-sensitivity-to-model-specification-robustness-checks",
    "title": "Sensitivity Tests",
    "section": "Example 4: Sensitivity to Model Specification (Robustness Checks)",
    "text": "Example 4: Sensitivity to Model Specification (Robustness Checks)\nWe check if the results change under different regression specifications.\nScenario: A study examines whether a new drug reduces blood pressure.\n\n# Simulated Data\nset.seed(789)\ndata &lt;- tibble(\n  patient_id = 1:500,\n  treated = sample(0:1, 500, replace = TRUE),\n  age = rnorm(500, 50, 10),\n  bmi = rnorm(500, 25, 4),\n  blood_pressure = 120 - treated * 5 + rnorm(500, 0, 5)  # Drug reduces BP by 5 units\n)\n\n# Model 1: Basic regression\nmodel1 &lt;- lm(blood_pressure ~ treated, data = data)\nsummary(model1)\n\n\nCall:\nlm(formula = blood_pressure ~ treated, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.4700  -3.1209  -0.1249   3.2771  13.6328 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 120.0594     0.3150  381.19   &lt;2e-16 ***\ntreated      -4.8644     0.4278  -11.37   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.766 on 498 degrees of freedom\nMultiple R-squared:  0.2061,    Adjusted R-squared:  0.2045 \nF-statistic: 129.3 on 1 and 498 DF,  p-value: &lt; 2.2e-16\n\n# Model 2: Controlling for age\nmodel2 &lt;- lm(blood_pressure ~ treated + age, data = data)\nsummary(model2)\n\n\nCall:\nlm(formula = blood_pressure ~ treated + age, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.3825  -3.2104  -0.1291   3.2562  13.4816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 119.5588     1.1514 103.834   &lt;2e-16 ***\ntreated      -4.8608     0.4282 -11.351   &lt;2e-16 ***\nage           0.0099     0.0219   0.452    0.651    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.77 on 497 degrees of freedom\nMultiple R-squared:  0.2064,    Adjusted R-squared:  0.2032 \nF-statistic: 64.64 on 2 and 497 DF,  p-value: &lt; 2.2e-16\n\n# Model 3: Controlling for age and BMI\nmodel3 &lt;- lm(blood_pressure ~ treated + age + bmi, data = data)\nsummary(model3)\n\n\nCall:\nlm(formula = blood_pressure ~ treated + age + bmi, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.3533  -3.1962  -0.1751   3.2654  13.4810 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 119.255805   1.674354  71.225   &lt;2e-16 ***\ntreated      -4.865551   0.429052 -11.340   &lt;2e-16 ***\nage           0.009542   0.021968   0.434    0.664    \nbmi           0.013045   0.052294   0.249    0.803    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.774 on 496 degrees of freedom\nMultiple R-squared:  0.2065,    Adjusted R-squared:  0.2017 \nF-statistic: 43.04 on 3 and 496 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretation:\n\nIf the estimated effect remains stable across models, it suggests robustness.\nIf the effect changes dramatically, it indicates sensitivity to model specification (potential omitted variable bias)."
  },
  {
    "objectID": "demos/Sensivity Test2.html#example-5-rosenbaum-bounds-for-sensitivity-to-unmeasured-confounding",
    "href": "demos/Sensivity Test2.html#example-5-rosenbaum-bounds-for-sensitivity-to-unmeasured-confounding",
    "title": "Sensitivity Tests",
    "section": "Example 5: Rosenbaum Bounds for Sensitivity to Unmeasured Confounding",
    "text": "Example 5: Rosenbaum Bounds for Sensitivity to Unmeasured Confounding\nWe test how strong an unmeasured confounder must be to invalidate our results using the rbounds package.\nScenario: We estimate the effect of a scholarship program on college enrollment and check for unmeasured confounding.\n\n# Simulated matched dataset (outcome: enrollment, treatment: scholarship)\nset.seed(111)\nscholarship &lt;- sample(0:1, 500, replace = TRUE)\nenrollment &lt;- scholarship + rnorm(500, 0, 1)\n\n# Wilcoxon signed-rank test for matching\npsens(enrollment, scholarship, Gamma = 2)  # Gamma = strength of confounding\n\n\n Rosenbaum Sensitivity Test for Wilcoxon Signed Rank P-Value \n \nUnconfounded estimate ....  0.5866 \n\n Gamma Lower bound Upper bound\n     1      0.5866      0.5866\n     2      0.0000      1.0000\n\n Note: Gamma is Odds of Differential Assignment To\n Treatment Due to Unobserved Factors \n \n\n\nInterpretation: If the effect remains significant for large values of Gamma (e.g., 2+), the results are robust. If the effect disappears for small values of Gamma (&lt;1.2), the findings are highly sensitive to hidden bias."
  },
  {
    "objectID": "demos/Sensivity Test2.html#example-x-sensitivity-analysis-using-the-sensemakr-package-in-r",
    "href": "demos/Sensivity Test2.html#example-x-sensitivity-analysis-using-the-sensemakr-package-in-r",
    "title": "Sensitivity Tests",
    "section": "Example X: Sensitivity Analysis Using the sensemakr Package in R",
    "text": "Example X: Sensitivity Analysis Using the sensemakr Package in R\nThe sensemakr package is a powerful tool for sensitivity analysis in causal inference. It helps assess how unmeasured confounders might impact estimated treatment effects.\n📌 Step 2: Simulate a Dataset We create a dataset where:\nwages: Worker’s wage after training training: Binary variable (1 = received job training, 0 = no training) education: Years of education (a potential confounder) experience: Years of work experience (another potential confounder)\nHere, we assume the true treatment effect of training on wages is $3 per hour.\n📌 Step 3: Run the Initial Regression We estimate the impact of job training on wages, controlling for education and experience.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Simulate data: Job training impact on wages\nn &lt;- 500\ntraining &lt;- rbinom(n, 1, 0.5)  # 50% received training\neducation &lt;- rnorm(n, mean = 12, sd = 2)  # Avg. 12 years of education\nexperience &lt;- rnorm(n, mean = 5, sd = 2)  # Avg. 5 years of experience\nwages &lt;- 20 + 3 * training + 2 * education + 1.5 * experience + rnorm(n, sd = 5)  # Wage equation\n\n# Create data frame\ndata &lt;- data.frame(wages, training, education, experience)\n\n# Fit linear regression model\nmodel &lt;- lm(wages ~ training + education + experience, data = data)\n\n# Display model summary\nsummary(model)\n\n\nCall:\nlm(formula = wages ~ training + education + experience, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.6127  -3.1967   0.0747   3.4101  17.0122 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  18.5307     1.4648  12.651  &lt; 2e-16 ***\ntraining      3.4949     0.4445   7.862 2.38e-14 ***\neducation     2.2004     0.1105  19.921  &lt; 2e-16 ***\nexperience    1.3084     0.1091  11.989  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.952 on 496 degrees of freedom\nMultiple R-squared:  0.5555,    Adjusted R-squared:  0.5528 \nF-statistic: 206.6 on 3 and 496 DF,  p-value: &lt; 2.2e-16\n\n\nNow, check the sensitivity:\n📌 Step 4: Conduct Sensitivity Analysis with sensemakr We now check if an unmeasured confounder could explain away the observed effect of training.\n\n# Perform sensitivity analysis with sensemakr\nsensitivity &lt;- sensemakr(\n  model = model,\n  treatment = \"training\",\n  benchmark_covariates = c(\"education\", \"experience\"),  # Use observed confounders as benchmarks\n  q = 1,  # Assess robustness to omitted variable bias\n  alpha = 0.05  # Significance level\n)\n\n# Print sensitivity analysis summary\nsummary(sensitivity)\n\nSensitivity Analysis to Unobserved Confounding\n\nModel Formula: wages ~ training + education + experience\n\nNull hypothesis: q = 1 and reduce = TRUE \n-- This means we are considering biases that reduce the absolute value of the current estimate.\n-- The null hypothesis deemed problematic is H0:tau = 0 \n\nUnadjusted Estimates of 'training': \n  Coef. estimate: 3.4949 \n  Standard Error: 0.4445 \n  t-value (H0:tau = 0): 7.8616 \n\nSensitivity Statistics:\n  Partial R2 of treatment with outcome: 0.1108 \n  Robustness Value, q = 1: 0.2961 \n  Robustness Value, q = 1, alpha = 0.05: 0.232 \n\nVerbal interpretation of sensitivity statistics:\n\n-- Partial R2 of the treatment with the outcome: an extreme confounder (orthogonal to the covariates) that explains 100% of the residual variance of the outcome, would need to explain at least 11.08% of the residual variance of the treatment to fully account for the observed estimated effect.\n\n-- Robustness Value, q = 1: unobserved confounders (orthogonal to the covariates) that explain more than 29.61% of the residual variance of both the treatment and the outcome are strong enough to bring the point estimate to 0 (a bias of 100% of the original estimate). Conversely, unobserved confounders that do not explain more than 29.61% of the residual variance of both the treatment and the outcome are not strong enough to bring the point estimate to 0.\n\n-- Robustness Value, q = 1, alpha = 0.05: unobserved confounders (orthogonal to the covariates) that explain more than 23.2% of the residual variance of both the treatment and the outcome are strong enough to bring the estimate to a range where it is no longer 'statistically different' from 0 (a bias of 100% of the original estimate), at the significance level of alpha = 0.05. Conversely, unobserved confounders that do not explain more than 23.2% of the residual variance of both the treatment and the outcome are not strong enough to bring the estimate to a range where it is no longer 'statistically different' from 0, at the significance level of alpha = 0.05.\n\nBounds on omitted variable bias:\n\n--The table below shows the maximum strength of unobserved confounders with association with the treatment and the outcome bounded by a multiple of the observed explanatory power of the chosen benchmark covariate(s).\n\n   Bound Label R2dz.x R2yz.dx Treatment Adjusted Estimate Adjusted Se\n  1x education 0.0037  0.8061  training            2.9529      0.1963\n 1x experience 0.0001  0.2898  training            3.4460      0.3750\n Adjusted T Adjusted Lower CI Adjusted Upper CI\n    15.0403            2.5671            3.3386\n     9.1888            2.7092            4.1828\n\n\n📌 Step 5: Visualize Sensitivity Results sensemakr provides a contour plot that shows how strong an unmeasured confounder would have to be to invalidate our conclusions.\n🔍 How to Interpret the Plot:\nThe x-axis represents the strength of an unmeasured confounder’s association with treatment (job training). The y-axis represents its effect on the outcome (wages). If the observed effect remains above the significance threshold across plausible values, the result is robust.\n\n# Plot sensitivity results\nplot(sensitivity)\n\n\n\n\n\n\n\n\nStep 6: Interpret the Sensitivity Analysis Results The summary output will show:\nR-squared of omitted variable needed to fully explain the effect How much the observed effect reduces under different confounding scenarios Whether the treatment effect remains significant under potential omitted variables If the sensitivity analysis shows that an extremely strong confounder is needed to nullify the effect, we can be confident in our causal estimate. If a moderate confounder could eliminate the effect, we should be cautious in our conclusions."
  },
  {
    "objectID": "demos/Sensivity Test2.html#introduction",
    "href": "demos/Sensivity Test2.html#introduction",
    "title": "Sensitivity Tests",
    "section": "Introduction",
    "text": "Introduction\nThis document demonstrates how to perform Rosenbaum Bounds Sensitivity Analysis using R, applied to an ecological study assessing the impact of deforestation on bird species richness."
  },
  {
    "objectID": "demos/Sensivity Test2.html#load-required-libraries",
    "href": "demos/Sensivity Test2.html#load-required-libraries",
    "title": "Sensitivity Tests",
    "section": "Load Required Libraries",
    "text": "Load Required Libraries\n\n# Install packages if not already installed\nif (!require(MatchIt)) install.packages(\"MatchIt\", dependencies = TRUE)\nif (!require(rbounds)) install.packages(\"rbounds\", dependencies = TRUE)\n\n# Load the libraries\nlibrary(MatchIt)\nlibrary(rbounds)"
  },
  {
    "objectID": "demos/Sensivity Test2.html#simulate-ecological-data",
    "href": "demos/Sensivity Test2.html#simulate-ecological-data",
    "title": "Sensitivity Tests",
    "section": "Simulate Ecological Data",
    "text": "Simulate Ecological Data\nWe create a dataset where bird species richness is compared between deforested and non-deforested sites while controlling for elevation and precipitation.\n\nset.seed(123)\ndata &lt;- data.frame(\n  deforestation = rep(c(1, 0), each = 50),\n  species_richness = c(rnorm(50, mean = 10, sd = 3), rnorm(50, mean = 15, sd = 3)),\n  elevation = rnorm(100, mean = 500, sd = 100),\n  precipitation = rnorm(100, mean = 1000, sd = 200)\n)"
  },
  {
    "objectID": "demos/Sensivity Test2.html#perform-propensity-score-matching",
    "href": "demos/Sensivity Test2.html#perform-propensity-score-matching",
    "title": "Sensitivity Tests",
    "section": "Perform Propensity Score Matching",
    "text": "Perform Propensity Score Matching\nWe match deforested and non-deforested sites based on elevation and precipitation.\n\nmatch_model &lt;- matchit(deforestation ~ elevation + precipitation, data = data, method = \"nearest\")\nmatched_data &lt;- match.data(match_model)"
  },
  {
    "objectID": "demos/Sensivity Test2.html#conduct-sensitivity-analysis-with-rosenbaum-bounds",
    "href": "demos/Sensivity Test2.html#conduct-sensitivity-analysis-with-rosenbaum-bounds",
    "title": "Sensitivity Tests",
    "section": "Conduct Sensitivity Analysis with Rosenbaum Bounds",
    "text": "Conduct Sensitivity Analysis with Rosenbaum Bounds\nWe check how sensitive our results are to hidden bias.\n\n# Extract matched treatment and control outcomes\ntreated_outcomes &lt;- matched_data$species_richness[matched_data$deforestation == 1]\ncontrol_outcomes &lt;- matched_data$species_richness[matched_data$deforestation == 0]\n\n# Perform Rosenbaum sensitivity analysis\npsens(x = treated_outcomes, y = control_outcomes, Gamma = 1.5)  # Adjust Gamma to test sensitivity\n\n\n Rosenbaum Sensitivity Test for Wilcoxon Signed Rank P-Value \n \nUnconfounded estimate ....  1 \n\n Gamma Lower bound Upper bound\n     1           1           1\n\n Note: Gamma is Odds of Differential Assignment To\n Treatment Due to Unobserved Factors"
  },
  {
    "objectID": "demos/Sensivity Test2.html#interpretation-of-results",
    "href": "demos/Sensivity Test2.html#interpretation-of-results",
    "title": "Sensitivity Tests",
    "section": "Interpretation of Results",
    "text": "Interpretation of Results\n\nIf the p-value remains small at Γ = 1.5, our conclusions are robust to moderate hidden bias.\nIf the p-value becomes insignificant at lower values of Γ, our study is highly sensitive to hidden confounders."
  },
  {
    "objectID": "demos/Sensivity Test2.html#conclusion",
    "href": "demos/Sensivity Test2.html#conclusion",
    "title": "Sensitivity Tests",
    "section": "Conclusion",
    "text": "Conclusion\nRosenbaum bounds allow us to assess the robustness of our ecological study results against unmeasured confounding. This is particularly useful when controlled experiments are infeasible.\n\nTake Aways\nRosenbaum bounds provide a way to assess how much an unmeasured confounder could influence the causal relationship between deforestation and bird species richness. In ecological studies, where controlled experiments are often impractical, this method helps gauge the reliability of observational findings."
  },
  {
    "objectID": "readings.html#due-mon.-414-mediation-analysis-sarah-elizabeth",
    "href": "readings.html#due-mon.-414-mediation-analysis-sarah-elizabeth",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Mon. 4/14 Mediation Analysis [Sarah Elizabeth]",
    "text": "Due Mon. 4/14 Mediation Analysis [Sarah Elizabeth]",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "exercises/Sensivity Test2.html",
    "href": "exercises/Sensivity Test2.html",
    "title": "Sensitivity Tests",
    "section": "",
    "text": "✅ Placebo tests ensure the estimated effect is not driven by spurious correlations. ✅ Sensitivity analyses check if conclusions remain valid under different assumptions. ✅ R provides powerful tools (e.g., EValue, sensemakr, rbounds, placebo regressions) for assessing robustness in causal inference.\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nLoading required package: zoo\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nSee details in:\n\nCarlos Cinelli and Chad Hazlett (2020). Making Sense of Sensitivity: Extending Omitted Variable Bias. Journal of the Royal Statistical Society, Series B (Statistical Methodology)."
  },
  {
    "objectID": "exercises/Sensivity Test2.html#example-1-placebo-test-with-pre-treatment-outcomes",
    "href": "exercises/Sensivity Test2.html#example-1-placebo-test-with-pre-treatment-outcomes",
    "title": "Sensitivity Tests",
    "section": "Example 1: Placebo Test with Pre-Treatment Outcomes",
    "text": "Example 1: Placebo Test with Pre-Treatment Outcomes\nWe check if a policy or treatment has an effect before it was implemented. If significant effects are found before the actual intervention, the causal claim is questionable.\nScenario Examples: A state implements a minimum wage increase in 2018, and we estimate its effect on employment. A placebo test checks if a pre-2018 placebo treatment also shows an effect.\n\n\nCode\n# Simulate data\nset.seed(123)\ndata &lt;- tibble(\n  state = rep(1:50, each = 10),\n  year = rep(2009:2018, times = 50),\n  treated = ifelse(state &lt;= 25, 1, 0), # 25 treated states\n  post_treatment = ifelse(year &gt;= 2018, 1, 0), # Policy starts in 2018\n  employment = rnorm(500, mean = 100, sd = 10) - 2 * treated * post_treatment # Simulated policy effect\n)\n\n# Run regression for actual treatment\nmodel_actual &lt;- lm(employment ~ treated * post_treatment + factor(state) + factor(year), data = data)\nsummary(model_actual)\n\n\n\nCall:\nlm(formula = employment ~ treated * post_treatment + factor(state) + \n    factor(year), data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-28.7438  -6.0938  -0.2846   6.0033  29.7668 \n\nCoefficients: (2 not defined because of singularities)\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            103.0010     3.3976  30.315   &lt;2e-16 ***\ntreated                  0.4352     4.4290   0.098   0.9218    \npost_treatment          -2.6077     2.4649  -1.058   0.2907    \nfactor(state)2           1.3400     4.4192   0.303   0.7619    \nfactor(state)3          -4.9918     4.4192  -1.130   0.2593    \nfactor(state)4           2.4742     4.4192   0.560   0.5759    \nfactor(state)5          -0.8334     4.4192  -0.189   0.8505    \nfactor(state)6           1.4706     4.4192   0.333   0.7395    \nfactor(state)7           0.4846     4.4192   0.110   0.9127    \nfactor(state)8          -4.3754     4.4192  -0.990   0.3227    \nfactor(state)9           2.3847     4.4192   0.540   0.5897    \nfactor(state)10          3.6247     4.4192   0.820   0.4125    \nfactor(state)11         -4.7047     4.4192  -1.065   0.2876    \nfactor(state)12         -3.9754     4.4192  -0.900   0.3688    \nfactor(state)13         -1.8506     4.4192  -0.419   0.6756    \nfactor(state)14         -0.4089     4.4192  -0.093   0.9263    \nfactor(state)15         -5.4867     4.4192  -1.242   0.2151    \nfactor(state)16          0.5274     4.4192   0.119   0.9050    \nfactor(state)17          2.1581     4.4192   0.488   0.6255    \nfactor(state)18         -0.3129     4.4192  -0.071   0.9436    \nfactor(state)19         -0.7456     4.4192  -0.169   0.8661    \nfactor(state)20         -3.4181     4.4192  -0.773   0.4397    \nfactor(state)21          2.3661     4.4192   0.535   0.5926    \nfactor(state)22         -2.6689     4.4192  -0.604   0.5462    \nfactor(state)23         -0.7415     4.4192  -0.168   0.8668    \nfactor(state)24         -1.5179     4.4192  -0.343   0.7314    \nfactor(state)25         -1.5951     4.4192  -0.361   0.7183    \nfactor(state)26         -0.1501     4.4192  -0.034   0.9729    \nfactor(state)27          1.4536     4.4192   0.329   0.7424    \nfactor(state)28          1.6677     4.4192   0.377   0.7061    \nfactor(state)29          0.7183     4.4192   0.163   0.8710    \nfactor(state)30          6.1122     4.4192   1.383   0.1673    \nfactor(state)31         -2.6271     4.4192  -0.594   0.5525    \nfactor(state)32          0.7785     4.4192   0.176   0.8602    \nfactor(state)33          2.3192     4.4192   0.525   0.6000    \nfactor(state)34         -2.3832     4.4192  -0.539   0.5900    \nfactor(state)35         -0.6556     4.4192  -0.148   0.8821    \nfactor(state)36         -1.8980     4.4192  -0.429   0.6678    \nfactor(state)37          1.6467     4.4192   0.373   0.7096    \nfactor(state)38         -2.1433     4.4192  -0.485   0.6279    \nfactor(state)39         -0.1039     4.4192  -0.024   0.9813    \nfactor(state)40         -3.8974     4.4192  -0.882   0.3783    \nfactor(state)41         -3.3247     4.4192  -0.752   0.4523    \nfactor(state)42         -1.9829     4.4192  -0.449   0.6539    \nfactor(state)43          4.3652     4.4192   0.988   0.3238    \nfactor(state)44         -1.3018     4.4192  -0.295   0.7684    \nfactor(state)45         -3.7068     4.4192  -0.839   0.4020    \nfactor(state)46          1.9103     4.4192   0.432   0.6657    \nfactor(state)47          3.8897     4.4192   0.880   0.3792    \nfactor(state)48          1.0666     4.4192   0.241   0.8094    \nfactor(state)49          4.3279     4.4192   0.979   0.3280    \nfactor(state)50              NA         NA      NA       NA    \nfactor(year)2010        -2.9238     1.9763  -1.479   0.1397    \nfactor(year)2011        -2.4424     1.9763  -1.236   0.2172    \nfactor(year)2012        -2.9507     1.9763  -1.493   0.1361    \nfactor(year)2013        -3.5011     1.9763  -1.772   0.0772 .  \nfactor(year)2014        -2.7767     1.9763  -1.405   0.1607    \nfactor(year)2015        -1.2810     1.9763  -0.648   0.5172    \nfactor(year)2016        -4.3472     1.9763  -2.200   0.0284 *  \nfactor(year)2017        -1.8373     1.9763  -0.930   0.3531    \nfactor(year)2018             NA         NA      NA       NA    \ntreated:post_treatment  -4.2308     2.9461  -1.436   0.1517    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.882 on 440 degrees of freedom\nMultiple R-squared:  0.09686,   Adjusted R-squared:  -0.02424 \nF-statistic: 0.7999 on 59 and 440 DF,  p-value: 0.8544\n\n\nCode\n# Placebo Test: Using 2016 as the fake treatment year\ndata$placebo_post &lt;- ifelse(data$year &gt;= 2016, 1, 0)\nmodel_placebo &lt;- lm(employment ~ treated * placebo_post + factor(state) + factor(year), data = data)\nsummary(model_placebo)\n\n\n\nCall:\nlm(formula = employment ~ treated * placebo_post + factor(state) + \n    factor(year), data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-28.7205  -6.1867  -0.1883   6.1179  29.7436 \n\nCoefficients: (2 not defined because of singularities)\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          102.9777     3.4122  30.179   &lt;2e-16 ***\ntreated                0.4817     4.4640   0.108   0.9141    \nplacebo_post          -3.9403     2.2026  -1.789   0.0743 .  \nfactor(state)2         1.3400     4.4262   0.303   0.7622    \nfactor(state)3        -4.9918     4.4262  -1.128   0.2600    \nfactor(state)4         2.4742     4.4262   0.559   0.5765    \nfactor(state)5        -0.8334     4.4262  -0.188   0.8507    \nfactor(state)6         1.4706     4.4262   0.332   0.7399    \nfactor(state)7         0.4846     4.4262   0.109   0.9129    \nfactor(state)8        -4.3754     4.4262  -0.989   0.3234    \nfactor(state)9         2.3847     4.4262   0.539   0.5903    \nfactor(state)10        3.6247     4.4262   0.819   0.4133    \nfactor(state)11       -4.7047     4.4262  -1.063   0.2884    \nfactor(state)12       -3.9754     4.4262  -0.898   0.3696    \nfactor(state)13       -1.8506     4.4262  -0.418   0.6761    \nfactor(state)14       -0.4089     4.4262  -0.092   0.9264    \nfactor(state)15       -5.4867     4.4262  -1.240   0.2158    \nfactor(state)16        0.5274     4.4262   0.119   0.9052    \nfactor(state)17        2.1581     4.4262   0.488   0.6261    \nfactor(state)18       -0.3129     4.4262  -0.071   0.9437    \nfactor(state)19       -0.7456     4.4262  -0.168   0.8663    \nfactor(state)20       -3.4181     4.4262  -0.772   0.4404    \nfactor(state)21        2.3661     4.4262   0.535   0.5932    \nfactor(state)22       -2.6689     4.4262  -0.603   0.5468    \nfactor(state)23       -0.7415     4.4262  -0.168   0.8670    \nfactor(state)24       -1.5179     4.4262  -0.343   0.7318    \nfactor(state)25       -1.5951     4.4262  -0.360   0.7187    \nfactor(state)26       -0.1501     4.4262  -0.034   0.9730    \nfactor(state)27        1.4536     4.4262   0.328   0.7428    \nfactor(state)28        1.6677     4.4262   0.377   0.7065    \nfactor(state)29        0.7183     4.4262   0.162   0.8712    \nfactor(state)30        6.1122     4.4262   1.381   0.1680    \nfactor(state)31       -2.6271     4.4262  -0.594   0.5531    \nfactor(state)32        0.7785     4.4262   0.176   0.8605    \nfactor(state)33        2.3192     4.4262   0.524   0.6006    \nfactor(state)34       -2.3832     4.4262  -0.538   0.5906    \nfactor(state)35       -0.6556     4.4262  -0.148   0.8823    \nfactor(state)36       -1.8980     4.4262  -0.429   0.6683    \nfactor(state)37        1.6467     4.4262   0.372   0.7101    \nfactor(state)38       -2.1433     4.4262  -0.484   0.6285    \nfactor(state)39       -0.1039     4.4262  -0.023   0.9813    \nfactor(state)40       -3.8974     4.4262  -0.881   0.3791    \nfactor(state)41       -3.3247     4.4262  -0.751   0.4530    \nfactor(state)42       -1.9829     4.4262  -0.448   0.6544    \nfactor(state)43        4.3652     4.4262   0.986   0.3246    \nfactor(state)44       -1.3018     4.4262  -0.294   0.7688    \nfactor(state)45       -3.7068     4.4262  -0.837   0.4028    \nfactor(state)46        1.9103     4.4262   0.432   0.6662    \nfactor(state)47        3.8897     4.4262   0.879   0.3800    \nfactor(state)48        1.0666     4.4262   0.241   0.8097    \nfactor(state)49        4.3279     4.4262   0.978   0.3287    \nfactor(state)50            NA         NA      NA       NA    \nfactor(year)2010      -2.9238     1.9795  -1.477   0.1404    \nfactor(year)2011      -2.4424     1.9795  -1.234   0.2179    \nfactor(year)2012      -2.9507     1.9795  -1.491   0.1368    \nfactor(year)2013      -3.5011     1.9795  -1.769   0.0776 .  \nfactor(year)2014      -2.7767     1.9795  -1.403   0.1614    \nfactor(year)2015      -1.2810     1.9795  -0.647   0.5179    \nfactor(year)2016       0.3758     1.9795   0.190   0.8495    \nfactor(year)2017       2.8858     1.9795   1.458   0.1456    \nfactor(year)2018           NA         NA      NA       NA    \ntreated:placebo_post  -1.5655     1.9318  -0.810   0.4182    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.897 on 440 degrees of freedom\nMultiple R-squared:  0.09398,   Adjusted R-squared:  -0.0275 \nF-statistic: 0.7736 on 59 and 440 DF,  p-value: 0.8878\n\n\nCode\n# Visualizing results\nggplot(data, aes(x = year, y = employment, color = as.factor(treated))) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~treated) +\n  labs(title = \"Employment Trends Before and After Policy\",\n       x = \"Year\", y = \"Employment Level\",\n       color = \"Treatment Group\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nInterpretation: If the placebo treatment (2016) shows a significant effect, it suggests that trends or confounding factors, not the actual policy, might be driving results. If no effect is found, it supports the credibility of the causal effect estimated from the actual policy change."
  },
  {
    "objectID": "exercises/Sensivity Test2.html#example-2-placebo-test-with-fake-treatment-groups",
    "href": "exercises/Sensivity Test2.html#example-2-placebo-test-with-fake-treatment-groups",
    "title": "Sensitivity Tests",
    "section": "Example 2: Placebo Test with Fake Treatment Groups",
    "text": "Example 2: Placebo Test with Fake Treatment Groups\nThis placebo test ensures that the observed effect isn’t due to random chance by testing a fake, randomized treatment assignment.\nScenario: We analyze the effect of an education program on students’ test scores, but we randomly assign treatment as a placebo test.\nExpanded Explanation:\nA real education program is tested to see if it improves student test scores. We run a placebo test where treatment is randomly assigned to see if the model falsely detects an effect. If the placebo treatment appears significant, our original causal claim is likely biased.\nWhy Randomly Assigning the Treatment is a Placebo Test True Treatment vs. Fake Treatment: In a real study, a policy or intervention (e.g., an education program) is assigned to students based on specific criteria. In a placebo test, we replace the real assignment with a completely random assignment. Key Idea: If our model finds a significant effect for a randomly assigned treatment, it suggests our original results may be driven by spurious correlations or bias rather than a true causal effect. Expected Outcome: A valid causal model should show no significant effect for a placebo (randomly assigned) treatment. If the placebo treatment is significant, it suggests our main analysis may be biased (e.g., due to confounding, model errors, or random chance).\n\n\nCode\n# Simulate Data\nset.seed(456)\ndata &lt;- tibble(\n  student_id = 1:1000,\n  treated_real = sample(0:1, 1000, replace = TRUE),  # Real treatment\n  treated_placebo = sample(0:1, 1000, replace = TRUE),  # Fake treatment\n  pre_score = rnorm(1000, 50, 10),  # Pre-treatment test scores\n  post_score = pre_score + treated_real * 5 + rnorm(1000, 0, 3)  # Treatment adds 5 points on avg\n)\n\n# Real treatment effect\nmodel_real &lt;- lm(post_score ~ treated_real + pre_score, data = data)\nsummary(model_real)\n\n\n\nCall:\nlm(formula = post_score ~ treated_real + pre_score, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-10.149  -1.991   0.069   1.907  11.665 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.669861   0.478638    -1.4    0.162    \ntreated_real  5.192764   0.182868    28.4   &lt;2e-16 ***\npre_score     1.009045   0.009113   110.7   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.891 on 997 degrees of freedom\nMultiple R-squared:  0.9292,    Adjusted R-squared:  0.9291 \nF-statistic:  6543 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Placebo test: Fake treatment group\nmodel_placebo &lt;- lm(post_score ~ treated_placebo + pre_score, data = data)\nsummary(model_placebo)\n\n\n\nCall:\nlm(formula = post_score ~ treated_placebo + pre_score, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2109  -2.7614   0.0739   2.8359  12.2893 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      1.97996    0.64436   3.073  0.00218 ** \ntreated_placebo -0.26389    0.24592  -1.073  0.28350    \npre_score        1.00997    0.01225  82.449  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.886 on 997 degrees of freedom\nMultiple R-squared:  0.8721,    Adjusted R-squared:  0.8718 \nF-statistic:  3399 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\n# Set seed for reproducibility\nset.seed(456)\n\n# Generate simulated data\ndata &lt;- tibble(\n  student_id = 1:1000,\n  treated_real = sample(0:1, 1000, replace = TRUE),  # True treatment assignment\n  treated_placebo = sample(0:1, 1000, replace = TRUE),  # Fake (random) treatment\n  pre_score = rnorm(1000, 50, 10),  # Pre-treatment test scores\n  post_score = pre_score + treated_real * 5 + rnorm(1000, 0, 3)  # True treatment adds 5 points\n)\n\n\ntreated_real: This represents the actual education program assignment. treated_placebo: This is a randomly assigned treatment (placebo). pre_score: Students’ scores before the program. post_score: Students’ scores after the program, where the real treatment increases scores by 5 points on average.\nStep 2: Run the Real Treatment Model\n\n\nCode\n# Analyze the effect of the real treatment\nmodel_real &lt;- lm(post_score ~ treated_real + pre_score, data = data)\nsummary(model_real)\n\n\n\nCall:\nlm(formula = post_score ~ treated_real + pre_score, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-10.149  -1.991   0.069   1.907  11.665 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.669861   0.478638    -1.4    0.162    \ntreated_real  5.192764   0.182868    28.4   &lt;2e-16 ***\npre_score     1.009045   0.009113   110.7   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.891 on 997 degrees of freedom\nMultiple R-squared:  0.9292,    Adjusted R-squared:  0.9291 \nF-statistic:  6543 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n\nThis regression estimates the effect of the real education program. We expect a positive, significant coefficient for treated_real, since the real program increases test scores.\nStep 3: Run the Placebo Test (Fake Treatment)\n\n\nCode\n# Analyze the effect of the fake (randomly assigned) treatment\nmodel_placebo &lt;- lm(post_score ~ treated_placebo + pre_score, data = data)\nsummary(model_placebo)\n\n\n\nCall:\nlm(formula = post_score ~ treated_placebo + pre_score, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2109  -2.7614   0.0739   2.8359  12.2893 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      1.97996    0.64436   3.073  0.00218 ** \ntreated_placebo -0.26389    0.24592  -1.073  0.28350    \npre_score        1.00997    0.01225  82.449  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.886 on 997 degrees of freedom\nMultiple R-squared:  0.8721,    Adjusted R-squared:  0.8718 \nF-statistic:  3399 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n\nIf the placebo treatment shows no effect, the original study is likely valid.\nIf the placebo treatment is significant, it suggests that our main analysis may be biased (e.g., by omitted variables, incorrect assumptions, or random noise).\nStep 4: Visualize Results We can plot test scores before and after the placebo treatment to see if any patterns emerge.\n\n\nCode\nggplot(data, aes(x = pre_score, y = post_score, color = as.factor(treated_placebo))) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Placebo Test: Fake Treatment vs. Test Scores\",\n       x = \"Pre-Test Score\", y = \"Post-Test Score\",\n       color = \"Fake Treatment Group\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nIf students who received the fake treatment have significantly different test scores, it suggests an issue in our model.\nIf the lines for placebo and non-placebo groups overlap, it means the fake treatment has no real effect, supporting the validity of our causal estimate.\nInterpretation:\nIf the placebo treatment (randomly assigned) is statistically significant, it suggests that the main analysis may be biased. If the placebo effect is insignificant, it strengthens the validity of the causal effect estimate."
  },
  {
    "objectID": "exercises/Sensivity Test2.html#example-3-sensitivity-analysis-for-unmeasured-confounding-e-value-calculation",
    "href": "exercises/Sensivity Test2.html#example-3-sensitivity-analysis-for-unmeasured-confounding-e-value-calculation",
    "title": "Sensitivity Tests",
    "section": "Example 3: Sensitivity Analysis for Unmeasured Confounding (E-value Calculation)",
    "text": "Example 3: Sensitivity Analysis for Unmeasured Confounding (E-value Calculation)\nThe E-value estimates how strong an unmeasured confounder would need to be to explain away the observed effect. We can use the E-value package.\nScenario: We estimate the effect of smoking on lung cancer and check how sensitive the result is to unmeasured confounding.\nThe E-value is a statistical tool used in causal inference to assess the robustness of an observed treatment effect to unmeasured confounding. It quantifies how strong an unmeasured confounder would have to be to fully explain away the observed association between treatment and outcome.\n\n1. Why Use E-Values?\nIn observational studies, treatment and control groups may differ due to unmeasured confounding (e.g., socioeconomic status, genetic factors, environment). E-values help answer: How strong must an unmeasured confounder be to reduce our estimated effect to zero (null effect)?\n\n\n2. How Do E-Values Work?\nSuppose we estimate the effect of smoking on lung cancer and find a risk ratio (RR) of 2.5. The E-value tells us how strong an unmeasured confounder must be (in terms of association with both treatment and outcome) to fully explain away the observed RR of 2.5. If the E-value is large (e.g., 5.0), an extremely strong confounder would be needed to invalidate the result. If the E-value is small (e.g., 1.5), even a weak confounder could explain away the effect. 3. Formula for E-Value Calculation\n\n\nFor a risk ratio (RR) or odds ratio (OR) estimate:\n\n\n4. Interpreting E-Values\nE-Value Interpretation High (e.g., &gt;4.0) ✅ Very strong confounding needed to explain away the result → The effect is robust. Moderate (e.g., 2.0 - 4.0) ⚠️ Some confounding could reduce the effect, but strong confounders are still needed. Low (e.g., &lt;2.0) ❌ A relatively weak confounder could fully explain away the observed effect → The effect is not robust.\n\n\nAssume an estimated risk ratio (RR) of 2.5 with a confidence interval (1.8, 3.2)\n\n\nCode\nhelp(package = \"EValue\")\nEValue::evalues.RR(2.5, lo = 1.8, hi = 3.2)\n\n\n            point lower upper\nRR       2.500000   1.8   3.2\nE-values 4.436492   3.0    NA\n\n\nCode\nEValue::evalues.RR(est = 2.5, lo = 1.8, hi = 3.2)\n\n\n            point lower upper\nRR       2.500000   1.8   3.2\nE-values 4.436492   3.0    NA\n\n\nOr do it manually\n\n\nCode\nRR &lt;- 2.5  # Estimated risk ratio\nE_value &lt;- RR + sqrt(RR * (RR - 1))\nE_value\n\n\n[1] 4.436492\n\n\n\n\nInterpretation:\nIf the E-value is high (e.g., 5+), an extremely strong unmeasured confounder would be needed to fully explain away the effect. If the E-value is low (e.g., 1.5), the result is more sensitive to hidden confounding."
  },
  {
    "objectID": "exercises/Sensivity Test2.html#example-4-sensitivity-to-model-specification-robustness-checks",
    "href": "exercises/Sensivity Test2.html#example-4-sensitivity-to-model-specification-robustness-checks",
    "title": "Sensitivity Tests",
    "section": "Example 4: Sensitivity to Model Specification (Robustness Checks)",
    "text": "Example 4: Sensitivity to Model Specification (Robustness Checks)\nWe check if the results change under different regression specifications.\nScenario: A study examines whether a new drug reduces blood pressure.\n\n\nCode\n# Simulated Data\nset.seed(789)\ndata &lt;- tibble(\n  patient_id = 1:500,\n  treated = sample(0:1, 500, replace = TRUE),\n  age = rnorm(500, 50, 10),\n  bmi = rnorm(500, 25, 4),\n  blood_pressure = 120 - treated * 5 + rnorm(500, 0, 5)  # Drug reduces BP by 5 units\n)\n\n# Model 1: Basic regression\nmodel1 &lt;- lm(blood_pressure ~ treated, data = data)\nsummary(model1)\n\n\n\nCall:\nlm(formula = blood_pressure ~ treated, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.4700  -3.1209  -0.1249   3.2771  13.6328 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 120.0594     0.3150  381.19   &lt;2e-16 ***\ntreated      -4.8644     0.4278  -11.37   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.766 on 498 degrees of freedom\nMultiple R-squared:  0.2061,    Adjusted R-squared:  0.2045 \nF-statistic: 129.3 on 1 and 498 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Model 2: Controlling for age\nmodel2 &lt;- lm(blood_pressure ~ treated + age, data = data)\nsummary(model2)\n\n\n\nCall:\nlm(formula = blood_pressure ~ treated + age, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.3825  -3.2104  -0.1291   3.2562  13.4816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 119.5588     1.1514 103.834   &lt;2e-16 ***\ntreated      -4.8608     0.4282 -11.351   &lt;2e-16 ***\nage           0.0099     0.0219   0.452    0.651    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.77 on 497 degrees of freedom\nMultiple R-squared:  0.2064,    Adjusted R-squared:  0.2032 \nF-statistic: 64.64 on 2 and 497 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Model 3: Controlling for age and BMI\nmodel3 &lt;- lm(blood_pressure ~ treated + age + bmi, data = data)\nsummary(model3)\n\n\n\nCall:\nlm(formula = blood_pressure ~ treated + age + bmi, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.3533  -3.1962  -0.1751   3.2654  13.4810 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 119.255805   1.674354  71.225   &lt;2e-16 ***\ntreated      -4.865551   0.429052 -11.340   &lt;2e-16 ***\nage           0.009542   0.021968   0.434    0.664    \nbmi           0.013045   0.052294   0.249    0.803    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.774 on 496 degrees of freedom\nMultiple R-squared:  0.2065,    Adjusted R-squared:  0.2017 \nF-statistic: 43.04 on 3 and 496 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretation:\n\nIf the estimated effect remains stable across models, it suggests robustness.\nIf the effect changes dramatically, it indicates sensitivity to model specification (potential omitted variable bias)."
  },
  {
    "objectID": "exercises/Sensivity Test2.html#example-5-rosenbaum-bounds-for-sensitivity-to-unmeasured-confounding",
    "href": "exercises/Sensivity Test2.html#example-5-rosenbaum-bounds-for-sensitivity-to-unmeasured-confounding",
    "title": "Sensitivity Tests",
    "section": "Example 5: Rosenbaum Bounds for Sensitivity to Unmeasured Confounding",
    "text": "Example 5: Rosenbaum Bounds for Sensitivity to Unmeasured Confounding\nWe test how strong an unmeasured confounder must be to invalidate our results using the rbounds package.\nScenario: We estimate the effect of a scholarship program on college enrollment and check for unmeasured confounding.\n\n\nCode\n# Simulated matched dataset (outcome: enrollment, treatment: scholarship)\nset.seed(111)\nscholarship &lt;- sample(0:1, 500, replace = TRUE)\nenrollment &lt;- scholarship + rnorm(500, 0, 1)\n\n# Wilcoxon signed-rank test for matching\npsens(enrollment, scholarship, Gamma = 2)  # Gamma = strength of confounding\n\n\n\n Rosenbaum Sensitivity Test for Wilcoxon Signed Rank P-Value \n \nUnconfounded estimate ....  0.5866 \n\n Gamma Lower bound Upper bound\n     1      0.5866      0.5866\n     2      0.0000      1.0000\n\n Note: Gamma is Odds of Differential Assignment To\n Treatment Due to Unobserved Factors \n \n\n\nInterpretation: If the effect remains significant for large values of Gamma (e.g., 2+), the results are robust. If the effect disappears for small values of Gamma (&lt;1.2), the findings are highly sensitive to hidden bias."
  },
  {
    "objectID": "exercises/Sensivity Test2.html#example-x-sensitivity-analysis-using-the-sensemakr-package-in-r",
    "href": "exercises/Sensivity Test2.html#example-x-sensitivity-analysis-using-the-sensemakr-package-in-r",
    "title": "Sensitivity Tests",
    "section": "Example X: Sensitivity Analysis Using the sensemakr Package in R",
    "text": "Example X: Sensitivity Analysis Using the sensemakr Package in R\nThe sensemakr package is a powerful tool for sensitivity analysis in causal inference. It helps assess how unmeasured confounders might impact estimated treatment effects.\n📌 Step 2: Simulate a Dataset We create a dataset where:\nwages: Worker’s wage after training training: Binary variable (1 = received job training, 0 = no training) education: Years of education (a potential confounder) experience: Years of work experience (another potential confounder)\nHere, we assume the true treatment effect of training on wages is $3 per hour.\n📌 Step 3: Run the Initial Regression We estimate the impact of job training on wages, controlling for education and experience.\n\n\nCode\n# Set seed for reproducibility\nset.seed(123)\n\n# Simulate data: Job training impact on wages\nn &lt;- 500\ntraining &lt;- rbinom(n, 1, 0.5)  # 50% received training\neducation &lt;- rnorm(n, mean = 12, sd = 2)  # Avg. 12 years of education\nexperience &lt;- rnorm(n, mean = 5, sd = 2)  # Avg. 5 years of experience\nwages &lt;- 20 + 3 * training + 2 * education + 1.5 * experience + rnorm(n, sd = 5)  # Wage equation\n\n# Create data frame\ndata &lt;- data.frame(wages, training, education, experience)\n\n# Fit linear regression model\nmodel &lt;- lm(wages ~ training + education + experience, data = data)\n\n# Display model summary\nsummary(model)\n\n\n\nCall:\nlm(formula = wages ~ training + education + experience, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.6127  -3.1967   0.0747   3.4101  17.0122 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  18.5307     1.4648  12.651  &lt; 2e-16 ***\ntraining      3.4949     0.4445   7.862 2.38e-14 ***\neducation     2.2004     0.1105  19.921  &lt; 2e-16 ***\nexperience    1.3084     0.1091  11.989  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.952 on 496 degrees of freedom\nMultiple R-squared:  0.5555,    Adjusted R-squared:  0.5528 \nF-statistic: 206.6 on 3 and 496 DF,  p-value: &lt; 2.2e-16\n\n\nNow, check the sensitivity:\n📌 Step 4: Conduct Sensitivity Analysis with sensemakr We now check if an unmeasured confounder could explain away the observed effect of training.\n\n\nCode\n# Perform sensitivity analysis with sensemakr\nsensitivity &lt;- sensemakr(\n  model = model,\n  treatment = \"training\",\n  benchmark_covariates = c(\"education\", \"experience\"),  # Use observed confounders as benchmarks\n  q = 1,  # Assess robustness to omitted variable bias\n  alpha = 0.05  # Significance level\n)\n\n# Print sensitivity analysis summary\nsummary(sensitivity)\n\n\nSensitivity Analysis to Unobserved Confounding\n\nModel Formula: wages ~ training + education + experience\n\nNull hypothesis: q = 1 and reduce = TRUE \n-- This means we are considering biases that reduce the absolute value of the current estimate.\n-- The null hypothesis deemed problematic is H0:tau = 0 \n\nUnadjusted Estimates of 'training': \n  Coef. estimate: 3.4949 \n  Standard Error: 0.4445 \n  t-value (H0:tau = 0): 7.8616 \n\nSensitivity Statistics:\n  Partial R2 of treatment with outcome: 0.1108 \n  Robustness Value, q = 1: 0.2961 \n  Robustness Value, q = 1, alpha = 0.05: 0.232 \n\nVerbal interpretation of sensitivity statistics:\n\n-- Partial R2 of the treatment with the outcome: an extreme confounder (orthogonal to the covariates) that explains 100% of the residual variance of the outcome, would need to explain at least 11.08% of the residual variance of the treatment to fully account for the observed estimated effect.\n\n-- Robustness Value, q = 1: unobserved confounders (orthogonal to the covariates) that explain more than 29.61% of the residual variance of both the treatment and the outcome are strong enough to bring the point estimate to 0 (a bias of 100% of the original estimate). Conversely, unobserved confounders that do not explain more than 29.61% of the residual variance of both the treatment and the outcome are not strong enough to bring the point estimate to 0.\n\n-- Robustness Value, q = 1, alpha = 0.05: unobserved confounders (orthogonal to the covariates) that explain more than 23.2% of the residual variance of both the treatment and the outcome are strong enough to bring the estimate to a range where it is no longer 'statistically different' from 0 (a bias of 100% of the original estimate), at the significance level of alpha = 0.05. Conversely, unobserved confounders that do not explain more than 23.2% of the residual variance of both the treatment and the outcome are not strong enough to bring the estimate to a range where it is no longer 'statistically different' from 0, at the significance level of alpha = 0.05.\n\nBounds on omitted variable bias:\n\n--The table below shows the maximum strength of unobserved confounders with association with the treatment and the outcome bounded by a multiple of the observed explanatory power of the chosen benchmark covariate(s).\n\n   Bound Label R2dz.x R2yz.dx Treatment Adjusted Estimate Adjusted Se\n  1x education 0.0037  0.8061  training            2.9529      0.1963\n 1x experience 0.0001  0.2898  training            3.4460      0.3750\n Adjusted T Adjusted Lower CI Adjusted Upper CI\n    15.0403            2.5671            3.3386\n     9.1888            2.7092            4.1828\n\n\n📌 Step 5: Visualize Sensitivity Results sensemakr provides a contour plot that shows how strong an unmeasured confounder would have to be to invalidate our conclusions.\n🔍 How to Interpret the Plot:\nThe x-axis represents the strength of an unmeasured confounder’s association with treatment (job training). The y-axis represents its effect on the outcome (wages). If the observed effect remains above the significance threshold across plausible values, the result is robust.\n\n\nCode\n# Plot sensitivity results\nplot(sensitivity)\n\n\n\n\n\n\n\n\n\nStep 6: Interpret the Sensitivity Analysis Results The summary output will show:\nR-squared of omitted variable needed to fully explain the effect How much the observed effect reduces under different confounding scenarios Whether the treatment effect remains significant under potential omitted variables If the sensitivity analysis shows that an extremely strong confounder is needed to nullify the effect, we can be confident in our causal estimate. If a moderate confounder could eliminate the effect, we should be cautious in our conclusions."
  },
  {
    "objectID": "exercises/Sensivity Test2.html#introduction",
    "href": "exercises/Sensivity Test2.html#introduction",
    "title": "Sensitivity Tests",
    "section": "Introduction",
    "text": "Introduction\nThis document demonstrates how to perform Rosenbaum Bounds Sensitivity Analysis using R, applied to an ecological study assessing the impact of deforestation on bird species richness."
  },
  {
    "objectID": "exercises/Sensivity Test2.html#load-required-libraries",
    "href": "exercises/Sensivity Test2.html#load-required-libraries",
    "title": "Sensitivity Tests",
    "section": "Load Required Libraries",
    "text": "Load Required Libraries\n\n\nCode\n# Install packages if not already installed\nif (!require(MatchIt)) install.packages(\"MatchIt\", dependencies = TRUE)\nif (!require(rbounds)) install.packages(\"rbounds\", dependencies = TRUE)\n\n# Load the libraries\nlibrary(MatchIt)\nlibrary(rbounds)"
  },
  {
    "objectID": "exercises/Sensivity Test2.html#simulate-ecological-data",
    "href": "exercises/Sensivity Test2.html#simulate-ecological-data",
    "title": "Sensitivity Tests",
    "section": "Simulate Ecological Data",
    "text": "Simulate Ecological Data\nWe create a dataset where bird species richness is compared between deforested and non-deforested sites while controlling for elevation and precipitation.\n\n\nCode\nset.seed(123)\ndata &lt;- data.frame(\n  deforestation = rep(c(1, 0), each = 50),\n  species_richness = c(rnorm(50, mean = 10, sd = 3), rnorm(50, mean = 15, sd = 3)),\n  elevation = rnorm(100, mean = 500, sd = 100),\n  precipitation = rnorm(100, mean = 1000, sd = 200)\n)"
  },
  {
    "objectID": "exercises/Sensivity Test2.html#perform-propensity-score-matching",
    "href": "exercises/Sensivity Test2.html#perform-propensity-score-matching",
    "title": "Sensitivity Tests",
    "section": "Perform Propensity Score Matching",
    "text": "Perform Propensity Score Matching\nWe match deforested and non-deforested sites based on elevation and precipitation.\n\n\nCode\nmatch_model &lt;- matchit(deforestation ~ elevation + precipitation, data = data, method = \"nearest\")\nmatched_data &lt;- match.data(match_model)"
  },
  {
    "objectID": "exercises/Sensivity Test2.html#conduct-sensitivity-analysis-with-rosenbaum-bounds",
    "href": "exercises/Sensivity Test2.html#conduct-sensitivity-analysis-with-rosenbaum-bounds",
    "title": "Sensitivity Tests",
    "section": "Conduct Sensitivity Analysis with Rosenbaum Bounds",
    "text": "Conduct Sensitivity Analysis with Rosenbaum Bounds\nWe check how sensitive our results are to hidden bias.\n\n\nCode\n# Extract matched treatment and control outcomes\ntreated_outcomes &lt;- matched_data$species_richness[matched_data$deforestation == 1]\ncontrol_outcomes &lt;- matched_data$species_richness[matched_data$deforestation == 0]\n\n# Perform Rosenbaum sensitivity analysis\npsens(x = treated_outcomes, y = control_outcomes, Gamma = 1.5)  # Adjust Gamma to test sensitivity\n\n\n\n Rosenbaum Sensitivity Test for Wilcoxon Signed Rank P-Value \n \nUnconfounded estimate ....  1 \n\n Gamma Lower bound Upper bound\n     1           1           1\n\n Note: Gamma is Odds of Differential Assignment To\n Treatment Due to Unobserved Factors"
  },
  {
    "objectID": "exercises/Sensivity Test2.html#interpretation-of-results",
    "href": "exercises/Sensivity Test2.html#interpretation-of-results",
    "title": "Sensitivity Tests",
    "section": "Interpretation of Results",
    "text": "Interpretation of Results\n\nIf the p-value remains small at Γ = 1.5, our conclusions are robust to moderate hidden bias.\nIf the p-value becomes insignificant at lower values of Γ, our study is highly sensitive to hidden confounders."
  },
  {
    "objectID": "exercises/Sensivity Test2.html#conclusion",
    "href": "exercises/Sensivity Test2.html#conclusion",
    "title": "Sensitivity Tests",
    "section": "Conclusion",
    "text": "Conclusion\nRosenbaum bounds allow us to assess the robustness of our ecological study results against unmeasured confounding. This is particularly useful when controlled experiments are infeasible.\n\nTake Aways\nRosenbaum bounds provide a way to assess how much an unmeasured confounder could influence the causal relationship between deforestation and bird species richness. In ecological studies, where controlled experiments are often impractical, this method helps gauge the reliability of observational findings."
  },
  {
    "objectID": "readings.html#due-wed.-416-power-standard-error-estimation-and-inference-mobeen-and-lincoln",
    "href": "readings.html#due-wed.-416-power-standard-error-estimation-and-inference-mobeen-and-lincoln",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed. 4/16 Power, standard error estimation, and inference [Mobeen and Lincoln]",
    "text": "Due Wed. 4/16 Power, standard error estimation, and inference [Mobeen and Lincoln]\nRead The Effect: Chapter 13 “Your standard errors are probably wrong”\nRead Kimmel, Avolio & Ferraro 2023\nFor a deeper dive on standard error estimation, read Cameron and Miller. A pracitioner’s guide to clustered robust standard errrors",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#due-wed-430-no-class-work-on-final-report",
    "href": "readings.html#due-wed-430-no-class-work-on-final-report",
    "title": "Readings by Class & Additional Materials",
    "section": "Due Wed 4/30 No Class: Work on final report",
    "text": "Due Wed 4/30 No Class: Work on final report",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  },
  {
    "objectID": "readings.html#projects-due-sunday-may-4-by-email",
    "href": "readings.html#projects-due-sunday-may-4-by-email",
    "title": "Readings by Class & Additional Materials",
    "section": "Projects Due Sunday May 4 by email",
    "text": "Projects Due Sunday May 4 by email\nSee Assignments & Evaluation tab for more information",
    "crumbs": [
      "Readings & Additional Materials"
    ]
  }
]